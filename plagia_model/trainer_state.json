{
  "best_global_step": 36288,
  "best_metric": 0.9861111111111112,
  "best_model_checkpoint": "./results/checkpoint-36288",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 36288,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0002755731922398589,
      "grad_norm": 2.5679662227630615,
      "learning_rate": 1.999503968253968e-05,
      "loss": 0.7,
      "step": 10
    },
    {
      "epoch": 0.0005511463844797178,
      "grad_norm": 2.7543156147003174,
      "learning_rate": 1.9989528218694887e-05,
      "loss": 0.699,
      "step": 20
    },
    {
      "epoch": 0.0008267195767195767,
      "grad_norm": 2.288376808166504,
      "learning_rate": 1.998401675485009e-05,
      "loss": 0.6759,
      "step": 30
    },
    {
      "epoch": 0.0011022927689594356,
      "grad_norm": 4.247477054595947,
      "learning_rate": 1.9978505291005293e-05,
      "loss": 0.6819,
      "step": 40
    },
    {
      "epoch": 0.0013778659611992945,
      "grad_norm": 5.335082530975342,
      "learning_rate": 1.9972993827160495e-05,
      "loss": 0.6849,
      "step": 50
    },
    {
      "epoch": 0.0016534391534391533,
      "grad_norm": 3.2906200885772705,
      "learning_rate": 1.9967482363315696e-05,
      "loss": 0.6708,
      "step": 60
    },
    {
      "epoch": 0.0019290123456790122,
      "grad_norm": 3.30582594871521,
      "learning_rate": 1.99619708994709e-05,
      "loss": 0.6664,
      "step": 70
    },
    {
      "epoch": 0.002204585537918871,
      "grad_norm": 2.7148263454437256,
      "learning_rate": 1.9956459435626103e-05,
      "loss": 0.6391,
      "step": 80
    },
    {
      "epoch": 0.00248015873015873,
      "grad_norm": 2.461091995239258,
      "learning_rate": 1.9950947971781308e-05,
      "loss": 0.6515,
      "step": 90
    },
    {
      "epoch": 0.002755731922398589,
      "grad_norm": 3.708730697631836,
      "learning_rate": 1.994543650793651e-05,
      "loss": 0.6317,
      "step": 100
    },
    {
      "epoch": 0.003031305114638448,
      "grad_norm": 3.7653274536132812,
      "learning_rate": 1.993992504409171e-05,
      "loss": 0.6253,
      "step": 110
    },
    {
      "epoch": 0.0033068783068783067,
      "grad_norm": 4.5417985916137695,
      "learning_rate": 1.9934413580246916e-05,
      "loss": 0.643,
      "step": 120
    },
    {
      "epoch": 0.0035824514991181656,
      "grad_norm": 2.4636566638946533,
      "learning_rate": 1.9928902116402117e-05,
      "loss": 0.6074,
      "step": 130
    },
    {
      "epoch": 0.0038580246913580245,
      "grad_norm": 2.2422945499420166,
      "learning_rate": 1.9923390652557322e-05,
      "loss": 0.6144,
      "step": 140
    },
    {
      "epoch": 0.004133597883597883,
      "grad_norm": 2.6251211166381836,
      "learning_rate": 1.9917879188712524e-05,
      "loss": 0.6149,
      "step": 150
    },
    {
      "epoch": 0.004409171075837742,
      "grad_norm": 2.832965135574341,
      "learning_rate": 1.9912367724867725e-05,
      "loss": 0.6263,
      "step": 160
    },
    {
      "epoch": 0.004684744268077601,
      "grad_norm": 4.82621955871582,
      "learning_rate": 1.990685626102293e-05,
      "loss": 0.5905,
      "step": 170
    },
    {
      "epoch": 0.00496031746031746,
      "grad_norm": 5.04459810256958,
      "learning_rate": 1.9901344797178132e-05,
      "loss": 0.6112,
      "step": 180
    },
    {
      "epoch": 0.005235890652557319,
      "grad_norm": 3.1774392127990723,
      "learning_rate": 1.9895833333333334e-05,
      "loss": 0.5881,
      "step": 190
    },
    {
      "epoch": 0.005511463844797178,
      "grad_norm": 2.8635904788970947,
      "learning_rate": 1.989032186948854e-05,
      "loss": 0.6041,
      "step": 200
    },
    {
      "epoch": 0.005787037037037037,
      "grad_norm": 3.003621816635132,
      "learning_rate": 1.988481040564374e-05,
      "loss": 0.5788,
      "step": 210
    },
    {
      "epoch": 0.006062610229276896,
      "grad_norm": 5.358158588409424,
      "learning_rate": 1.9879298941798945e-05,
      "loss": 0.5645,
      "step": 220
    },
    {
      "epoch": 0.0063381834215167545,
      "grad_norm": 4.09895133972168,
      "learning_rate": 1.9873787477954147e-05,
      "loss": 0.5824,
      "step": 230
    },
    {
      "epoch": 0.006613756613756613,
      "grad_norm": 3.5296919345855713,
      "learning_rate": 1.9868276014109348e-05,
      "loss": 0.5721,
      "step": 240
    },
    {
      "epoch": 0.006889329805996472,
      "grad_norm": 2.763392210006714,
      "learning_rate": 1.9862764550264553e-05,
      "loss": 0.6052,
      "step": 250
    },
    {
      "epoch": 0.007164902998236331,
      "grad_norm": 2.7137467861175537,
      "learning_rate": 1.9857253086419755e-05,
      "loss": 0.5082,
      "step": 260
    },
    {
      "epoch": 0.00744047619047619,
      "grad_norm": 4.844370365142822,
      "learning_rate": 1.985174162257496e-05,
      "loss": 0.5664,
      "step": 270
    },
    {
      "epoch": 0.007716049382716049,
      "grad_norm": 7.477538108825684,
      "learning_rate": 1.984623015873016e-05,
      "loss": 0.5555,
      "step": 280
    },
    {
      "epoch": 0.007991622574955908,
      "grad_norm": 4.089080333709717,
      "learning_rate": 1.9840718694885363e-05,
      "loss": 0.5644,
      "step": 290
    },
    {
      "epoch": 0.008267195767195767,
      "grad_norm": 3.367280960083008,
      "learning_rate": 1.9835207231040564e-05,
      "loss": 0.4989,
      "step": 300
    },
    {
      "epoch": 0.008542768959435626,
      "grad_norm": 3.1661314964294434,
      "learning_rate": 1.982969576719577e-05,
      "loss": 0.5214,
      "step": 310
    },
    {
      "epoch": 0.008818342151675485,
      "grad_norm": 6.7261810302734375,
      "learning_rate": 1.982418430335097e-05,
      "loss": 0.4834,
      "step": 320
    },
    {
      "epoch": 0.009093915343915343,
      "grad_norm": 5.991037845611572,
      "learning_rate": 1.9818672839506176e-05,
      "loss": 0.5359,
      "step": 330
    },
    {
      "epoch": 0.009369488536155202,
      "grad_norm": 2.9501023292541504,
      "learning_rate": 1.9813161375661377e-05,
      "loss": 0.5187,
      "step": 340
    },
    {
      "epoch": 0.009645061728395061,
      "grad_norm": 3.062241315841675,
      "learning_rate": 1.980764991181658e-05,
      "loss": 0.5347,
      "step": 350
    },
    {
      "epoch": 0.00992063492063492,
      "grad_norm": 4.644716262817383,
      "learning_rate": 1.9802138447971784e-05,
      "loss": 0.4827,
      "step": 360
    },
    {
      "epoch": 0.010196208112874779,
      "grad_norm": 9.580293655395508,
      "learning_rate": 1.9796626984126985e-05,
      "loss": 0.4682,
      "step": 370
    },
    {
      "epoch": 0.010471781305114638,
      "grad_norm": 6.411031723022461,
      "learning_rate": 1.979111552028219e-05,
      "loss": 0.5126,
      "step": 380
    },
    {
      "epoch": 0.010747354497354497,
      "grad_norm": 8.658954620361328,
      "learning_rate": 1.9785604056437392e-05,
      "loss": 0.4876,
      "step": 390
    },
    {
      "epoch": 0.011022927689594356,
      "grad_norm": 4.228001594543457,
      "learning_rate": 1.9780092592592594e-05,
      "loss": 0.4711,
      "step": 400
    },
    {
      "epoch": 0.011298500881834215,
      "grad_norm": 3.190375804901123,
      "learning_rate": 1.9774581128747795e-05,
      "loss": 0.4772,
      "step": 410
    },
    {
      "epoch": 0.011574074074074073,
      "grad_norm": 7.440104961395264,
      "learning_rate": 1.9769069664903e-05,
      "loss": 0.4173,
      "step": 420
    },
    {
      "epoch": 0.011849647266313932,
      "grad_norm": 5.803762435913086,
      "learning_rate": 1.97635582010582e-05,
      "loss": 0.4779,
      "step": 430
    },
    {
      "epoch": 0.012125220458553791,
      "grad_norm": 3.601322889328003,
      "learning_rate": 1.9758046737213407e-05,
      "loss": 0.4383,
      "step": 440
    },
    {
      "epoch": 0.01240079365079365,
      "grad_norm": 8.753759384155273,
      "learning_rate": 1.9752535273368608e-05,
      "loss": 0.4647,
      "step": 450
    },
    {
      "epoch": 0.012676366843033509,
      "grad_norm": 9.25944995880127,
      "learning_rate": 1.974702380952381e-05,
      "loss": 0.4391,
      "step": 460
    },
    {
      "epoch": 0.012951940035273368,
      "grad_norm": 3.2941606044769287,
      "learning_rate": 1.9741512345679015e-05,
      "loss": 0.3798,
      "step": 470
    },
    {
      "epoch": 0.013227513227513227,
      "grad_norm": 5.811287879943848,
      "learning_rate": 1.9736000881834216e-05,
      "loss": 0.4132,
      "step": 480
    },
    {
      "epoch": 0.013503086419753086,
      "grad_norm": 4.380153179168701,
      "learning_rate": 1.973048941798942e-05,
      "loss": 0.4717,
      "step": 490
    },
    {
      "epoch": 0.013778659611992945,
      "grad_norm": 7.443408966064453,
      "learning_rate": 1.9724977954144623e-05,
      "loss": 0.4132,
      "step": 500
    },
    {
      "epoch": 0.014054232804232803,
      "grad_norm": 3.3941900730133057,
      "learning_rate": 1.9719466490299824e-05,
      "loss": 0.4042,
      "step": 510
    },
    {
      "epoch": 0.014329805996472662,
      "grad_norm": 6.517534255981445,
      "learning_rate": 1.9713955026455026e-05,
      "loss": 0.4201,
      "step": 520
    },
    {
      "epoch": 0.014605379188712521,
      "grad_norm": 4.898617744445801,
      "learning_rate": 1.970844356261023e-05,
      "loss": 0.4011,
      "step": 530
    },
    {
      "epoch": 0.01488095238095238,
      "grad_norm": 8.324699401855469,
      "learning_rate": 1.9702932098765432e-05,
      "loss": 0.3989,
      "step": 540
    },
    {
      "epoch": 0.015156525573192239,
      "grad_norm": 4.344242095947266,
      "learning_rate": 1.9697420634920637e-05,
      "loss": 0.3925,
      "step": 550
    },
    {
      "epoch": 0.015432098765432098,
      "grad_norm": 7.96613073348999,
      "learning_rate": 1.969190917107584e-05,
      "loss": 0.4385,
      "step": 560
    },
    {
      "epoch": 0.01570767195767196,
      "grad_norm": 10.148584365844727,
      "learning_rate": 1.968639770723104e-05,
      "loss": 0.3608,
      "step": 570
    },
    {
      "epoch": 0.015983245149911816,
      "grad_norm": 3.5378544330596924,
      "learning_rate": 1.9680886243386245e-05,
      "loss": 0.3676,
      "step": 580
    },
    {
      "epoch": 0.016258818342151676,
      "grad_norm": 6.879810333251953,
      "learning_rate": 1.9675374779541447e-05,
      "loss": 0.3615,
      "step": 590
    },
    {
      "epoch": 0.016534391534391533,
      "grad_norm": 6.244984149932861,
      "learning_rate": 1.9669863315696652e-05,
      "loss": 0.3752,
      "step": 600
    },
    {
      "epoch": 0.016809964726631394,
      "grad_norm": 5.593819618225098,
      "learning_rate": 1.9664351851851853e-05,
      "loss": 0.4393,
      "step": 610
    },
    {
      "epoch": 0.01708553791887125,
      "grad_norm": 4.211980819702148,
      "learning_rate": 1.9658840388007055e-05,
      "loss": 0.3296,
      "step": 620
    },
    {
      "epoch": 0.017361111111111112,
      "grad_norm": 12.5972261428833,
      "learning_rate": 1.9653328924162257e-05,
      "loss": 0.3555,
      "step": 630
    },
    {
      "epoch": 0.01763668430335097,
      "grad_norm": 6.399786472320557,
      "learning_rate": 1.964781746031746e-05,
      "loss": 0.3784,
      "step": 640
    },
    {
      "epoch": 0.01791225749559083,
      "grad_norm": 10.6549711227417,
      "learning_rate": 1.9642305996472663e-05,
      "loss": 0.363,
      "step": 650
    },
    {
      "epoch": 0.018187830687830687,
      "grad_norm": 2.4395816326141357,
      "learning_rate": 1.9636794532627868e-05,
      "loss": 0.3013,
      "step": 660
    },
    {
      "epoch": 0.018463403880070547,
      "grad_norm": 3.132096529006958,
      "learning_rate": 1.963128306878307e-05,
      "loss": 0.3845,
      "step": 670
    },
    {
      "epoch": 0.018738977072310405,
      "grad_norm": 7.187466144561768,
      "learning_rate": 1.9625771604938275e-05,
      "loss": 0.3848,
      "step": 680
    },
    {
      "epoch": 0.019014550264550265,
      "grad_norm": 10.454071044921875,
      "learning_rate": 1.9620260141093476e-05,
      "loss": 0.3717,
      "step": 690
    },
    {
      "epoch": 0.019290123456790122,
      "grad_norm": 21.252574920654297,
      "learning_rate": 1.9614748677248678e-05,
      "loss": 0.3314,
      "step": 700
    },
    {
      "epoch": 0.019565696649029983,
      "grad_norm": 3.3906912803649902,
      "learning_rate": 1.9609237213403883e-05,
      "loss": 0.2839,
      "step": 710
    },
    {
      "epoch": 0.01984126984126984,
      "grad_norm": 11.362034797668457,
      "learning_rate": 1.9603725749559084e-05,
      "loss": 0.3522,
      "step": 720
    },
    {
      "epoch": 0.0201168430335097,
      "grad_norm": 17.603513717651367,
      "learning_rate": 1.959821428571429e-05,
      "loss": 0.2898,
      "step": 730
    },
    {
      "epoch": 0.020392416225749558,
      "grad_norm": 20.061168670654297,
      "learning_rate": 1.959270282186949e-05,
      "loss": 0.3012,
      "step": 740
    },
    {
      "epoch": 0.02066798941798942,
      "grad_norm": 12.872127532958984,
      "learning_rate": 1.9587191358024692e-05,
      "loss": 0.2982,
      "step": 750
    },
    {
      "epoch": 0.020943562610229276,
      "grad_norm": 8.936501502990723,
      "learning_rate": 1.9581679894179894e-05,
      "loss": 0.3198,
      "step": 760
    },
    {
      "epoch": 0.021219135802469136,
      "grad_norm": 2.165215253829956,
      "learning_rate": 1.95761684303351e-05,
      "loss": 0.3084,
      "step": 770
    },
    {
      "epoch": 0.021494708994708994,
      "grad_norm": 2.97540020942688,
      "learning_rate": 1.9570656966490304e-05,
      "loss": 0.3024,
      "step": 780
    },
    {
      "epoch": 0.021770282186948854,
      "grad_norm": 19.670427322387695,
      "learning_rate": 1.9565145502645505e-05,
      "loss": 0.2855,
      "step": 790
    },
    {
      "epoch": 0.02204585537918871,
      "grad_norm": 11.31971549987793,
      "learning_rate": 1.9559634038800707e-05,
      "loss": 0.3715,
      "step": 800
    },
    {
      "epoch": 0.022321428571428572,
      "grad_norm": 9.736669540405273,
      "learning_rate": 1.955412257495591e-05,
      "loss": 0.315,
      "step": 810
    },
    {
      "epoch": 0.02259700176366843,
      "grad_norm": 2.7016892433166504,
      "learning_rate": 1.9548611111111113e-05,
      "loss": 0.2131,
      "step": 820
    },
    {
      "epoch": 0.02287257495590829,
      "grad_norm": 7.684514045715332,
      "learning_rate": 1.9543099647266315e-05,
      "loss": 0.306,
      "step": 830
    },
    {
      "epoch": 0.023148148148148147,
      "grad_norm": 12.322202682495117,
      "learning_rate": 1.953758818342152e-05,
      "loss": 0.3357,
      "step": 840
    },
    {
      "epoch": 0.023423721340388007,
      "grad_norm": 24.017229080200195,
      "learning_rate": 1.953207671957672e-05,
      "loss": 0.2829,
      "step": 850
    },
    {
      "epoch": 0.023699294532627865,
      "grad_norm": 7.961202621459961,
      "learning_rate": 1.9526565255731923e-05,
      "loss": 0.3453,
      "step": 860
    },
    {
      "epoch": 0.023974867724867725,
      "grad_norm": 4.360997200012207,
      "learning_rate": 1.9521053791887128e-05,
      "loss": 0.2468,
      "step": 870
    },
    {
      "epoch": 0.024250440917107582,
      "grad_norm": 2.385521173477173,
      "learning_rate": 1.951554232804233e-05,
      "loss": 0.3144,
      "step": 880
    },
    {
      "epoch": 0.024526014109347443,
      "grad_norm": 7.367006778717041,
      "learning_rate": 1.9510030864197535e-05,
      "loss": 0.2871,
      "step": 890
    },
    {
      "epoch": 0.0248015873015873,
      "grad_norm": 6.671397686004639,
      "learning_rate": 1.9504519400352736e-05,
      "loss": 0.2652,
      "step": 900
    },
    {
      "epoch": 0.02507716049382716,
      "grad_norm": 7.301712989807129,
      "learning_rate": 1.9499007936507938e-05,
      "loss": 0.2712,
      "step": 910
    },
    {
      "epoch": 0.025352733686067018,
      "grad_norm": 13.616202354431152,
      "learning_rate": 1.949349647266314e-05,
      "loss": 0.2846,
      "step": 920
    },
    {
      "epoch": 0.02562830687830688,
      "grad_norm": 15.200571060180664,
      "learning_rate": 1.9487985008818344e-05,
      "loss": 0.2764,
      "step": 930
    },
    {
      "epoch": 0.025903880070546736,
      "grad_norm": 5.902643203735352,
      "learning_rate": 1.9482473544973546e-05,
      "loss": 0.2423,
      "step": 940
    },
    {
      "epoch": 0.026179453262786596,
      "grad_norm": 24.191791534423828,
      "learning_rate": 1.947696208112875e-05,
      "loss": 0.3326,
      "step": 950
    },
    {
      "epoch": 0.026455026455026454,
      "grad_norm": 2.857405424118042,
      "learning_rate": 1.9471450617283952e-05,
      "loss": 0.3627,
      "step": 960
    },
    {
      "epoch": 0.026730599647266314,
      "grad_norm": 50.34782409667969,
      "learning_rate": 1.9465939153439154e-05,
      "loss": 0.2783,
      "step": 970
    },
    {
      "epoch": 0.02700617283950617,
      "grad_norm": 39.47982406616211,
      "learning_rate": 1.946042768959436e-05,
      "loss": 0.3128,
      "step": 980
    },
    {
      "epoch": 0.027281746031746032,
      "grad_norm": 1.5623810291290283,
      "learning_rate": 1.945491622574956e-05,
      "loss": 0.1679,
      "step": 990
    },
    {
      "epoch": 0.02755731922398589,
      "grad_norm": 2.647020101547241,
      "learning_rate": 1.9449404761904765e-05,
      "loss": 0.2276,
      "step": 1000
    },
    {
      "epoch": 0.02783289241622575,
      "grad_norm": 15.59131908416748,
      "learning_rate": 1.9443893298059967e-05,
      "loss": 0.256,
      "step": 1010
    },
    {
      "epoch": 0.028108465608465607,
      "grad_norm": 1.3375493288040161,
      "learning_rate": 1.943838183421517e-05,
      "loss": 0.2476,
      "step": 1020
    },
    {
      "epoch": 0.028384038800705468,
      "grad_norm": 1.357032299041748,
      "learning_rate": 1.943287037037037e-05,
      "loss": 0.2803,
      "step": 1030
    },
    {
      "epoch": 0.028659611992945325,
      "grad_norm": 0.9648671746253967,
      "learning_rate": 1.9427358906525575e-05,
      "loss": 0.3297,
      "step": 1040
    },
    {
      "epoch": 0.028935185185185185,
      "grad_norm": 1.0509579181671143,
      "learning_rate": 1.9421847442680777e-05,
      "loss": 0.2177,
      "step": 1050
    },
    {
      "epoch": 0.029210758377425042,
      "grad_norm": 35.8299446105957,
      "learning_rate": 1.941633597883598e-05,
      "loss": 0.2889,
      "step": 1060
    },
    {
      "epoch": 0.029486331569664903,
      "grad_norm": 1.5894290208816528,
      "learning_rate": 1.9410824514991183e-05,
      "loss": 0.2397,
      "step": 1070
    },
    {
      "epoch": 0.02976190476190476,
      "grad_norm": 31.008520126342773,
      "learning_rate": 1.9405313051146385e-05,
      "loss": 0.3171,
      "step": 1080
    },
    {
      "epoch": 0.03003747795414462,
      "grad_norm": 19.965482711791992,
      "learning_rate": 1.939980158730159e-05,
      "loss": 0.2628,
      "step": 1090
    },
    {
      "epoch": 0.030313051146384478,
      "grad_norm": 28.74945068359375,
      "learning_rate": 1.939429012345679e-05,
      "loss": 0.2887,
      "step": 1100
    },
    {
      "epoch": 0.03058862433862434,
      "grad_norm": 11.552401542663574,
      "learning_rate": 1.9388778659611996e-05,
      "loss": 0.2203,
      "step": 1110
    },
    {
      "epoch": 0.030864197530864196,
      "grad_norm": 1.0325207710266113,
      "learning_rate": 1.9383267195767198e-05,
      "loss": 0.1683,
      "step": 1120
    },
    {
      "epoch": 0.031139770723104056,
      "grad_norm": 37.64442825317383,
      "learning_rate": 1.93777557319224e-05,
      "loss": 0.3752,
      "step": 1130
    },
    {
      "epoch": 0.03141534391534392,
      "grad_norm": 14.916778564453125,
      "learning_rate": 1.93722442680776e-05,
      "loss": 0.2282,
      "step": 1140
    },
    {
      "epoch": 0.031690917107583774,
      "grad_norm": 5.8876543045043945,
      "learning_rate": 1.9366732804232806e-05,
      "loss": 0.2781,
      "step": 1150
    },
    {
      "epoch": 0.03196649029982363,
      "grad_norm": 4.061999797821045,
      "learning_rate": 1.9361221340388007e-05,
      "loss": 0.2436,
      "step": 1160
    },
    {
      "epoch": 0.032242063492063495,
      "grad_norm": 2.100905418395996,
      "learning_rate": 1.9355709876543212e-05,
      "loss": 0.1865,
      "step": 1170
    },
    {
      "epoch": 0.03251763668430335,
      "grad_norm": 10.821487426757812,
      "learning_rate": 1.9350198412698414e-05,
      "loss": 0.3249,
      "step": 1180
    },
    {
      "epoch": 0.03279320987654321,
      "grad_norm": 9.71623706817627,
      "learning_rate": 1.934468694885362e-05,
      "loss": 0.2295,
      "step": 1190
    },
    {
      "epoch": 0.03306878306878307,
      "grad_norm": 24.71424674987793,
      "learning_rate": 1.933917548500882e-05,
      "loss": 0.288,
      "step": 1200
    },
    {
      "epoch": 0.03334435626102293,
      "grad_norm": 25.26862335205078,
      "learning_rate": 1.9333664021164022e-05,
      "loss": 0.4339,
      "step": 1210
    },
    {
      "epoch": 0.03361992945326279,
      "grad_norm": 13.871379852294922,
      "learning_rate": 1.9328152557319227e-05,
      "loss": 0.3338,
      "step": 1220
    },
    {
      "epoch": 0.033895502645502645,
      "grad_norm": 8.34208869934082,
      "learning_rate": 1.932264109347443e-05,
      "loss": 0.2677,
      "step": 1230
    },
    {
      "epoch": 0.0341710758377425,
      "grad_norm": 31.112628936767578,
      "learning_rate": 1.9317129629629633e-05,
      "loss": 0.1777,
      "step": 1240
    },
    {
      "epoch": 0.03444664902998237,
      "grad_norm": 13.302045822143555,
      "learning_rate": 1.9311618165784835e-05,
      "loss": 0.3256,
      "step": 1250
    },
    {
      "epoch": 0.034722222222222224,
      "grad_norm": 12.079092025756836,
      "learning_rate": 1.9306106701940036e-05,
      "loss": 0.2606,
      "step": 1260
    },
    {
      "epoch": 0.03499779541446208,
      "grad_norm": 1.2477355003356934,
      "learning_rate": 1.9300595238095238e-05,
      "loss": 0.2158,
      "step": 1270
    },
    {
      "epoch": 0.03527336860670194,
      "grad_norm": 26.607582092285156,
      "learning_rate": 1.9295083774250443e-05,
      "loss": 0.3033,
      "step": 1280
    },
    {
      "epoch": 0.0355489417989418,
      "grad_norm": 2.2325127124786377,
      "learning_rate": 1.9289572310405645e-05,
      "loss": 0.2388,
      "step": 1290
    },
    {
      "epoch": 0.03582451499118166,
      "grad_norm": 0.5801912546157837,
      "learning_rate": 1.928406084656085e-05,
      "loss": 0.1875,
      "step": 1300
    },
    {
      "epoch": 0.036100088183421516,
      "grad_norm": 48.06755447387695,
      "learning_rate": 1.927854938271605e-05,
      "loss": 0.3618,
      "step": 1310
    },
    {
      "epoch": 0.036375661375661374,
      "grad_norm": 0.4773120582103729,
      "learning_rate": 1.9273037918871253e-05,
      "loss": 0.2841,
      "step": 1320
    },
    {
      "epoch": 0.03665123456790124,
      "grad_norm": 19.814056396484375,
      "learning_rate": 1.9267526455026458e-05,
      "loss": 0.2137,
      "step": 1330
    },
    {
      "epoch": 0.036926807760141095,
      "grad_norm": 3.4748430252075195,
      "learning_rate": 1.926201499118166e-05,
      "loss": 0.1475,
      "step": 1340
    },
    {
      "epoch": 0.03720238095238095,
      "grad_norm": 2.46638560295105,
      "learning_rate": 1.9256503527336864e-05,
      "loss": 0.169,
      "step": 1350
    },
    {
      "epoch": 0.03747795414462081,
      "grad_norm": 55.63356399536133,
      "learning_rate": 1.9250992063492066e-05,
      "loss": 0.2523,
      "step": 1360
    },
    {
      "epoch": 0.03775352733686067,
      "grad_norm": 4.710741996765137,
      "learning_rate": 1.9245480599647267e-05,
      "loss": 0.2786,
      "step": 1370
    },
    {
      "epoch": 0.03802910052910053,
      "grad_norm": 22.224594116210938,
      "learning_rate": 1.923996913580247e-05,
      "loss": 0.294,
      "step": 1380
    },
    {
      "epoch": 0.03830467372134039,
      "grad_norm": 8.43044376373291,
      "learning_rate": 1.9234457671957674e-05,
      "loss": 0.2073,
      "step": 1390
    },
    {
      "epoch": 0.038580246913580245,
      "grad_norm": 44.63589859008789,
      "learning_rate": 1.9228946208112875e-05,
      "loss": 0.1751,
      "step": 1400
    },
    {
      "epoch": 0.03885582010582011,
      "grad_norm": 33.50349807739258,
      "learning_rate": 1.922343474426808e-05,
      "loss": 0.2032,
      "step": 1410
    },
    {
      "epoch": 0.039131393298059966,
      "grad_norm": 19.086597442626953,
      "learning_rate": 1.9217923280423282e-05,
      "loss": 0.2936,
      "step": 1420
    },
    {
      "epoch": 0.03940696649029982,
      "grad_norm": 29.70170021057129,
      "learning_rate": 1.9212411816578483e-05,
      "loss": 0.2281,
      "step": 1430
    },
    {
      "epoch": 0.03968253968253968,
      "grad_norm": 4.537534236907959,
      "learning_rate": 1.920690035273369e-05,
      "loss": 0.1794,
      "step": 1440
    },
    {
      "epoch": 0.039958112874779544,
      "grad_norm": 23.406747817993164,
      "learning_rate": 1.920138888888889e-05,
      "loss": 0.2851,
      "step": 1450
    },
    {
      "epoch": 0.0402336860670194,
      "grad_norm": 0.729008138179779,
      "learning_rate": 1.9195877425044095e-05,
      "loss": 0.1543,
      "step": 1460
    },
    {
      "epoch": 0.04050925925925926,
      "grad_norm": 2.6918468475341797,
      "learning_rate": 1.9190365961199296e-05,
      "loss": 0.1081,
      "step": 1470
    },
    {
      "epoch": 0.040784832451499116,
      "grad_norm": 41.715232849121094,
      "learning_rate": 1.9184854497354498e-05,
      "loss": 0.1737,
      "step": 1480
    },
    {
      "epoch": 0.04106040564373898,
      "grad_norm": 3.010498523712158,
      "learning_rate": 1.91793430335097e-05,
      "loss": 0.1113,
      "step": 1490
    },
    {
      "epoch": 0.04133597883597884,
      "grad_norm": 0.5369434952735901,
      "learning_rate": 1.9173831569664905e-05,
      "loss": 0.0948,
      "step": 1500
    },
    {
      "epoch": 0.041611552028218694,
      "grad_norm": 0.8847678303718567,
      "learning_rate": 1.916832010582011e-05,
      "loss": 0.3461,
      "step": 1510
    },
    {
      "epoch": 0.04188712522045855,
      "grad_norm": 5.362828731536865,
      "learning_rate": 1.916280864197531e-05,
      "loss": 0.1399,
      "step": 1520
    },
    {
      "epoch": 0.042162698412698416,
      "grad_norm": 21.72470474243164,
      "learning_rate": 1.9157297178130513e-05,
      "loss": 0.1391,
      "step": 1530
    },
    {
      "epoch": 0.04243827160493827,
      "grad_norm": 23.1661319732666,
      "learning_rate": 1.9151785714285714e-05,
      "loss": 0.1568,
      "step": 1540
    },
    {
      "epoch": 0.04271384479717813,
      "grad_norm": 2.499781608581543,
      "learning_rate": 1.914627425044092e-05,
      "loss": 0.2887,
      "step": 1550
    },
    {
      "epoch": 0.04298941798941799,
      "grad_norm": 34.45148849487305,
      "learning_rate": 1.914076278659612e-05,
      "loss": 0.1885,
      "step": 1560
    },
    {
      "epoch": 0.04326499118165785,
      "grad_norm": 15.989119529724121,
      "learning_rate": 1.9135251322751326e-05,
      "loss": 0.2493,
      "step": 1570
    },
    {
      "epoch": 0.04354056437389771,
      "grad_norm": 6.698824882507324,
      "learning_rate": 1.9129739858906527e-05,
      "loss": 0.2344,
      "step": 1580
    },
    {
      "epoch": 0.043816137566137565,
      "grad_norm": 34.208351135253906,
      "learning_rate": 1.912422839506173e-05,
      "loss": 0.0828,
      "step": 1590
    },
    {
      "epoch": 0.04409171075837742,
      "grad_norm": 16.685304641723633,
      "learning_rate": 1.911871693121693e-05,
      "loss": 0.2447,
      "step": 1600
    },
    {
      "epoch": 0.04436728395061729,
      "grad_norm": 26.29063606262207,
      "learning_rate": 1.9113205467372135e-05,
      "loss": 0.2862,
      "step": 1610
    },
    {
      "epoch": 0.044642857142857144,
      "grad_norm": 2.3179216384887695,
      "learning_rate": 1.910769400352734e-05,
      "loss": 0.1651,
      "step": 1620
    },
    {
      "epoch": 0.044918430335097,
      "grad_norm": 70.45172882080078,
      "learning_rate": 1.9102182539682542e-05,
      "loss": 0.2822,
      "step": 1630
    },
    {
      "epoch": 0.04519400352733686,
      "grad_norm": 9.5404052734375,
      "learning_rate": 1.9096671075837743e-05,
      "loss": 0.2739,
      "step": 1640
    },
    {
      "epoch": 0.04546957671957672,
      "grad_norm": 0.7716514468193054,
      "learning_rate": 1.9091159611992945e-05,
      "loss": 0.2425,
      "step": 1650
    },
    {
      "epoch": 0.04574514991181658,
      "grad_norm": 0.2615138590335846,
      "learning_rate": 1.908564814814815e-05,
      "loss": 0.2786,
      "step": 1660
    },
    {
      "epoch": 0.04602072310405644,
      "grad_norm": 0.49888086318969727,
      "learning_rate": 1.908013668430335e-05,
      "loss": 0.1289,
      "step": 1670
    },
    {
      "epoch": 0.046296296296296294,
      "grad_norm": 3.1518523693084717,
      "learning_rate": 1.9074625220458556e-05,
      "loss": 0.1797,
      "step": 1680
    },
    {
      "epoch": 0.04657186948853616,
      "grad_norm": 6.682518005371094,
      "learning_rate": 1.9069113756613758e-05,
      "loss": 0.1787,
      "step": 1690
    },
    {
      "epoch": 0.046847442680776015,
      "grad_norm": 15.726584434509277,
      "learning_rate": 1.9063602292768963e-05,
      "loss": 0.2984,
      "step": 1700
    },
    {
      "epoch": 0.04712301587301587,
      "grad_norm": 47.28031539916992,
      "learning_rate": 1.9058090828924164e-05,
      "loss": 0.1298,
      "step": 1710
    },
    {
      "epoch": 0.04739858906525573,
      "grad_norm": 33.609832763671875,
      "learning_rate": 1.9052579365079366e-05,
      "loss": 0.2565,
      "step": 1720
    },
    {
      "epoch": 0.04767416225749559,
      "grad_norm": 7.093606472015381,
      "learning_rate": 1.904706790123457e-05,
      "loss": 0.1334,
      "step": 1730
    },
    {
      "epoch": 0.04794973544973545,
      "grad_norm": 11.730360984802246,
      "learning_rate": 1.9041556437389773e-05,
      "loss": 0.1727,
      "step": 1740
    },
    {
      "epoch": 0.04822530864197531,
      "grad_norm": 0.3977496325969696,
      "learning_rate": 1.9036044973544978e-05,
      "loss": 0.1239,
      "step": 1750
    },
    {
      "epoch": 0.048500881834215165,
      "grad_norm": 0.6567578911781311,
      "learning_rate": 1.903053350970018e-05,
      "loss": 0.1753,
      "step": 1760
    },
    {
      "epoch": 0.04877645502645503,
      "grad_norm": 27.3704833984375,
      "learning_rate": 1.902502204585538e-05,
      "loss": 0.4268,
      "step": 1770
    },
    {
      "epoch": 0.049052028218694886,
      "grad_norm": 0.5320053100585938,
      "learning_rate": 1.9019510582010582e-05,
      "loss": 0.0968,
      "step": 1780
    },
    {
      "epoch": 0.04932760141093474,
      "grad_norm": 23.98048973083496,
      "learning_rate": 1.9013999118165787e-05,
      "loss": 0.2244,
      "step": 1790
    },
    {
      "epoch": 0.0496031746031746,
      "grad_norm": 1.0803968906402588,
      "learning_rate": 1.900848765432099e-05,
      "loss": 0.267,
      "step": 1800
    },
    {
      "epoch": 0.049878747795414465,
      "grad_norm": 70.95984649658203,
      "learning_rate": 1.9002976190476194e-05,
      "loss": 0.2551,
      "step": 1810
    },
    {
      "epoch": 0.05015432098765432,
      "grad_norm": 16.553377151489258,
      "learning_rate": 1.8997464726631395e-05,
      "loss": 0.2813,
      "step": 1820
    },
    {
      "epoch": 0.05042989417989418,
      "grad_norm": 0.3598279654979706,
      "learning_rate": 1.8991953262786597e-05,
      "loss": 0.2318,
      "step": 1830
    },
    {
      "epoch": 0.050705467372134036,
      "grad_norm": 43.388580322265625,
      "learning_rate": 1.8986441798941802e-05,
      "loss": 0.2214,
      "step": 1840
    },
    {
      "epoch": 0.0509810405643739,
      "grad_norm": 8.801946640014648,
      "learning_rate": 1.8980930335097003e-05,
      "loss": 0.2122,
      "step": 1850
    },
    {
      "epoch": 0.05125661375661376,
      "grad_norm": 0.3684004247188568,
      "learning_rate": 1.8975418871252208e-05,
      "loss": 0.219,
      "step": 1860
    },
    {
      "epoch": 0.051532186948853614,
      "grad_norm": 1.8836287260055542,
      "learning_rate": 1.896990740740741e-05,
      "loss": 0.1796,
      "step": 1870
    },
    {
      "epoch": 0.05180776014109347,
      "grad_norm": 42.916046142578125,
      "learning_rate": 1.896439594356261e-05,
      "loss": 0.1524,
      "step": 1880
    },
    {
      "epoch": 0.052083333333333336,
      "grad_norm": 0.24282263219356537,
      "learning_rate": 1.8958884479717813e-05,
      "loss": 0.3035,
      "step": 1890
    },
    {
      "epoch": 0.05235890652557319,
      "grad_norm": 25.252323150634766,
      "learning_rate": 1.8953373015873018e-05,
      "loss": 0.2401,
      "step": 1900
    },
    {
      "epoch": 0.05263447971781305,
      "grad_norm": 18.699186325073242,
      "learning_rate": 1.894786155202822e-05,
      "loss": 0.2713,
      "step": 1910
    },
    {
      "epoch": 0.05291005291005291,
      "grad_norm": 2.3221075534820557,
      "learning_rate": 1.8942350088183424e-05,
      "loss": 0.1997,
      "step": 1920
    },
    {
      "epoch": 0.05318562610229277,
      "grad_norm": 41.55318069458008,
      "learning_rate": 1.8936838624338626e-05,
      "loss": 0.1571,
      "step": 1930
    },
    {
      "epoch": 0.05346119929453263,
      "grad_norm": 3.1211326122283936,
      "learning_rate": 1.8931327160493828e-05,
      "loss": 0.3257,
      "step": 1940
    },
    {
      "epoch": 0.053736772486772486,
      "grad_norm": 44.57716369628906,
      "learning_rate": 1.8925815696649033e-05,
      "loss": 0.2493,
      "step": 1950
    },
    {
      "epoch": 0.05401234567901234,
      "grad_norm": 0.45030975341796875,
      "learning_rate": 1.8920304232804234e-05,
      "loss": 0.2418,
      "step": 1960
    },
    {
      "epoch": 0.05428791887125221,
      "grad_norm": 15.573121070861816,
      "learning_rate": 1.891479276895944e-05,
      "loss": 0.284,
      "step": 1970
    },
    {
      "epoch": 0.054563492063492064,
      "grad_norm": 14.290651321411133,
      "learning_rate": 1.890928130511464e-05,
      "loss": 0.3319,
      "step": 1980
    },
    {
      "epoch": 0.05483906525573192,
      "grad_norm": 46.863216400146484,
      "learning_rate": 1.8903769841269842e-05,
      "loss": 0.1672,
      "step": 1990
    },
    {
      "epoch": 0.05511463844797178,
      "grad_norm": 73.4308090209961,
      "learning_rate": 1.8898258377425044e-05,
      "loss": 0.3384,
      "step": 2000
    },
    {
      "epoch": 0.05539021164021164,
      "grad_norm": 0.47265490889549255,
      "learning_rate": 1.889274691358025e-05,
      "loss": 0.2314,
      "step": 2010
    },
    {
      "epoch": 0.0556657848324515,
      "grad_norm": 43.7544059753418,
      "learning_rate": 1.888723544973545e-05,
      "loss": 0.1855,
      "step": 2020
    },
    {
      "epoch": 0.05594135802469136,
      "grad_norm": 6.919421195983887,
      "learning_rate": 1.8881723985890655e-05,
      "loss": 0.1945,
      "step": 2030
    },
    {
      "epoch": 0.056216931216931214,
      "grad_norm": 29.96140480041504,
      "learning_rate": 1.8876212522045857e-05,
      "loss": 0.3397,
      "step": 2040
    },
    {
      "epoch": 0.05649250440917108,
      "grad_norm": 1.7055675983428955,
      "learning_rate": 1.887070105820106e-05,
      "loss": 0.2147,
      "step": 2050
    },
    {
      "epoch": 0.056768077601410935,
      "grad_norm": 0.30689558386802673,
      "learning_rate": 1.8865189594356263e-05,
      "loss": 0.1426,
      "step": 2060
    },
    {
      "epoch": 0.05704365079365079,
      "grad_norm": 7.410424709320068,
      "learning_rate": 1.8859678130511465e-05,
      "loss": 0.1144,
      "step": 2070
    },
    {
      "epoch": 0.05731922398589065,
      "grad_norm": 37.73258972167969,
      "learning_rate": 1.885416666666667e-05,
      "loss": 0.1654,
      "step": 2080
    },
    {
      "epoch": 0.057594797178130513,
      "grad_norm": 36.549224853515625,
      "learning_rate": 1.884865520282187e-05,
      "loss": 0.2052,
      "step": 2090
    },
    {
      "epoch": 0.05787037037037037,
      "grad_norm": 6.201378345489502,
      "learning_rate": 1.8843143738977073e-05,
      "loss": 0.1936,
      "step": 2100
    },
    {
      "epoch": 0.05814594356261023,
      "grad_norm": 104.96640014648438,
      "learning_rate": 1.8837632275132275e-05,
      "loss": 0.2214,
      "step": 2110
    },
    {
      "epoch": 0.058421516754850085,
      "grad_norm": 0.24604348838329315,
      "learning_rate": 1.883212081128748e-05,
      "loss": 0.207,
      "step": 2120
    },
    {
      "epoch": 0.05869708994708995,
      "grad_norm": 38.5236701965332,
      "learning_rate": 1.882660934744268e-05,
      "loss": 0.1464,
      "step": 2130
    },
    {
      "epoch": 0.058972663139329806,
      "grad_norm": 1.1965522766113281,
      "learning_rate": 1.8821097883597886e-05,
      "loss": 0.1129,
      "step": 2140
    },
    {
      "epoch": 0.05924823633156966,
      "grad_norm": 0.206634059548378,
      "learning_rate": 1.8815586419753088e-05,
      "loss": 0.1876,
      "step": 2150
    },
    {
      "epoch": 0.05952380952380952,
      "grad_norm": 0.39124757051467896,
      "learning_rate": 1.881007495590829e-05,
      "loss": 0.2002,
      "step": 2160
    },
    {
      "epoch": 0.059799382716049385,
      "grad_norm": 29.7854061126709,
      "learning_rate": 1.8804563492063494e-05,
      "loss": 0.302,
      "step": 2170
    },
    {
      "epoch": 0.06007495590828924,
      "grad_norm": 1.0434963703155518,
      "learning_rate": 1.8799052028218696e-05,
      "loss": 0.2454,
      "step": 2180
    },
    {
      "epoch": 0.0603505291005291,
      "grad_norm": 1.0935965776443481,
      "learning_rate": 1.87935405643739e-05,
      "loss": 0.19,
      "step": 2190
    },
    {
      "epoch": 0.060626102292768956,
      "grad_norm": 0.9026768207550049,
      "learning_rate": 1.8788029100529102e-05,
      "loss": 0.1609,
      "step": 2200
    },
    {
      "epoch": 0.06090167548500882,
      "grad_norm": 34.245113372802734,
      "learning_rate": 1.8782517636684304e-05,
      "loss": 0.1418,
      "step": 2210
    },
    {
      "epoch": 0.06117724867724868,
      "grad_norm": 25.130319595336914,
      "learning_rate": 1.877700617283951e-05,
      "loss": 0.4291,
      "step": 2220
    },
    {
      "epoch": 0.061452821869488534,
      "grad_norm": 2.76163911819458,
      "learning_rate": 1.877149470899471e-05,
      "loss": 0.3448,
      "step": 2230
    },
    {
      "epoch": 0.06172839506172839,
      "grad_norm": 0.592012345790863,
      "learning_rate": 1.8765983245149912e-05,
      "loss": 0.1888,
      "step": 2240
    },
    {
      "epoch": 0.062003968253968256,
      "grad_norm": 0.19713877141475677,
      "learning_rate": 1.8760471781305117e-05,
      "loss": 0.1251,
      "step": 2250
    },
    {
      "epoch": 0.06227954144620811,
      "grad_norm": 0.3466433882713318,
      "learning_rate": 1.875496031746032e-05,
      "loss": 0.1052,
      "step": 2260
    },
    {
      "epoch": 0.06255511463844797,
      "grad_norm": 17.668928146362305,
      "learning_rate": 1.8749448853615523e-05,
      "loss": 0.1236,
      "step": 2270
    },
    {
      "epoch": 0.06283068783068783,
      "grad_norm": 0.48988109827041626,
      "learning_rate": 1.8743937389770725e-05,
      "loss": 0.2378,
      "step": 2280
    },
    {
      "epoch": 0.06310626102292768,
      "grad_norm": 0.7194238305091858,
      "learning_rate": 1.8738425925925926e-05,
      "loss": 0.2238,
      "step": 2290
    },
    {
      "epoch": 0.06338183421516755,
      "grad_norm": 54.45978546142578,
      "learning_rate": 1.873291446208113e-05,
      "loss": 0.2062,
      "step": 2300
    },
    {
      "epoch": 0.06365740740740741,
      "grad_norm": 2.097212314605713,
      "learning_rate": 1.8727402998236333e-05,
      "loss": 0.1087,
      "step": 2310
    },
    {
      "epoch": 0.06393298059964726,
      "grad_norm": 5.154679298400879,
      "learning_rate": 1.8721891534391538e-05,
      "loss": 0.2157,
      "step": 2320
    },
    {
      "epoch": 0.06420855379188713,
      "grad_norm": 26.903053283691406,
      "learning_rate": 1.871638007054674e-05,
      "loss": 0.2651,
      "step": 2330
    },
    {
      "epoch": 0.06448412698412699,
      "grad_norm": 28.394590377807617,
      "learning_rate": 1.871086860670194e-05,
      "loss": 0.3069,
      "step": 2340
    },
    {
      "epoch": 0.06475970017636684,
      "grad_norm": 0.17563070356845856,
      "learning_rate": 1.8705357142857143e-05,
      "loss": 0.1839,
      "step": 2350
    },
    {
      "epoch": 0.0650352733686067,
      "grad_norm": 6.826104640960693,
      "learning_rate": 1.8699845679012348e-05,
      "loss": 0.235,
      "step": 2360
    },
    {
      "epoch": 0.06531084656084656,
      "grad_norm": 33.42930603027344,
      "learning_rate": 1.8694334215167552e-05,
      "loss": 0.0702,
      "step": 2370
    },
    {
      "epoch": 0.06558641975308642,
      "grad_norm": 10.408428192138672,
      "learning_rate": 1.8688822751322754e-05,
      "loss": 0.3272,
      "step": 2380
    },
    {
      "epoch": 0.06586199294532628,
      "grad_norm": 47.43098068237305,
      "learning_rate": 1.8683311287477956e-05,
      "loss": 0.1557,
      "step": 2390
    },
    {
      "epoch": 0.06613756613756613,
      "grad_norm": 1.4278661012649536,
      "learning_rate": 1.8677799823633157e-05,
      "loss": 0.1693,
      "step": 2400
    },
    {
      "epoch": 0.066413139329806,
      "grad_norm": 0.6952769160270691,
      "learning_rate": 1.8672288359788362e-05,
      "loss": 0.172,
      "step": 2410
    },
    {
      "epoch": 0.06668871252204586,
      "grad_norm": 0.28000038862228394,
      "learning_rate": 1.8666776895943564e-05,
      "loss": 0.056,
      "step": 2420
    },
    {
      "epoch": 0.06696428571428571,
      "grad_norm": 7.431087017059326,
      "learning_rate": 1.866126543209877e-05,
      "loss": 0.212,
      "step": 2430
    },
    {
      "epoch": 0.06723985890652558,
      "grad_norm": 0.32171061635017395,
      "learning_rate": 1.865575396825397e-05,
      "loss": 0.2056,
      "step": 2440
    },
    {
      "epoch": 0.06751543209876543,
      "grad_norm": 99.2706527709961,
      "learning_rate": 1.8650242504409172e-05,
      "loss": 0.23,
      "step": 2450
    },
    {
      "epoch": 0.06779100529100529,
      "grad_norm": 0.9646008610725403,
      "learning_rate": 1.8644731040564373e-05,
      "loss": 0.2076,
      "step": 2460
    },
    {
      "epoch": 0.06806657848324515,
      "grad_norm": 4.748296737670898,
      "learning_rate": 1.8639219576719578e-05,
      "loss": 0.2462,
      "step": 2470
    },
    {
      "epoch": 0.068342151675485,
      "grad_norm": 43.84990692138672,
      "learning_rate": 1.8633708112874783e-05,
      "loss": 0.2863,
      "step": 2480
    },
    {
      "epoch": 0.06861772486772487,
      "grad_norm": 0.1561594158411026,
      "learning_rate": 1.8628196649029985e-05,
      "loss": 0.1291,
      "step": 2490
    },
    {
      "epoch": 0.06889329805996473,
      "grad_norm": 0.6018375754356384,
      "learning_rate": 1.8622685185185186e-05,
      "loss": 0.1949,
      "step": 2500
    },
    {
      "epoch": 0.06916887125220458,
      "grad_norm": 52.28816223144531,
      "learning_rate": 1.8617173721340388e-05,
      "loss": 0.3449,
      "step": 2510
    },
    {
      "epoch": 0.06944444444444445,
      "grad_norm": 64.56511688232422,
      "learning_rate": 1.8611662257495593e-05,
      "loss": 0.323,
      "step": 2520
    },
    {
      "epoch": 0.0697200176366843,
      "grad_norm": 0.882606029510498,
      "learning_rate": 1.8606150793650794e-05,
      "loss": 0.0527,
      "step": 2530
    },
    {
      "epoch": 0.06999559082892416,
      "grad_norm": 44.90903854370117,
      "learning_rate": 1.8600639329806e-05,
      "loss": 0.2685,
      "step": 2540
    },
    {
      "epoch": 0.07027116402116403,
      "grad_norm": 51.20518112182617,
      "learning_rate": 1.85951278659612e-05,
      "loss": 0.2505,
      "step": 2550
    },
    {
      "epoch": 0.07054673721340388,
      "grad_norm": 8.052008628845215,
      "learning_rate": 1.8589616402116403e-05,
      "loss": 0.1343,
      "step": 2560
    },
    {
      "epoch": 0.07082231040564374,
      "grad_norm": 2.3945960998535156,
      "learning_rate": 1.8584104938271604e-05,
      "loss": 0.1772,
      "step": 2570
    },
    {
      "epoch": 0.0710978835978836,
      "grad_norm": 36.431678771972656,
      "learning_rate": 1.857859347442681e-05,
      "loss": 0.1884,
      "step": 2580
    },
    {
      "epoch": 0.07137345679012345,
      "grad_norm": 0.16706429421901703,
      "learning_rate": 1.8573082010582014e-05,
      "loss": 0.0674,
      "step": 2590
    },
    {
      "epoch": 0.07164902998236332,
      "grad_norm": 14.255938529968262,
      "learning_rate": 1.8567570546737216e-05,
      "loss": 0.2724,
      "step": 2600
    },
    {
      "epoch": 0.07192460317460317,
      "grad_norm": 57.82595443725586,
      "learning_rate": 1.8562059082892417e-05,
      "loss": 0.3472,
      "step": 2610
    },
    {
      "epoch": 0.07220017636684303,
      "grad_norm": 62.636810302734375,
      "learning_rate": 1.855654761904762e-05,
      "loss": 0.2196,
      "step": 2620
    },
    {
      "epoch": 0.0724757495590829,
      "grad_norm": 0.38798919320106506,
      "learning_rate": 1.8551036155202824e-05,
      "loss": 0.1382,
      "step": 2630
    },
    {
      "epoch": 0.07275132275132275,
      "grad_norm": 59.348812103271484,
      "learning_rate": 1.8545524691358025e-05,
      "loss": 0.27,
      "step": 2640
    },
    {
      "epoch": 0.07302689594356261,
      "grad_norm": 49.37019729614258,
      "learning_rate": 1.854001322751323e-05,
      "loss": 0.2485,
      "step": 2650
    },
    {
      "epoch": 0.07330246913580248,
      "grad_norm": 0.255877286195755,
      "learning_rate": 1.8534501763668432e-05,
      "loss": 0.0862,
      "step": 2660
    },
    {
      "epoch": 0.07357804232804233,
      "grad_norm": 1.9248933792114258,
      "learning_rate": 1.8528990299823633e-05,
      "loss": 0.2023,
      "step": 2670
    },
    {
      "epoch": 0.07385361552028219,
      "grad_norm": 7.17033052444458,
      "learning_rate": 1.8523478835978835e-05,
      "loss": 0.2215,
      "step": 2680
    },
    {
      "epoch": 0.07412918871252204,
      "grad_norm": 6.988349914550781,
      "learning_rate": 1.851796737213404e-05,
      "loss": 0.3375,
      "step": 2690
    },
    {
      "epoch": 0.0744047619047619,
      "grad_norm": 0.12648801505565643,
      "learning_rate": 1.8512455908289245e-05,
      "loss": 0.3792,
      "step": 2700
    },
    {
      "epoch": 0.07468033509700177,
      "grad_norm": 0.5399911999702454,
      "learning_rate": 1.8506944444444446e-05,
      "loss": 0.1773,
      "step": 2710
    },
    {
      "epoch": 0.07495590828924162,
      "grad_norm": 8.304300308227539,
      "learning_rate": 1.8501432980599648e-05,
      "loss": 0.2536,
      "step": 2720
    },
    {
      "epoch": 0.07523148148148148,
      "grad_norm": 0.25068312883377075,
      "learning_rate": 1.8495921516754853e-05,
      "loss": 0.2328,
      "step": 2730
    },
    {
      "epoch": 0.07550705467372135,
      "grad_norm": 0.10765332728624344,
      "learning_rate": 1.8490410052910054e-05,
      "loss": 0.2046,
      "step": 2740
    },
    {
      "epoch": 0.0757826278659612,
      "grad_norm": 31.23480796813965,
      "learning_rate": 1.8484898589065256e-05,
      "loss": 0.2129,
      "step": 2750
    },
    {
      "epoch": 0.07605820105820106,
      "grad_norm": 0.11938439309597015,
      "learning_rate": 1.847938712522046e-05,
      "loss": 0.181,
      "step": 2760
    },
    {
      "epoch": 0.07633377425044091,
      "grad_norm": 3.3480966091156006,
      "learning_rate": 1.8473875661375662e-05,
      "loss": 0.1278,
      "step": 2770
    },
    {
      "epoch": 0.07660934744268078,
      "grad_norm": 1.8353363275527954,
      "learning_rate": 1.8468364197530867e-05,
      "loss": 0.3719,
      "step": 2780
    },
    {
      "epoch": 0.07688492063492064,
      "grad_norm": 0.2892610728740692,
      "learning_rate": 1.846285273368607e-05,
      "loss": 0.1993,
      "step": 2790
    },
    {
      "epoch": 0.07716049382716049,
      "grad_norm": 0.12935775518417358,
      "learning_rate": 1.845734126984127e-05,
      "loss": 0.1885,
      "step": 2800
    },
    {
      "epoch": 0.07743606701940035,
      "grad_norm": 20.33839988708496,
      "learning_rate": 1.8451829805996476e-05,
      "loss": 0.208,
      "step": 2810
    },
    {
      "epoch": 0.07771164021164022,
      "grad_norm": 1.786046028137207,
      "learning_rate": 1.8446318342151677e-05,
      "loss": 0.2342,
      "step": 2820
    },
    {
      "epoch": 0.07798721340388007,
      "grad_norm": 73.0835189819336,
      "learning_rate": 1.8440806878306882e-05,
      "loss": 0.1266,
      "step": 2830
    },
    {
      "epoch": 0.07826278659611993,
      "grad_norm": 15.634725570678711,
      "learning_rate": 1.8435295414462084e-05,
      "loss": 0.2692,
      "step": 2840
    },
    {
      "epoch": 0.07853835978835978,
      "grad_norm": 0.14091943204402924,
      "learning_rate": 1.8429783950617285e-05,
      "loss": 0.4451,
      "step": 2850
    },
    {
      "epoch": 0.07881393298059965,
      "grad_norm": 12.723177909851074,
      "learning_rate": 1.8424272486772487e-05,
      "loss": 0.3267,
      "step": 2860
    },
    {
      "epoch": 0.07908950617283951,
      "grad_norm": 35.13154983520508,
      "learning_rate": 1.841876102292769e-05,
      "loss": 0.2178,
      "step": 2870
    },
    {
      "epoch": 0.07936507936507936,
      "grad_norm": 67.70555114746094,
      "learning_rate": 1.8413249559082893e-05,
      "loss": 0.1789,
      "step": 2880
    },
    {
      "epoch": 0.07964065255731922,
      "grad_norm": 57.628807067871094,
      "learning_rate": 1.8407738095238098e-05,
      "loss": 0.1873,
      "step": 2890
    },
    {
      "epoch": 0.07991622574955909,
      "grad_norm": 5.027241230010986,
      "learning_rate": 1.84022266313933e-05,
      "loss": 0.2881,
      "step": 2900
    },
    {
      "epoch": 0.08019179894179894,
      "grad_norm": 9.294960021972656,
      "learning_rate": 1.83967151675485e-05,
      "loss": 0.2954,
      "step": 2910
    },
    {
      "epoch": 0.0804673721340388,
      "grad_norm": 0.28295227885246277,
      "learning_rate": 1.8391203703703706e-05,
      "loss": 0.0567,
      "step": 2920
    },
    {
      "epoch": 0.08074294532627865,
      "grad_norm": 28.96562385559082,
      "learning_rate": 1.8385692239858908e-05,
      "loss": 0.2004,
      "step": 2930
    },
    {
      "epoch": 0.08101851851851852,
      "grad_norm": 14.96462345123291,
      "learning_rate": 1.8380180776014113e-05,
      "loss": 0.2287,
      "step": 2940
    },
    {
      "epoch": 0.08129409171075838,
      "grad_norm": 67.22220611572266,
      "learning_rate": 1.8374669312169314e-05,
      "loss": 0.2678,
      "step": 2950
    },
    {
      "epoch": 0.08156966490299823,
      "grad_norm": 6.902585983276367,
      "learning_rate": 1.8369157848324516e-05,
      "loss": 0.1748,
      "step": 2960
    },
    {
      "epoch": 0.0818452380952381,
      "grad_norm": 35.79216766357422,
      "learning_rate": 1.8363646384479717e-05,
      "loss": 0.08,
      "step": 2970
    },
    {
      "epoch": 0.08212081128747796,
      "grad_norm": 2.745176076889038,
      "learning_rate": 1.8358134920634922e-05,
      "loss": 0.2066,
      "step": 2980
    },
    {
      "epoch": 0.08239638447971781,
      "grad_norm": 73.54154205322266,
      "learning_rate": 1.8352623456790124e-05,
      "loss": 0.2269,
      "step": 2990
    },
    {
      "epoch": 0.08267195767195767,
      "grad_norm": 0.11582885682582855,
      "learning_rate": 1.834711199294533e-05,
      "loss": 0.1235,
      "step": 3000
    },
    {
      "epoch": 0.08294753086419752,
      "grad_norm": 54.74040222167969,
      "learning_rate": 1.834160052910053e-05,
      "loss": 0.1068,
      "step": 3010
    },
    {
      "epoch": 0.08322310405643739,
      "grad_norm": 21.584842681884766,
      "learning_rate": 1.8336089065255732e-05,
      "loss": 0.1317,
      "step": 3020
    },
    {
      "epoch": 0.08349867724867725,
      "grad_norm": 2.1178202629089355,
      "learning_rate": 1.8330577601410937e-05,
      "loss": 0.3204,
      "step": 3030
    },
    {
      "epoch": 0.0837742504409171,
      "grad_norm": 12.60631275177002,
      "learning_rate": 1.832506613756614e-05,
      "loss": 0.2807,
      "step": 3040
    },
    {
      "epoch": 0.08404982363315697,
      "grad_norm": 29.679908752441406,
      "learning_rate": 1.8319554673721344e-05,
      "loss": 0.1424,
      "step": 3050
    },
    {
      "epoch": 0.08432539682539683,
      "grad_norm": 40.006038665771484,
      "learning_rate": 1.8314043209876545e-05,
      "loss": 0.2347,
      "step": 3060
    },
    {
      "epoch": 0.08460097001763668,
      "grad_norm": 20.245948791503906,
      "learning_rate": 1.8308531746031747e-05,
      "loss": 0.4215,
      "step": 3070
    },
    {
      "epoch": 0.08487654320987655,
      "grad_norm": 4.3345465660095215,
      "learning_rate": 1.8303020282186948e-05,
      "loss": 0.2496,
      "step": 3080
    },
    {
      "epoch": 0.0851521164021164,
      "grad_norm": 45.60309600830078,
      "learning_rate": 1.8297508818342153e-05,
      "loss": 0.1314,
      "step": 3090
    },
    {
      "epoch": 0.08542768959435626,
      "grad_norm": 42.59995651245117,
      "learning_rate": 1.8291997354497355e-05,
      "loss": 0.16,
      "step": 3100
    },
    {
      "epoch": 0.08570326278659612,
      "grad_norm": 0.12178260087966919,
      "learning_rate": 1.828648589065256e-05,
      "loss": 0.1891,
      "step": 3110
    },
    {
      "epoch": 0.08597883597883597,
      "grad_norm": 0.12879863381385803,
      "learning_rate": 1.828097442680776e-05,
      "loss": 0.2361,
      "step": 3120
    },
    {
      "epoch": 0.08625440917107584,
      "grad_norm": 0.32107454538345337,
      "learning_rate": 1.8275462962962963e-05,
      "loss": 0.4027,
      "step": 3130
    },
    {
      "epoch": 0.0865299823633157,
      "grad_norm": 30.538114547729492,
      "learning_rate": 1.8269951499118168e-05,
      "loss": 0.086,
      "step": 3140
    },
    {
      "epoch": 0.08680555555555555,
      "grad_norm": 57.69131851196289,
      "learning_rate": 1.826444003527337e-05,
      "loss": 0.1081,
      "step": 3150
    },
    {
      "epoch": 0.08708112874779542,
      "grad_norm": 56.855796813964844,
      "learning_rate": 1.8258928571428574e-05,
      "loss": 0.2345,
      "step": 3160
    },
    {
      "epoch": 0.08735670194003527,
      "grad_norm": 0.32127854228019714,
      "learning_rate": 1.8253417107583776e-05,
      "loss": 0.1288,
      "step": 3170
    },
    {
      "epoch": 0.08763227513227513,
      "grad_norm": 0.12027064710855484,
      "learning_rate": 1.8247905643738977e-05,
      "loss": 0.1294,
      "step": 3180
    },
    {
      "epoch": 0.087907848324515,
      "grad_norm": 3.3105719089508057,
      "learning_rate": 1.824239417989418e-05,
      "loss": 0.2237,
      "step": 3190
    },
    {
      "epoch": 0.08818342151675485,
      "grad_norm": 34.92464065551758,
      "learning_rate": 1.8236882716049384e-05,
      "loss": 0.0502,
      "step": 3200
    },
    {
      "epoch": 0.08845899470899471,
      "grad_norm": 0.5621853470802307,
      "learning_rate": 1.8231371252204586e-05,
      "loss": 0.246,
      "step": 3210
    },
    {
      "epoch": 0.08873456790123457,
      "grad_norm": 80.90641021728516,
      "learning_rate": 1.822585978835979e-05,
      "loss": 0.189,
      "step": 3220
    },
    {
      "epoch": 0.08901014109347442,
      "grad_norm": 44.388423919677734,
      "learning_rate": 1.8220348324514992e-05,
      "loss": 0.2273,
      "step": 3230
    },
    {
      "epoch": 0.08928571428571429,
      "grad_norm": 0.9204140901565552,
      "learning_rate": 1.8214836860670197e-05,
      "loss": 0.2104,
      "step": 3240
    },
    {
      "epoch": 0.08956128747795414,
      "grad_norm": 35.802181243896484,
      "learning_rate": 1.82093253968254e-05,
      "loss": 0.2754,
      "step": 3250
    },
    {
      "epoch": 0.089836860670194,
      "grad_norm": 25.80173110961914,
      "learning_rate": 1.82038139329806e-05,
      "loss": 0.4493,
      "step": 3260
    },
    {
      "epoch": 0.09011243386243387,
      "grad_norm": 83.26622772216797,
      "learning_rate": 1.8198302469135805e-05,
      "loss": 0.318,
      "step": 3270
    },
    {
      "epoch": 0.09038800705467372,
      "grad_norm": 0.19201481342315674,
      "learning_rate": 1.8192791005291007e-05,
      "loss": 0.1957,
      "step": 3280
    },
    {
      "epoch": 0.09066358024691358,
      "grad_norm": 0.5593536496162415,
      "learning_rate": 1.818727954144621e-05,
      "loss": 0.1467,
      "step": 3290
    },
    {
      "epoch": 0.09093915343915344,
      "grad_norm": 11.520792007446289,
      "learning_rate": 1.8181768077601413e-05,
      "loss": 0.2265,
      "step": 3300
    },
    {
      "epoch": 0.0912147266313933,
      "grad_norm": 26.633922576904297,
      "learning_rate": 1.8176256613756615e-05,
      "loss": 0.3406,
      "step": 3310
    },
    {
      "epoch": 0.09149029982363316,
      "grad_norm": 0.12343878298997879,
      "learning_rate": 1.8170745149911816e-05,
      "loss": 0.1546,
      "step": 3320
    },
    {
      "epoch": 0.09176587301587301,
      "grad_norm": 67.26316833496094,
      "learning_rate": 1.816523368606702e-05,
      "loss": 0.2535,
      "step": 3330
    },
    {
      "epoch": 0.09204144620811287,
      "grad_norm": 2.7387921810150146,
      "learning_rate": 1.8159722222222226e-05,
      "loss": 0.1831,
      "step": 3340
    },
    {
      "epoch": 0.09231701940035274,
      "grad_norm": 0.14654745161533356,
      "learning_rate": 1.8154210758377428e-05,
      "loss": 0.185,
      "step": 3350
    },
    {
      "epoch": 0.09259259259259259,
      "grad_norm": 20.712970733642578,
      "learning_rate": 1.814869929453263e-05,
      "loss": 0.1877,
      "step": 3360
    },
    {
      "epoch": 0.09286816578483245,
      "grad_norm": 49.0997428894043,
      "learning_rate": 1.814318783068783e-05,
      "loss": 0.1067,
      "step": 3370
    },
    {
      "epoch": 0.09314373897707232,
      "grad_norm": 0.2535701096057892,
      "learning_rate": 1.8137676366843036e-05,
      "loss": 0.214,
      "step": 3380
    },
    {
      "epoch": 0.09341931216931217,
      "grad_norm": 4.482967853546143,
      "learning_rate": 1.8132164902998237e-05,
      "loss": 0.2131,
      "step": 3390
    },
    {
      "epoch": 0.09369488536155203,
      "grad_norm": 0.3704380989074707,
      "learning_rate": 1.8126653439153442e-05,
      "loss": 0.2623,
      "step": 3400
    },
    {
      "epoch": 0.0939704585537919,
      "grad_norm": 0.11531345546245575,
      "learning_rate": 1.8121141975308644e-05,
      "loss": 0.2345,
      "step": 3410
    },
    {
      "epoch": 0.09424603174603174,
      "grad_norm": 57.436954498291016,
      "learning_rate": 1.8115630511463845e-05,
      "loss": 0.2504,
      "step": 3420
    },
    {
      "epoch": 0.09452160493827161,
      "grad_norm": 89.49627685546875,
      "learning_rate": 1.8110119047619047e-05,
      "loss": 0.1529,
      "step": 3430
    },
    {
      "epoch": 0.09479717813051146,
      "grad_norm": 2.4122447967529297,
      "learning_rate": 1.8104607583774252e-05,
      "loss": 0.1934,
      "step": 3440
    },
    {
      "epoch": 0.09507275132275132,
      "grad_norm": 0.2203831672668457,
      "learning_rate": 1.8099096119929457e-05,
      "loss": 0.1783,
      "step": 3450
    },
    {
      "epoch": 0.09534832451499119,
      "grad_norm": 62.51273727416992,
      "learning_rate": 1.809358465608466e-05,
      "loss": 0.2882,
      "step": 3460
    },
    {
      "epoch": 0.09562389770723104,
      "grad_norm": 49.123268127441406,
      "learning_rate": 1.808807319223986e-05,
      "loss": 0.3666,
      "step": 3470
    },
    {
      "epoch": 0.0958994708994709,
      "grad_norm": 67.26117706298828,
      "learning_rate": 1.808256172839506e-05,
      "loss": 0.2071,
      "step": 3480
    },
    {
      "epoch": 0.09617504409171077,
      "grad_norm": 0.48074251413345337,
      "learning_rate": 1.8077050264550267e-05,
      "loss": 0.0972,
      "step": 3490
    },
    {
      "epoch": 0.09645061728395062,
      "grad_norm": 1.5804921388626099,
      "learning_rate": 1.8071538800705468e-05,
      "loss": 0.1702,
      "step": 3500
    },
    {
      "epoch": 0.09672619047619048,
      "grad_norm": 38.144004821777344,
      "learning_rate": 1.8066027336860673e-05,
      "loss": 0.0875,
      "step": 3510
    },
    {
      "epoch": 0.09700176366843033,
      "grad_norm": 0.39157044887542725,
      "learning_rate": 1.8060515873015875e-05,
      "loss": 0.0219,
      "step": 3520
    },
    {
      "epoch": 0.0972773368606702,
      "grad_norm": 0.4275343120098114,
      "learning_rate": 1.8055004409171076e-05,
      "loss": 0.1182,
      "step": 3530
    },
    {
      "epoch": 0.09755291005291006,
      "grad_norm": 42.88633346557617,
      "learning_rate": 1.8049492945326278e-05,
      "loss": 0.4105,
      "step": 3540
    },
    {
      "epoch": 0.09782848324514991,
      "grad_norm": 0.778397798538208,
      "learning_rate": 1.8043981481481483e-05,
      "loss": 0.1528,
      "step": 3550
    },
    {
      "epoch": 0.09810405643738977,
      "grad_norm": 43.31858825683594,
      "learning_rate": 1.8038470017636688e-05,
      "loss": 0.2296,
      "step": 3560
    },
    {
      "epoch": 0.09837962962962964,
      "grad_norm": 7.9855122566223145,
      "learning_rate": 1.803295855379189e-05,
      "loss": 0.4242,
      "step": 3570
    },
    {
      "epoch": 0.09865520282186949,
      "grad_norm": 0.37469130754470825,
      "learning_rate": 1.802744708994709e-05,
      "loss": 0.1227,
      "step": 3580
    },
    {
      "epoch": 0.09893077601410935,
      "grad_norm": 1.8729361295700073,
      "learning_rate": 1.8021935626102292e-05,
      "loss": 0.2622,
      "step": 3590
    },
    {
      "epoch": 0.0992063492063492,
      "grad_norm": 20.040843963623047,
      "learning_rate": 1.8016424162257497e-05,
      "loss": 0.2853,
      "step": 3600
    },
    {
      "epoch": 0.09948192239858906,
      "grad_norm": 0.09015028178691864,
      "learning_rate": 1.80109126984127e-05,
      "loss": 0.0997,
      "step": 3610
    },
    {
      "epoch": 0.09975749559082893,
      "grad_norm": 13.268887519836426,
      "learning_rate": 1.8005401234567904e-05,
      "loss": 0.0374,
      "step": 3620
    },
    {
      "epoch": 0.10003306878306878,
      "grad_norm": 39.576698303222656,
      "learning_rate": 1.7999889770723105e-05,
      "loss": 0.1968,
      "step": 3630
    },
    {
      "epoch": 0.10030864197530864,
      "grad_norm": 8.124246597290039,
      "learning_rate": 1.7994378306878307e-05,
      "loss": 0.0344,
      "step": 3640
    },
    {
      "epoch": 0.10058421516754851,
      "grad_norm": 0.11504032462835312,
      "learning_rate": 1.7988866843033512e-05,
      "loss": 0.0402,
      "step": 3650
    },
    {
      "epoch": 0.10085978835978836,
      "grad_norm": 32.247005462646484,
      "learning_rate": 1.7983355379188714e-05,
      "loss": 0.0862,
      "step": 3660
    },
    {
      "epoch": 0.10113536155202822,
      "grad_norm": 3.432823896408081,
      "learning_rate": 1.797784391534392e-05,
      "loss": 0.1764,
      "step": 3670
    },
    {
      "epoch": 0.10141093474426807,
      "grad_norm": 0.11742114275693893,
      "learning_rate": 1.797233245149912e-05,
      "loss": 0.0967,
      "step": 3680
    },
    {
      "epoch": 0.10168650793650794,
      "grad_norm": 15.120450019836426,
      "learning_rate": 1.796682098765432e-05,
      "loss": 0.1338,
      "step": 3690
    },
    {
      "epoch": 0.1019620811287478,
      "grad_norm": 1.6318079233169556,
      "learning_rate": 1.7961309523809523e-05,
      "loss": 0.1476,
      "step": 3700
    },
    {
      "epoch": 0.10223765432098765,
      "grad_norm": 50.8770866394043,
      "learning_rate": 1.7955798059964728e-05,
      "loss": 0.124,
      "step": 3710
    },
    {
      "epoch": 0.10251322751322751,
      "grad_norm": 80.46334075927734,
      "learning_rate": 1.795028659611993e-05,
      "loss": 0.2106,
      "step": 3720
    },
    {
      "epoch": 0.10278880070546738,
      "grad_norm": 0.1033417358994484,
      "learning_rate": 1.7944775132275135e-05,
      "loss": 0.0994,
      "step": 3730
    },
    {
      "epoch": 0.10306437389770723,
      "grad_norm": 0.3413263261318207,
      "learning_rate": 1.7939263668430336e-05,
      "loss": 0.1222,
      "step": 3740
    },
    {
      "epoch": 0.10333994708994709,
      "grad_norm": 53.37855529785156,
      "learning_rate": 1.7933752204585538e-05,
      "loss": 0.3902,
      "step": 3750
    },
    {
      "epoch": 0.10361552028218694,
      "grad_norm": 0.10204517841339111,
      "learning_rate": 1.7928240740740743e-05,
      "loss": 0.1985,
      "step": 3760
    },
    {
      "epoch": 0.10389109347442681,
      "grad_norm": 28.426132202148438,
      "learning_rate": 1.7922729276895944e-05,
      "loss": 0.2077,
      "step": 3770
    },
    {
      "epoch": 0.10416666666666667,
      "grad_norm": 19.607702255249023,
      "learning_rate": 1.791721781305115e-05,
      "loss": 0.2439,
      "step": 3780
    },
    {
      "epoch": 0.10444223985890652,
      "grad_norm": 106.56884002685547,
      "learning_rate": 1.791170634920635e-05,
      "loss": 0.2094,
      "step": 3790
    },
    {
      "epoch": 0.10471781305114639,
      "grad_norm": 0.23961271345615387,
      "learning_rate": 1.7906194885361556e-05,
      "loss": 0.0783,
      "step": 3800
    },
    {
      "epoch": 0.10499338624338625,
      "grad_norm": 86.9728775024414,
      "learning_rate": 1.7900683421516757e-05,
      "loss": 0.2386,
      "step": 3810
    },
    {
      "epoch": 0.1052689594356261,
      "grad_norm": 0.277334064245224,
      "learning_rate": 1.789517195767196e-05,
      "loss": 0.1283,
      "step": 3820
    },
    {
      "epoch": 0.10554453262786596,
      "grad_norm": 7.134130001068115,
      "learning_rate": 1.788966049382716e-05,
      "loss": 0.1096,
      "step": 3830
    },
    {
      "epoch": 0.10582010582010581,
      "grad_norm": 28.069393157958984,
      "learning_rate": 1.7884149029982365e-05,
      "loss": 0.1838,
      "step": 3840
    },
    {
      "epoch": 0.10609567901234568,
      "grad_norm": 0.14963865280151367,
      "learning_rate": 1.7878637566137567e-05,
      "loss": 0.1811,
      "step": 3850
    },
    {
      "epoch": 0.10637125220458554,
      "grad_norm": 0.29481035470962524,
      "learning_rate": 1.7873126102292772e-05,
      "loss": 0.0736,
      "step": 3860
    },
    {
      "epoch": 0.10664682539682539,
      "grad_norm": 5.7770161628723145,
      "learning_rate": 1.7867614638447973e-05,
      "loss": 0.2865,
      "step": 3870
    },
    {
      "epoch": 0.10692239858906526,
      "grad_norm": 0.15252262353897095,
      "learning_rate": 1.7862103174603175e-05,
      "loss": 0.2099,
      "step": 3880
    },
    {
      "epoch": 0.10719797178130512,
      "grad_norm": 0.11537197977304459,
      "learning_rate": 1.785659171075838e-05,
      "loss": 0.054,
      "step": 3890
    },
    {
      "epoch": 0.10747354497354497,
      "grad_norm": 32.070960998535156,
      "learning_rate": 1.785108024691358e-05,
      "loss": 0.1223,
      "step": 3900
    },
    {
      "epoch": 0.10774911816578484,
      "grad_norm": 0.12807005643844604,
      "learning_rate": 1.7845568783068787e-05,
      "loss": 0.1461,
      "step": 3910
    },
    {
      "epoch": 0.10802469135802469,
      "grad_norm": 6.165536403656006,
      "learning_rate": 1.7840057319223988e-05,
      "loss": 0.1456,
      "step": 3920
    },
    {
      "epoch": 0.10830026455026455,
      "grad_norm": 3.5778210163116455,
      "learning_rate": 1.783454585537919e-05,
      "loss": 0.2882,
      "step": 3930
    },
    {
      "epoch": 0.10857583774250441,
      "grad_norm": 0.3231477439403534,
      "learning_rate": 1.782903439153439e-05,
      "loss": 0.3,
      "step": 3940
    },
    {
      "epoch": 0.10885141093474426,
      "grad_norm": 88.06925201416016,
      "learning_rate": 1.7823522927689596e-05,
      "loss": 0.1303,
      "step": 3950
    },
    {
      "epoch": 0.10912698412698413,
      "grad_norm": 0.22430798411369324,
      "learning_rate": 1.7818011463844798e-05,
      "loss": 0.0798,
      "step": 3960
    },
    {
      "epoch": 0.10940255731922399,
      "grad_norm": 0.08064749836921692,
      "learning_rate": 1.7812500000000003e-05,
      "loss": 0.1215,
      "step": 3970
    },
    {
      "epoch": 0.10967813051146384,
      "grad_norm": 29.331207275390625,
      "learning_rate": 1.7806988536155204e-05,
      "loss": 0.2086,
      "step": 3980
    },
    {
      "epoch": 0.1099537037037037,
      "grad_norm": 0.08251390606164932,
      "learning_rate": 1.7801477072310406e-05,
      "loss": 0.2091,
      "step": 3990
    },
    {
      "epoch": 0.11022927689594356,
      "grad_norm": 0.23737074434757233,
      "learning_rate": 1.779596560846561e-05,
      "loss": 0.228,
      "step": 4000
    },
    {
      "epoch": 0.11050485008818342,
      "grad_norm": 0.2784363329410553,
      "learning_rate": 1.7790454144620812e-05,
      "loss": 0.1395,
      "step": 4010
    },
    {
      "epoch": 0.11078042328042328,
      "grad_norm": 0.24960622191429138,
      "learning_rate": 1.7784942680776017e-05,
      "loss": 0.4109,
      "step": 4020
    },
    {
      "epoch": 0.11105599647266313,
      "grad_norm": 0.09100203961133957,
      "learning_rate": 1.777943121693122e-05,
      "loss": 0.0971,
      "step": 4030
    },
    {
      "epoch": 0.111331569664903,
      "grad_norm": 0.7922073006629944,
      "learning_rate": 1.777391975308642e-05,
      "loss": 0.0895,
      "step": 4040
    },
    {
      "epoch": 0.11160714285714286,
      "grad_norm": 15.28370475769043,
      "learning_rate": 1.7768408289241622e-05,
      "loss": 0.2845,
      "step": 4050
    },
    {
      "epoch": 0.11188271604938271,
      "grad_norm": 69.65026092529297,
      "learning_rate": 1.7762896825396827e-05,
      "loss": 0.3414,
      "step": 4060
    },
    {
      "epoch": 0.11215828924162258,
      "grad_norm": 0.10047881305217743,
      "learning_rate": 1.775738536155203e-05,
      "loss": 0.2015,
      "step": 4070
    },
    {
      "epoch": 0.11243386243386243,
      "grad_norm": 58.66477966308594,
      "learning_rate": 1.7751873897707233e-05,
      "loss": 0.2329,
      "step": 4080
    },
    {
      "epoch": 0.11270943562610229,
      "grad_norm": 2.492577314376831,
      "learning_rate": 1.7746362433862435e-05,
      "loss": 0.1567,
      "step": 4090
    },
    {
      "epoch": 0.11298500881834216,
      "grad_norm": 0.09935779869556427,
      "learning_rate": 1.7740850970017637e-05,
      "loss": 0.2798,
      "step": 4100
    },
    {
      "epoch": 0.113260582010582,
      "grad_norm": 3.06193470954895,
      "learning_rate": 1.773533950617284e-05,
      "loss": 0.427,
      "step": 4110
    },
    {
      "epoch": 0.11353615520282187,
      "grad_norm": 67.37147521972656,
      "learning_rate": 1.7729828042328043e-05,
      "loss": 0.2211,
      "step": 4120
    },
    {
      "epoch": 0.11381172839506173,
      "grad_norm": 3.489244222640991,
      "learning_rate": 1.7724316578483248e-05,
      "loss": 0.2509,
      "step": 4130
    },
    {
      "epoch": 0.11408730158730158,
      "grad_norm": 0.1704804003238678,
      "learning_rate": 1.771880511463845e-05,
      "loss": 0.1505,
      "step": 4140
    },
    {
      "epoch": 0.11436287477954145,
      "grad_norm": 0.9910880327224731,
      "learning_rate": 1.771329365079365e-05,
      "loss": 0.11,
      "step": 4150
    },
    {
      "epoch": 0.1146384479717813,
      "grad_norm": 0.2788574695587158,
      "learning_rate": 1.7707782186948853e-05,
      "loss": 0.4343,
      "step": 4160
    },
    {
      "epoch": 0.11491402116402116,
      "grad_norm": 13.806835174560547,
      "learning_rate": 1.7702270723104058e-05,
      "loss": 0.2347,
      "step": 4170
    },
    {
      "epoch": 0.11518959435626103,
      "grad_norm": 81.39839172363281,
      "learning_rate": 1.769675925925926e-05,
      "loss": 0.0276,
      "step": 4180
    },
    {
      "epoch": 0.11546516754850088,
      "grad_norm": 8.886946678161621,
      "learning_rate": 1.7691247795414464e-05,
      "loss": 0.4016,
      "step": 4190
    },
    {
      "epoch": 0.11574074074074074,
      "grad_norm": 0.19550497829914093,
      "learning_rate": 1.7685736331569666e-05,
      "loss": 0.1903,
      "step": 4200
    },
    {
      "epoch": 0.1160163139329806,
      "grad_norm": 0.6572739481925964,
      "learning_rate": 1.7680224867724867e-05,
      "loss": 0.1352,
      "step": 4210
    },
    {
      "epoch": 0.11629188712522046,
      "grad_norm": 3.0730984210968018,
      "learning_rate": 1.7674713403880072e-05,
      "loss": 0.3127,
      "step": 4220
    },
    {
      "epoch": 0.11656746031746032,
      "grad_norm": 4.306893825531006,
      "learning_rate": 1.7669201940035274e-05,
      "loss": 0.0449,
      "step": 4230
    },
    {
      "epoch": 0.11684303350970017,
      "grad_norm": 0.08075127005577087,
      "learning_rate": 1.766369047619048e-05,
      "loss": 0.1079,
      "step": 4240
    },
    {
      "epoch": 0.11711860670194003,
      "grad_norm": 50.62412643432617,
      "learning_rate": 1.765817901234568e-05,
      "loss": 0.2535,
      "step": 4250
    },
    {
      "epoch": 0.1173941798941799,
      "grad_norm": 0.0976242795586586,
      "learning_rate": 1.7652667548500882e-05,
      "loss": 0.1147,
      "step": 4260
    },
    {
      "epoch": 0.11766975308641975,
      "grad_norm": 0.07135840505361557,
      "learning_rate": 1.7647156084656087e-05,
      "loss": 0.2118,
      "step": 4270
    },
    {
      "epoch": 0.11794532627865961,
      "grad_norm": 69.5555648803711,
      "learning_rate": 1.764164462081129e-05,
      "loss": 0.2114,
      "step": 4280
    },
    {
      "epoch": 0.11822089947089948,
      "grad_norm": 32.901634216308594,
      "learning_rate": 1.763613315696649e-05,
      "loss": 0.1178,
      "step": 4290
    },
    {
      "epoch": 0.11849647266313933,
      "grad_norm": 0.14552685618400574,
      "learning_rate": 1.7630621693121695e-05,
      "loss": 0.0742,
      "step": 4300
    },
    {
      "epoch": 0.11877204585537919,
      "grad_norm": 112.8969497680664,
      "learning_rate": 1.76251102292769e-05,
      "loss": 0.304,
      "step": 4310
    },
    {
      "epoch": 0.11904761904761904,
      "grad_norm": 0.10583069920539856,
      "learning_rate": 1.76195987654321e-05,
      "loss": 0.1751,
      "step": 4320
    },
    {
      "epoch": 0.1193231922398589,
      "grad_norm": 0.30170494318008423,
      "learning_rate": 1.7614087301587303e-05,
      "loss": 0.1128,
      "step": 4330
    },
    {
      "epoch": 0.11959876543209877,
      "grad_norm": 29.45062828063965,
      "learning_rate": 1.7608575837742505e-05,
      "loss": 0.303,
      "step": 4340
    },
    {
      "epoch": 0.11987433862433862,
      "grad_norm": 1.7871744632720947,
      "learning_rate": 1.760306437389771e-05,
      "loss": 0.2012,
      "step": 4350
    },
    {
      "epoch": 0.12014991181657848,
      "grad_norm": 106.54244232177734,
      "learning_rate": 1.759755291005291e-05,
      "loss": 0.1195,
      "step": 4360
    },
    {
      "epoch": 0.12042548500881835,
      "grad_norm": 30.3544864654541,
      "learning_rate": 1.7592041446208116e-05,
      "loss": 0.294,
      "step": 4370
    },
    {
      "epoch": 0.1207010582010582,
      "grad_norm": 0.13191497325897217,
      "learning_rate": 1.7586529982363318e-05,
      "loss": 0.1312,
      "step": 4380
    },
    {
      "epoch": 0.12097663139329806,
      "grad_norm": 0.06553889065980911,
      "learning_rate": 1.758101851851852e-05,
      "loss": 0.1802,
      "step": 4390
    },
    {
      "epoch": 0.12125220458553791,
      "grad_norm": 0.1376616507768631,
      "learning_rate": 1.7575507054673724e-05,
      "loss": 0.0839,
      "step": 4400
    },
    {
      "epoch": 0.12152777777777778,
      "grad_norm": 0.4991815686225891,
      "learning_rate": 1.7569995590828926e-05,
      "loss": 0.0572,
      "step": 4410
    },
    {
      "epoch": 0.12180335097001764,
      "grad_norm": 0.1050102487206459,
      "learning_rate": 1.756448412698413e-05,
      "loss": 0.1371,
      "step": 4420
    },
    {
      "epoch": 0.12207892416225749,
      "grad_norm": 4.4212141036987305,
      "learning_rate": 1.7558972663139332e-05,
      "loss": 0.1088,
      "step": 4430
    },
    {
      "epoch": 0.12235449735449735,
      "grad_norm": 0.6145212054252625,
      "learning_rate": 1.7553461199294534e-05,
      "loss": 0.18,
      "step": 4440
    },
    {
      "epoch": 0.12263007054673722,
      "grad_norm": 0.08493689447641373,
      "learning_rate": 1.7547949735449735e-05,
      "loss": 0.2386,
      "step": 4450
    },
    {
      "epoch": 0.12290564373897707,
      "grad_norm": 72.89415740966797,
      "learning_rate": 1.754243827160494e-05,
      "loss": 0.1808,
      "step": 4460
    },
    {
      "epoch": 0.12318121693121693,
      "grad_norm": 69.97270202636719,
      "learning_rate": 1.7536926807760142e-05,
      "loss": 0.3056,
      "step": 4470
    },
    {
      "epoch": 0.12345679012345678,
      "grad_norm": 0.08670677989721298,
      "learning_rate": 1.7531415343915347e-05,
      "loss": 0.1791,
      "step": 4480
    },
    {
      "epoch": 0.12373236331569665,
      "grad_norm": 0.7887651324272156,
      "learning_rate": 1.752590388007055e-05,
      "loss": 0.226,
      "step": 4490
    },
    {
      "epoch": 0.12400793650793651,
      "grad_norm": 0.07215264439582825,
      "learning_rate": 1.752039241622575e-05,
      "loss": 0.1091,
      "step": 4500
    },
    {
      "epoch": 0.12428350970017636,
      "grad_norm": 0.2763002812862396,
      "learning_rate": 1.7514880952380955e-05,
      "loss": 0.2253,
      "step": 4510
    },
    {
      "epoch": 0.12455908289241623,
      "grad_norm": 0.0840490385890007,
      "learning_rate": 1.7509369488536156e-05,
      "loss": 0.2057,
      "step": 4520
    },
    {
      "epoch": 0.12483465608465609,
      "grad_norm": 0.10879533737897873,
      "learning_rate": 1.750385802469136e-05,
      "loss": 0.2064,
      "step": 4530
    },
    {
      "epoch": 0.12511022927689594,
      "grad_norm": 0.09103678166866302,
      "learning_rate": 1.7498346560846563e-05,
      "loss": 0.1013,
      "step": 4540
    },
    {
      "epoch": 0.1253858024691358,
      "grad_norm": 0.18486949801445007,
      "learning_rate": 1.7492835097001765e-05,
      "loss": 0.0925,
      "step": 4550
    },
    {
      "epoch": 0.12566137566137567,
      "grad_norm": 64.87156677246094,
      "learning_rate": 1.7487323633156966e-05,
      "loss": 0.2802,
      "step": 4560
    },
    {
      "epoch": 0.12593694885361553,
      "grad_norm": 0.8775029182434082,
      "learning_rate": 1.748181216931217e-05,
      "loss": 0.0823,
      "step": 4570
    },
    {
      "epoch": 0.12621252204585537,
      "grad_norm": 0.1054629385471344,
      "learning_rate": 1.7476300705467373e-05,
      "loss": 0.2551,
      "step": 4580
    },
    {
      "epoch": 0.12648809523809523,
      "grad_norm": 7.1554694175720215,
      "learning_rate": 1.7470789241622578e-05,
      "loss": 0.3575,
      "step": 4590
    },
    {
      "epoch": 0.1267636684303351,
      "grad_norm": 0.09299621731042862,
      "learning_rate": 1.746527777777778e-05,
      "loss": 0.1274,
      "step": 4600
    },
    {
      "epoch": 0.12703924162257496,
      "grad_norm": 0.30971378087997437,
      "learning_rate": 1.745976631393298e-05,
      "loss": 0.2102,
      "step": 4610
    },
    {
      "epoch": 0.12731481481481483,
      "grad_norm": 12.09376335144043,
      "learning_rate": 1.7454254850088186e-05,
      "loss": 0.0854,
      "step": 4620
    },
    {
      "epoch": 0.12759038800705466,
      "grad_norm": 157.24334716796875,
      "learning_rate": 1.7448743386243387e-05,
      "loss": 0.1402,
      "step": 4630
    },
    {
      "epoch": 0.12786596119929453,
      "grad_norm": 0.06422943621873856,
      "learning_rate": 1.7443231922398592e-05,
      "loss": 0.1348,
      "step": 4640
    },
    {
      "epoch": 0.1281415343915344,
      "grad_norm": 0.19617435336112976,
      "learning_rate": 1.7437720458553794e-05,
      "loss": 0.0702,
      "step": 4650
    },
    {
      "epoch": 0.12841710758377425,
      "grad_norm": 51.162437438964844,
      "learning_rate": 1.7432208994708995e-05,
      "loss": 0.1499,
      "step": 4660
    },
    {
      "epoch": 0.12869268077601412,
      "grad_norm": 77.41252136230469,
      "learning_rate": 1.7426697530864197e-05,
      "loss": 0.2463,
      "step": 4670
    },
    {
      "epoch": 0.12896825396825398,
      "grad_norm": 0.06790982186794281,
      "learning_rate": 1.7421186067019402e-05,
      "loss": 0.0078,
      "step": 4680
    },
    {
      "epoch": 0.12924382716049382,
      "grad_norm": 53.20433807373047,
      "learning_rate": 1.7415674603174603e-05,
      "loss": 0.2075,
      "step": 4690
    },
    {
      "epoch": 0.12951940035273368,
      "grad_norm": 0.6953231692314148,
      "learning_rate": 1.741016313932981e-05,
      "loss": 0.103,
      "step": 4700
    },
    {
      "epoch": 0.12979497354497355,
      "grad_norm": 0.12434608489274979,
      "learning_rate": 1.740465167548501e-05,
      "loss": 0.2152,
      "step": 4710
    },
    {
      "epoch": 0.1300705467372134,
      "grad_norm": 134.37135314941406,
      "learning_rate": 1.739914021164021e-05,
      "loss": 0.2197,
      "step": 4720
    },
    {
      "epoch": 0.13034611992945327,
      "grad_norm": 17.79107666015625,
      "learning_rate": 1.7393628747795416e-05,
      "loss": 0.4749,
      "step": 4730
    },
    {
      "epoch": 0.1306216931216931,
      "grad_norm": 0.09042246639728546,
      "learning_rate": 1.7388117283950618e-05,
      "loss": 0.1405,
      "step": 4740
    },
    {
      "epoch": 0.13089726631393298,
      "grad_norm": 80.12200927734375,
      "learning_rate": 1.7382605820105823e-05,
      "loss": 0.1984,
      "step": 4750
    },
    {
      "epoch": 0.13117283950617284,
      "grad_norm": 73.01242065429688,
      "learning_rate": 1.7377094356261025e-05,
      "loss": 0.2361,
      "step": 4760
    },
    {
      "epoch": 0.1314484126984127,
      "grad_norm": 0.07189486920833588,
      "learning_rate": 1.7371582892416226e-05,
      "loss": 0.3017,
      "step": 4770
    },
    {
      "epoch": 0.13172398589065257,
      "grad_norm": 0.20454993844032288,
      "learning_rate": 1.736607142857143e-05,
      "loss": 0.2736,
      "step": 4780
    },
    {
      "epoch": 0.1319995590828924,
      "grad_norm": 27.053592681884766,
      "learning_rate": 1.7360559964726633e-05,
      "loss": 0.2013,
      "step": 4790
    },
    {
      "epoch": 0.13227513227513227,
      "grad_norm": 8.112464904785156,
      "learning_rate": 1.7355048500881834e-05,
      "loss": 0.3069,
      "step": 4800
    },
    {
      "epoch": 0.13255070546737213,
      "grad_norm": 85.62460327148438,
      "learning_rate": 1.734953703703704e-05,
      "loss": 0.1524,
      "step": 4810
    },
    {
      "epoch": 0.132826278659612,
      "grad_norm": 10.904701232910156,
      "learning_rate": 1.734402557319224e-05,
      "loss": 0.0997,
      "step": 4820
    },
    {
      "epoch": 0.13310185185185186,
      "grad_norm": 0.07726841419935226,
      "learning_rate": 1.7338514109347446e-05,
      "loss": 0.1971,
      "step": 4830
    },
    {
      "epoch": 0.13337742504409172,
      "grad_norm": 1.7860796451568604,
      "learning_rate": 1.7333002645502647e-05,
      "loss": 0.0256,
      "step": 4840
    },
    {
      "epoch": 0.13365299823633156,
      "grad_norm": 53.16562271118164,
      "learning_rate": 1.732749118165785e-05,
      "loss": 0.3276,
      "step": 4850
    },
    {
      "epoch": 0.13392857142857142,
      "grad_norm": 137.41664123535156,
      "learning_rate": 1.7321979717813054e-05,
      "loss": 0.3104,
      "step": 4860
    },
    {
      "epoch": 0.1342041446208113,
      "grad_norm": 0.12950025498867035,
      "learning_rate": 1.7316468253968255e-05,
      "loss": 0.1896,
      "step": 4870
    },
    {
      "epoch": 0.13447971781305115,
      "grad_norm": 4.702335357666016,
      "learning_rate": 1.731095679012346e-05,
      "loss": 0.1631,
      "step": 4880
    },
    {
      "epoch": 0.13475529100529102,
      "grad_norm": 0.07132987678050995,
      "learning_rate": 1.7305445326278662e-05,
      "loss": 0.2658,
      "step": 4890
    },
    {
      "epoch": 0.13503086419753085,
      "grad_norm": 35.63935852050781,
      "learning_rate": 1.7299933862433863e-05,
      "loss": 0.1133,
      "step": 4900
    },
    {
      "epoch": 0.13530643738977072,
      "grad_norm": 13.736174583435059,
      "learning_rate": 1.7294422398589065e-05,
      "loss": 0.0519,
      "step": 4910
    },
    {
      "epoch": 0.13558201058201058,
      "grad_norm": 27.10442543029785,
      "learning_rate": 1.728891093474427e-05,
      "loss": 0.0816,
      "step": 4920
    },
    {
      "epoch": 0.13585758377425045,
      "grad_norm": 0.45108339190483093,
      "learning_rate": 1.728339947089947e-05,
      "loss": 0.1574,
      "step": 4930
    },
    {
      "epoch": 0.1361331569664903,
      "grad_norm": 0.05742236226797104,
      "learning_rate": 1.7277888007054676e-05,
      "loss": 0.143,
      "step": 4940
    },
    {
      "epoch": 0.13640873015873015,
      "grad_norm": 0.14877121150493622,
      "learning_rate": 1.7272376543209878e-05,
      "loss": 0.2661,
      "step": 4950
    },
    {
      "epoch": 0.13668430335097,
      "grad_norm": 0.2856226861476898,
      "learning_rate": 1.726686507936508e-05,
      "loss": 0.2558,
      "step": 4960
    },
    {
      "epoch": 0.13695987654320987,
      "grad_norm": 1.4665333032608032,
      "learning_rate": 1.7261353615520284e-05,
      "loss": 0.0298,
      "step": 4970
    },
    {
      "epoch": 0.13723544973544974,
      "grad_norm": 0.43498495221138,
      "learning_rate": 1.7255842151675486e-05,
      "loss": 0.192,
      "step": 4980
    },
    {
      "epoch": 0.1375110229276896,
      "grad_norm": 1.6011546850204468,
      "learning_rate": 1.725033068783069e-05,
      "loss": 0.0982,
      "step": 4990
    },
    {
      "epoch": 0.13778659611992947,
      "grad_norm": 13.272625923156738,
      "learning_rate": 1.7244819223985893e-05,
      "loss": 0.2528,
      "step": 5000
    },
    {
      "epoch": 0.1380621693121693,
      "grad_norm": 0.06647327542304993,
      "learning_rate": 1.7239307760141094e-05,
      "loss": 0.2015,
      "step": 5010
    },
    {
      "epoch": 0.13833774250440917,
      "grad_norm": 0.9003395438194275,
      "learning_rate": 1.7233796296296296e-05,
      "loss": 0.2632,
      "step": 5020
    },
    {
      "epoch": 0.13861331569664903,
      "grad_norm": 0.11983653903007507,
      "learning_rate": 1.72282848324515e-05,
      "loss": 0.1003,
      "step": 5030
    },
    {
      "epoch": 0.1388888888888889,
      "grad_norm": 109.58673858642578,
      "learning_rate": 1.7222773368606706e-05,
      "loss": 0.2301,
      "step": 5040
    },
    {
      "epoch": 0.13916446208112876,
      "grad_norm": 0.06915771216154099,
      "learning_rate": 1.7217261904761907e-05,
      "loss": 0.1723,
      "step": 5050
    },
    {
      "epoch": 0.1394400352733686,
      "grad_norm": 4.792028427124023,
      "learning_rate": 1.721175044091711e-05,
      "loss": 0.0685,
      "step": 5060
    },
    {
      "epoch": 0.13971560846560846,
      "grad_norm": 0.5641928315162659,
      "learning_rate": 1.720623897707231e-05,
      "loss": 0.2373,
      "step": 5070
    },
    {
      "epoch": 0.13999118165784832,
      "grad_norm": 70.09688568115234,
      "learning_rate": 1.7200727513227515e-05,
      "loss": 0.0738,
      "step": 5080
    },
    {
      "epoch": 0.1402667548500882,
      "grad_norm": 1.795127511024475,
      "learning_rate": 1.7195216049382717e-05,
      "loss": 0.1433,
      "step": 5090
    },
    {
      "epoch": 0.14054232804232805,
      "grad_norm": 0.09246505051851273,
      "learning_rate": 1.7189704585537922e-05,
      "loss": 0.0895,
      "step": 5100
    },
    {
      "epoch": 0.1408179012345679,
      "grad_norm": 0.06998389959335327,
      "learning_rate": 1.7184193121693123e-05,
      "loss": 0.2346,
      "step": 5110
    },
    {
      "epoch": 0.14109347442680775,
      "grad_norm": 3.599240779876709,
      "learning_rate": 1.7178681657848325e-05,
      "loss": 0.2594,
      "step": 5120
    },
    {
      "epoch": 0.14136904761904762,
      "grad_norm": 0.6852805018424988,
      "learning_rate": 1.7173170194003526e-05,
      "loss": 0.1004,
      "step": 5130
    },
    {
      "epoch": 0.14164462081128748,
      "grad_norm": 38.34248733520508,
      "learning_rate": 1.716765873015873e-05,
      "loss": 0.1854,
      "step": 5140
    },
    {
      "epoch": 0.14192019400352734,
      "grad_norm": 0.44583386182785034,
      "learning_rate": 1.7162147266313936e-05,
      "loss": 0.073,
      "step": 5150
    },
    {
      "epoch": 0.1421957671957672,
      "grad_norm": 0.0901988074183464,
      "learning_rate": 1.7156635802469138e-05,
      "loss": 0.0048,
      "step": 5160
    },
    {
      "epoch": 0.14247134038800705,
      "grad_norm": 56.36955261230469,
      "learning_rate": 1.715112433862434e-05,
      "loss": 0.1107,
      "step": 5170
    },
    {
      "epoch": 0.1427469135802469,
      "grad_norm": 28.461383819580078,
      "learning_rate": 1.714561287477954e-05,
      "loss": 0.0214,
      "step": 5180
    },
    {
      "epoch": 0.14302248677248677,
      "grad_norm": 1.4812484979629517,
      "learning_rate": 1.7140101410934746e-05,
      "loss": 0.2048,
      "step": 5190
    },
    {
      "epoch": 0.14329805996472664,
      "grad_norm": 108.8642807006836,
      "learning_rate": 1.7134589947089948e-05,
      "loss": 0.4353,
      "step": 5200
    },
    {
      "epoch": 0.1435736331569665,
      "grad_norm": 54.80766296386719,
      "learning_rate": 1.7129078483245153e-05,
      "loss": 0.4084,
      "step": 5210
    },
    {
      "epoch": 0.14384920634920634,
      "grad_norm": 25.873336791992188,
      "learning_rate": 1.7123567019400354e-05,
      "loss": 0.1458,
      "step": 5220
    },
    {
      "epoch": 0.1441247795414462,
      "grad_norm": 0.10246699303388596,
      "learning_rate": 1.7118055555555556e-05,
      "loss": 0.1357,
      "step": 5230
    },
    {
      "epoch": 0.14440035273368607,
      "grad_norm": 108.87126922607422,
      "learning_rate": 1.7112544091710757e-05,
      "loss": 0.169,
      "step": 5240
    },
    {
      "epoch": 0.14467592592592593,
      "grad_norm": 81.58685302734375,
      "learning_rate": 1.7107032627865962e-05,
      "loss": 0.1196,
      "step": 5250
    },
    {
      "epoch": 0.1449514991181658,
      "grad_norm": 0.19820483028888702,
      "learning_rate": 1.7101521164021167e-05,
      "loss": 0.2043,
      "step": 5260
    },
    {
      "epoch": 0.14522707231040563,
      "grad_norm": 117.80754089355469,
      "learning_rate": 1.709600970017637e-05,
      "loss": 0.1689,
      "step": 5270
    },
    {
      "epoch": 0.1455026455026455,
      "grad_norm": 0.07646693289279938,
      "learning_rate": 1.709049823633157e-05,
      "loss": 0.0057,
      "step": 5280
    },
    {
      "epoch": 0.14577821869488536,
      "grad_norm": 0.0756077766418457,
      "learning_rate": 1.7084986772486772e-05,
      "loss": 0.0997,
      "step": 5290
    },
    {
      "epoch": 0.14605379188712522,
      "grad_norm": 6.815609931945801,
      "learning_rate": 1.7079475308641977e-05,
      "loss": 0.3972,
      "step": 5300
    },
    {
      "epoch": 0.1463293650793651,
      "grad_norm": 31.676240921020508,
      "learning_rate": 1.707396384479718e-05,
      "loss": 0.2005,
      "step": 5310
    },
    {
      "epoch": 0.14660493827160495,
      "grad_norm": 0.07176420837640762,
      "learning_rate": 1.7068452380952383e-05,
      "loss": 0.1939,
      "step": 5320
    },
    {
      "epoch": 0.1468805114638448,
      "grad_norm": 56.27678298950195,
      "learning_rate": 1.7062940917107585e-05,
      "loss": 0.2397,
      "step": 5330
    },
    {
      "epoch": 0.14715608465608465,
      "grad_norm": 0.07819467037916183,
      "learning_rate": 1.705742945326279e-05,
      "loss": 0.1269,
      "step": 5340
    },
    {
      "epoch": 0.14743165784832452,
      "grad_norm": 12.16518783569336,
      "learning_rate": 1.705191798941799e-05,
      "loss": 0.1997,
      "step": 5350
    },
    {
      "epoch": 0.14770723104056438,
      "grad_norm": 3.9187967777252197,
      "learning_rate": 1.7046406525573193e-05,
      "loss": 0.0109,
      "step": 5360
    },
    {
      "epoch": 0.14798280423280424,
      "grad_norm": 39.22724914550781,
      "learning_rate": 1.7040895061728398e-05,
      "loss": 0.2623,
      "step": 5370
    },
    {
      "epoch": 0.14825837742504408,
      "grad_norm": 143.6794891357422,
      "learning_rate": 1.70353835978836e-05,
      "loss": 0.1738,
      "step": 5380
    },
    {
      "epoch": 0.14853395061728394,
      "grad_norm": 0.04935256019234657,
      "learning_rate": 1.7029872134038804e-05,
      "loss": 0.1263,
      "step": 5390
    },
    {
      "epoch": 0.1488095238095238,
      "grad_norm": 0.17038685083389282,
      "learning_rate": 1.7024360670194006e-05,
      "loss": 0.18,
      "step": 5400
    },
    {
      "epoch": 0.14908509700176367,
      "grad_norm": 0.09747761487960815,
      "learning_rate": 1.7018849206349208e-05,
      "loss": 0.0759,
      "step": 5410
    },
    {
      "epoch": 0.14936067019400354,
      "grad_norm": 0.7116398811340332,
      "learning_rate": 1.701333774250441e-05,
      "loss": 0.1025,
      "step": 5420
    },
    {
      "epoch": 0.14963624338624337,
      "grad_norm": 0.06643439829349518,
      "learning_rate": 1.7007826278659614e-05,
      "loss": 0.0603,
      "step": 5430
    },
    {
      "epoch": 0.14991181657848324,
      "grad_norm": 0.05493790656328201,
      "learning_rate": 1.7002314814814816e-05,
      "loss": 0.0693,
      "step": 5440
    },
    {
      "epoch": 0.1501873897707231,
      "grad_norm": 84.87044525146484,
      "learning_rate": 1.699680335097002e-05,
      "loss": 0.0958,
      "step": 5450
    },
    {
      "epoch": 0.15046296296296297,
      "grad_norm": 14.940844535827637,
      "learning_rate": 1.6991291887125222e-05,
      "loss": 0.1619,
      "step": 5460
    },
    {
      "epoch": 0.15073853615520283,
      "grad_norm": 10.717606544494629,
      "learning_rate": 1.6985780423280424e-05,
      "loss": 0.1284,
      "step": 5470
    },
    {
      "epoch": 0.1510141093474427,
      "grad_norm": 0.08949245512485504,
      "learning_rate": 1.698026895943563e-05,
      "loss": 0.104,
      "step": 5480
    },
    {
      "epoch": 0.15128968253968253,
      "grad_norm": 0.057885199785232544,
      "learning_rate": 1.697475749559083e-05,
      "loss": 0.2419,
      "step": 5490
    },
    {
      "epoch": 0.1515652557319224,
      "grad_norm": 0.11402224004268646,
      "learning_rate": 1.6969246031746035e-05,
      "loss": 0.0095,
      "step": 5500
    },
    {
      "epoch": 0.15184082892416226,
      "grad_norm": 0.05298786610364914,
      "learning_rate": 1.6963734567901237e-05,
      "loss": 0.1961,
      "step": 5510
    },
    {
      "epoch": 0.15211640211640212,
      "grad_norm": 0.1022370457649231,
      "learning_rate": 1.6958223104056438e-05,
      "loss": 0.1109,
      "step": 5520
    },
    {
      "epoch": 0.15239197530864199,
      "grad_norm": 0.12317430227994919,
      "learning_rate": 1.695271164021164e-05,
      "loss": 0.0158,
      "step": 5530
    },
    {
      "epoch": 0.15266754850088182,
      "grad_norm": 1.2488391399383545,
      "learning_rate": 1.6947200176366845e-05,
      "loss": 0.377,
      "step": 5540
    },
    {
      "epoch": 0.1529431216931217,
      "grad_norm": 69.96695709228516,
      "learning_rate": 1.6941688712522046e-05,
      "loss": 0.2585,
      "step": 5550
    },
    {
      "epoch": 0.15321869488536155,
      "grad_norm": 60.528343200683594,
      "learning_rate": 1.693617724867725e-05,
      "loss": 0.0646,
      "step": 5560
    },
    {
      "epoch": 0.15349426807760141,
      "grad_norm": 89.02947998046875,
      "learning_rate": 1.6930665784832453e-05,
      "loss": 0.2039,
      "step": 5570
    },
    {
      "epoch": 0.15376984126984128,
      "grad_norm": 103.22663879394531,
      "learning_rate": 1.6925154320987654e-05,
      "loss": 0.1653,
      "step": 5580
    },
    {
      "epoch": 0.15404541446208111,
      "grad_norm": 15.328937530517578,
      "learning_rate": 1.691964285714286e-05,
      "loss": 0.423,
      "step": 5590
    },
    {
      "epoch": 0.15432098765432098,
      "grad_norm": 12.075921058654785,
      "learning_rate": 1.691413139329806e-05,
      "loss": 0.0826,
      "step": 5600
    },
    {
      "epoch": 0.15459656084656084,
      "grad_norm": 0.13936123251914978,
      "learning_rate": 1.6908619929453266e-05,
      "loss": 0.1081,
      "step": 5610
    },
    {
      "epoch": 0.1548721340388007,
      "grad_norm": 139.04188537597656,
      "learning_rate": 1.6903108465608467e-05,
      "loss": 0.114,
      "step": 5620
    },
    {
      "epoch": 0.15514770723104057,
      "grad_norm": 39.098854064941406,
      "learning_rate": 1.689759700176367e-05,
      "loss": 0.098,
      "step": 5630
    },
    {
      "epoch": 0.15542328042328044,
      "grad_norm": 68.9981689453125,
      "learning_rate": 1.689208553791887e-05,
      "loss": 0.4308,
      "step": 5640
    },
    {
      "epoch": 0.15569885361552027,
      "grad_norm": 42.238162994384766,
      "learning_rate": 1.6886574074074076e-05,
      "loss": 0.0753,
      "step": 5650
    },
    {
      "epoch": 0.15597442680776014,
      "grad_norm": 30.19706153869629,
      "learning_rate": 1.6881062610229277e-05,
      "loss": 0.0087,
      "step": 5660
    },
    {
      "epoch": 0.15625,
      "grad_norm": 0.06582935899496078,
      "learning_rate": 1.6875551146384482e-05,
      "loss": 0.229,
      "step": 5670
    },
    {
      "epoch": 0.15652557319223986,
      "grad_norm": 0.12282831221818924,
      "learning_rate": 1.6870039682539684e-05,
      "loss": 0.1385,
      "step": 5680
    },
    {
      "epoch": 0.15680114638447973,
      "grad_norm": 51.24998092651367,
      "learning_rate": 1.6864528218694885e-05,
      "loss": 0.1133,
      "step": 5690
    },
    {
      "epoch": 0.15707671957671956,
      "grad_norm": 0.05518525093793869,
      "learning_rate": 1.685901675485009e-05,
      "loss": 0.2576,
      "step": 5700
    },
    {
      "epoch": 0.15735229276895943,
      "grad_norm": 0.05792637914419174,
      "learning_rate": 1.6853505291005292e-05,
      "loss": 0.2647,
      "step": 5710
    },
    {
      "epoch": 0.1576278659611993,
      "grad_norm": 32.970863342285156,
      "learning_rate": 1.6847993827160497e-05,
      "loss": 0.2486,
      "step": 5720
    },
    {
      "epoch": 0.15790343915343916,
      "grad_norm": 53.45225524902344,
      "learning_rate": 1.6842482363315698e-05,
      "loss": 0.2347,
      "step": 5730
    },
    {
      "epoch": 0.15817901234567902,
      "grad_norm": 1.585268259048462,
      "learning_rate": 1.68369708994709e-05,
      "loss": 0.2114,
      "step": 5740
    },
    {
      "epoch": 0.15845458553791889,
      "grad_norm": 41.010704040527344,
      "learning_rate": 1.68314594356261e-05,
      "loss": 0.1346,
      "step": 5750
    },
    {
      "epoch": 0.15873015873015872,
      "grad_norm": 33.71567153930664,
      "learning_rate": 1.6825947971781306e-05,
      "loss": 0.1846,
      "step": 5760
    },
    {
      "epoch": 0.15900573192239859,
      "grad_norm": 3.200425148010254,
      "learning_rate": 1.6820436507936508e-05,
      "loss": 0.1969,
      "step": 5770
    },
    {
      "epoch": 0.15928130511463845,
      "grad_norm": 88.63294219970703,
      "learning_rate": 1.6814925044091713e-05,
      "loss": 0.2413,
      "step": 5780
    },
    {
      "epoch": 0.1595568783068783,
      "grad_norm": 11.248644828796387,
      "learning_rate": 1.6809413580246914e-05,
      "loss": 0.1588,
      "step": 5790
    },
    {
      "epoch": 0.15983245149911818,
      "grad_norm": 0.11148752272129059,
      "learning_rate": 1.6803902116402116e-05,
      "loss": 0.1069,
      "step": 5800
    },
    {
      "epoch": 0.16010802469135801,
      "grad_norm": 49.24430847167969,
      "learning_rate": 1.679839065255732e-05,
      "loss": 0.3065,
      "step": 5810
    },
    {
      "epoch": 0.16038359788359788,
      "grad_norm": 72.11449432373047,
      "learning_rate": 1.6792879188712523e-05,
      "loss": 0.175,
      "step": 5820
    },
    {
      "epoch": 0.16065917107583774,
      "grad_norm": 94.29130554199219,
      "learning_rate": 1.6787367724867727e-05,
      "loss": 0.0943,
      "step": 5830
    },
    {
      "epoch": 0.1609347442680776,
      "grad_norm": 0.5911316871643066,
      "learning_rate": 1.678185626102293e-05,
      "loss": 0.1122,
      "step": 5840
    },
    {
      "epoch": 0.16121031746031747,
      "grad_norm": 0.26143473386764526,
      "learning_rate": 1.6776344797178134e-05,
      "loss": 0.2935,
      "step": 5850
    },
    {
      "epoch": 0.1614858906525573,
      "grad_norm": 0.13838186860084534,
      "learning_rate": 1.6770833333333336e-05,
      "loss": 0.2957,
      "step": 5860
    },
    {
      "epoch": 0.16176146384479717,
      "grad_norm": 70.65451049804688,
      "learning_rate": 1.6765321869488537e-05,
      "loss": 0.2023,
      "step": 5870
    },
    {
      "epoch": 0.16203703703703703,
      "grad_norm": 0.07619590312242508,
      "learning_rate": 1.675981040564374e-05,
      "loss": 0.1002,
      "step": 5880
    },
    {
      "epoch": 0.1623126102292769,
      "grad_norm": 1.3474388122558594,
      "learning_rate": 1.6754298941798944e-05,
      "loss": 0.2631,
      "step": 5890
    },
    {
      "epoch": 0.16258818342151676,
      "grad_norm": 0.40426385402679443,
      "learning_rate": 1.674878747795415e-05,
      "loss": 0.1384,
      "step": 5900
    },
    {
      "epoch": 0.16286375661375663,
      "grad_norm": 0.445061057806015,
      "learning_rate": 1.674327601410935e-05,
      "loss": 0.1838,
      "step": 5910
    },
    {
      "epoch": 0.16313932980599646,
      "grad_norm": 0.11535689979791641,
      "learning_rate": 1.6737764550264552e-05,
      "loss": 0.1868,
      "step": 5920
    },
    {
      "epoch": 0.16341490299823633,
      "grad_norm": 0.25607019662857056,
      "learning_rate": 1.6732253086419753e-05,
      "loss": 0.0102,
      "step": 5930
    },
    {
      "epoch": 0.1636904761904762,
      "grad_norm": 2.2800374031066895,
      "learning_rate": 1.6726741622574958e-05,
      "loss": 0.1603,
      "step": 5940
    },
    {
      "epoch": 0.16396604938271606,
      "grad_norm": 35.92097091674805,
      "learning_rate": 1.672123015873016e-05,
      "loss": 0.0698,
      "step": 5950
    },
    {
      "epoch": 0.16424162257495592,
      "grad_norm": 67.02259826660156,
      "learning_rate": 1.6715718694885365e-05,
      "loss": 0.1038,
      "step": 5960
    },
    {
      "epoch": 0.16451719576719576,
      "grad_norm": 76.85353088378906,
      "learning_rate": 1.6710207231040566e-05,
      "loss": 0.0255,
      "step": 5970
    },
    {
      "epoch": 0.16479276895943562,
      "grad_norm": 9.959506034851074,
      "learning_rate": 1.6704695767195768e-05,
      "loss": 0.1716,
      "step": 5980
    },
    {
      "epoch": 0.16506834215167548,
      "grad_norm": 8.50860595703125,
      "learning_rate": 1.669918430335097e-05,
      "loss": 0.0761,
      "step": 5990
    },
    {
      "epoch": 0.16534391534391535,
      "grad_norm": 0.116040900349617,
      "learning_rate": 1.6693672839506174e-05,
      "loss": 0.0494,
      "step": 6000
    },
    {
      "epoch": 0.1656194885361552,
      "grad_norm": 0.10430680215358734,
      "learning_rate": 1.668816137566138e-05,
      "loss": 0.0792,
      "step": 6010
    },
    {
      "epoch": 0.16589506172839505,
      "grad_norm": 0.2622712254524231,
      "learning_rate": 1.668264991181658e-05,
      "loss": 0.1034,
      "step": 6020
    },
    {
      "epoch": 0.1661706349206349,
      "grad_norm": 6.544944763183594,
      "learning_rate": 1.6677138447971782e-05,
      "loss": 0.2625,
      "step": 6030
    },
    {
      "epoch": 0.16644620811287478,
      "grad_norm": 69.73892211914062,
      "learning_rate": 1.6671626984126984e-05,
      "loss": 0.2695,
      "step": 6040
    },
    {
      "epoch": 0.16672178130511464,
      "grad_norm": 0.05073269084095955,
      "learning_rate": 1.666611552028219e-05,
      "loss": 0.2518,
      "step": 6050
    },
    {
      "epoch": 0.1669973544973545,
      "grad_norm": 0.045693717896938324,
      "learning_rate": 1.666060405643739e-05,
      "loss": 0.1388,
      "step": 6060
    },
    {
      "epoch": 0.16727292768959437,
      "grad_norm": 77.34677124023438,
      "learning_rate": 1.6655092592592595e-05,
      "loss": 0.1809,
      "step": 6070
    },
    {
      "epoch": 0.1675485008818342,
      "grad_norm": 118.8729019165039,
      "learning_rate": 1.6649581128747797e-05,
      "loss": 0.0947,
      "step": 6080
    },
    {
      "epoch": 0.16782407407407407,
      "grad_norm": 0.04119958356022835,
      "learning_rate": 1.6644069664903e-05,
      "loss": 0.1069,
      "step": 6090
    },
    {
      "epoch": 0.16809964726631393,
      "grad_norm": 0.11327331513166428,
      "learning_rate": 1.66385582010582e-05,
      "loss": 0.0822,
      "step": 6100
    },
    {
      "epoch": 0.1683752204585538,
      "grad_norm": 0.046241872012615204,
      "learning_rate": 1.6633046737213405e-05,
      "loss": 0.1655,
      "step": 6110
    },
    {
      "epoch": 0.16865079365079366,
      "grad_norm": 53.2998046875,
      "learning_rate": 1.662753527336861e-05,
      "loss": 0.1917,
      "step": 6120
    },
    {
      "epoch": 0.1689263668430335,
      "grad_norm": 0.08807359635829926,
      "learning_rate": 1.662202380952381e-05,
      "loss": 0.1892,
      "step": 6130
    },
    {
      "epoch": 0.16920194003527336,
      "grad_norm": 0.06952060014009476,
      "learning_rate": 1.6616512345679013e-05,
      "loss": 0.0695,
      "step": 6140
    },
    {
      "epoch": 0.16947751322751323,
      "grad_norm": 0.061633411794900894,
      "learning_rate": 1.6611000881834215e-05,
      "loss": 0.1393,
      "step": 6150
    },
    {
      "epoch": 0.1697530864197531,
      "grad_norm": 0.04691321402788162,
      "learning_rate": 1.660548941798942e-05,
      "loss": 0.0616,
      "step": 6160
    },
    {
      "epoch": 0.17002865961199295,
      "grad_norm": 0.11893622577190399,
      "learning_rate": 1.659997795414462e-05,
      "loss": 0.1489,
      "step": 6170
    },
    {
      "epoch": 0.1703042328042328,
      "grad_norm": 0.061619147658348083,
      "learning_rate": 1.6594466490299826e-05,
      "loss": 0.1318,
      "step": 6180
    },
    {
      "epoch": 0.17057980599647266,
      "grad_norm": 110.96728515625,
      "learning_rate": 1.6588955026455028e-05,
      "loss": 0.2604,
      "step": 6190
    },
    {
      "epoch": 0.17085537918871252,
      "grad_norm": 64.52403259277344,
      "learning_rate": 1.658344356261023e-05,
      "loss": 0.3597,
      "step": 6200
    },
    {
      "epoch": 0.17113095238095238,
      "grad_norm": 0.04991854727268219,
      "learning_rate": 1.657793209876543e-05,
      "loss": 0.2351,
      "step": 6210
    },
    {
      "epoch": 0.17140652557319225,
      "grad_norm": 0.06512703001499176,
      "learning_rate": 1.6572420634920636e-05,
      "loss": 0.199,
      "step": 6220
    },
    {
      "epoch": 0.1716820987654321,
      "grad_norm": 47.623050689697266,
      "learning_rate": 1.656690917107584e-05,
      "loss": 0.195,
      "step": 6230
    },
    {
      "epoch": 0.17195767195767195,
      "grad_norm": 4.526436805725098,
      "learning_rate": 1.6561397707231042e-05,
      "loss": 0.0839,
      "step": 6240
    },
    {
      "epoch": 0.1722332451499118,
      "grad_norm": 47.390567779541016,
      "learning_rate": 1.6555886243386244e-05,
      "loss": 0.3514,
      "step": 6250
    },
    {
      "epoch": 0.17250881834215168,
      "grad_norm": 0.4076386094093323,
      "learning_rate": 1.6550374779541446e-05,
      "loss": 0.095,
      "step": 6260
    },
    {
      "epoch": 0.17278439153439154,
      "grad_norm": 9.162700653076172,
      "learning_rate": 1.654486331569665e-05,
      "loss": 0.362,
      "step": 6270
    },
    {
      "epoch": 0.1730599647266314,
      "grad_norm": 2.6555874347686768,
      "learning_rate": 1.6539351851851852e-05,
      "loss": 0.1807,
      "step": 6280
    },
    {
      "epoch": 0.17333553791887124,
      "grad_norm": 3.9836838245391846,
      "learning_rate": 1.6533840388007057e-05,
      "loss": 0.2536,
      "step": 6290
    },
    {
      "epoch": 0.1736111111111111,
      "grad_norm": 82.60812377929688,
      "learning_rate": 1.652832892416226e-05,
      "loss": 0.093,
      "step": 6300
    },
    {
      "epoch": 0.17388668430335097,
      "grad_norm": 0.12200424075126648,
      "learning_rate": 1.652281746031746e-05,
      "loss": 0.058,
      "step": 6310
    },
    {
      "epoch": 0.17416225749559083,
      "grad_norm": 3.5332212448120117,
      "learning_rate": 1.6517305996472665e-05,
      "loss": 0.2549,
      "step": 6320
    },
    {
      "epoch": 0.1744378306878307,
      "grad_norm": 0.08032174408435822,
      "learning_rate": 1.6511794532627867e-05,
      "loss": 0.2756,
      "step": 6330
    },
    {
      "epoch": 0.17471340388007053,
      "grad_norm": 11.359664916992188,
      "learning_rate": 1.650628306878307e-05,
      "loss": 0.3157,
      "step": 6340
    },
    {
      "epoch": 0.1749889770723104,
      "grad_norm": 0.5395475029945374,
      "learning_rate": 1.6500771604938273e-05,
      "loss": 0.0065,
      "step": 6350
    },
    {
      "epoch": 0.17526455026455026,
      "grad_norm": 49.45077133178711,
      "learning_rate": 1.6495260141093478e-05,
      "loss": 0.0505,
      "step": 6360
    },
    {
      "epoch": 0.17554012345679013,
      "grad_norm": 3.0810182094573975,
      "learning_rate": 1.648974867724868e-05,
      "loss": 0.2339,
      "step": 6370
    },
    {
      "epoch": 0.17581569664903,
      "grad_norm": 1.022444725036621,
      "learning_rate": 1.648423721340388e-05,
      "loss": 0.054,
      "step": 6380
    },
    {
      "epoch": 0.17609126984126985,
      "grad_norm": 10.496840476989746,
      "learning_rate": 1.6478725749559083e-05,
      "loss": 0.1015,
      "step": 6390
    },
    {
      "epoch": 0.1763668430335097,
      "grad_norm": 0.13567174971103668,
      "learning_rate": 1.6473214285714288e-05,
      "loss": 0.2813,
      "step": 6400
    },
    {
      "epoch": 0.17664241622574955,
      "grad_norm": 0.6062453389167786,
      "learning_rate": 1.646770282186949e-05,
      "loss": 0.0803,
      "step": 6410
    },
    {
      "epoch": 0.17691798941798942,
      "grad_norm": 0.05120471864938736,
      "learning_rate": 1.6462191358024694e-05,
      "loss": 0.0694,
      "step": 6420
    },
    {
      "epoch": 0.17719356261022928,
      "grad_norm": 1.7081040143966675,
      "learning_rate": 1.6456679894179896e-05,
      "loss": 0.2044,
      "step": 6430
    },
    {
      "epoch": 0.17746913580246915,
      "grad_norm": 8.537467956542969,
      "learning_rate": 1.6451168430335097e-05,
      "loss": 0.2527,
      "step": 6440
    },
    {
      "epoch": 0.17774470899470898,
      "grad_norm": 0.043445467948913574,
      "learning_rate": 1.6445656966490302e-05,
      "loss": 0.0515,
      "step": 6450
    },
    {
      "epoch": 0.17802028218694885,
      "grad_norm": 0.6062086224555969,
      "learning_rate": 1.6440145502645504e-05,
      "loss": 0.2219,
      "step": 6460
    },
    {
      "epoch": 0.1782958553791887,
      "grad_norm": 7.304430961608887,
      "learning_rate": 1.643463403880071e-05,
      "loss": 0.1802,
      "step": 6470
    },
    {
      "epoch": 0.17857142857142858,
      "grad_norm": 0.04325997084379196,
      "learning_rate": 1.642912257495591e-05,
      "loss": 0.0954,
      "step": 6480
    },
    {
      "epoch": 0.17884700176366844,
      "grad_norm": 3.4527883529663086,
      "learning_rate": 1.6423611111111112e-05,
      "loss": 0.1359,
      "step": 6490
    },
    {
      "epoch": 0.17912257495590828,
      "grad_norm": 122.47998809814453,
      "learning_rate": 1.6418099647266314e-05,
      "loss": 0.1437,
      "step": 6500
    },
    {
      "epoch": 0.17939814814814814,
      "grad_norm": 0.10757093131542206,
      "learning_rate": 1.641258818342152e-05,
      "loss": 0.0448,
      "step": 6510
    },
    {
      "epoch": 0.179673721340388,
      "grad_norm": 0.10335255414247513,
      "learning_rate": 1.640707671957672e-05,
      "loss": 0.1962,
      "step": 6520
    },
    {
      "epoch": 0.17994929453262787,
      "grad_norm": 0.6307438611984253,
      "learning_rate": 1.6401565255731925e-05,
      "loss": 0.0156,
      "step": 6530
    },
    {
      "epoch": 0.18022486772486773,
      "grad_norm": 0.3253251016139984,
      "learning_rate": 1.6396053791887127e-05,
      "loss": 0.1137,
      "step": 6540
    },
    {
      "epoch": 0.1805004409171076,
      "grad_norm": 0.1835351139307022,
      "learning_rate": 1.6390542328042328e-05,
      "loss": 0.2112,
      "step": 6550
    },
    {
      "epoch": 0.18077601410934743,
      "grad_norm": 65.42730712890625,
      "learning_rate": 1.6385030864197533e-05,
      "loss": 0.0602,
      "step": 6560
    },
    {
      "epoch": 0.1810515873015873,
      "grad_norm": 1.5198674201965332,
      "learning_rate": 1.6379519400352735e-05,
      "loss": 0.006,
      "step": 6570
    },
    {
      "epoch": 0.18132716049382716,
      "grad_norm": 0.45916178822517395,
      "learning_rate": 1.637400793650794e-05,
      "loss": 0.1524,
      "step": 6580
    },
    {
      "epoch": 0.18160273368606702,
      "grad_norm": 26.277685165405273,
      "learning_rate": 1.636849647266314e-05,
      "loss": 0.2043,
      "step": 6590
    },
    {
      "epoch": 0.1818783068783069,
      "grad_norm": 28.026596069335938,
      "learning_rate": 1.6362985008818343e-05,
      "loss": 0.0242,
      "step": 6600
    },
    {
      "epoch": 0.18215388007054673,
      "grad_norm": 1.5432621240615845,
      "learning_rate": 1.6357473544973544e-05,
      "loss": 0.1947,
      "step": 6610
    },
    {
      "epoch": 0.1824294532627866,
      "grad_norm": 0.09309788048267365,
      "learning_rate": 1.635196208112875e-05,
      "loss": 0.0519,
      "step": 6620
    },
    {
      "epoch": 0.18270502645502645,
      "grad_norm": 0.040927156805992126,
      "learning_rate": 1.634645061728395e-05,
      "loss": 0.3144,
      "step": 6630
    },
    {
      "epoch": 0.18298059964726632,
      "grad_norm": 0.42624449729919434,
      "learning_rate": 1.6340939153439156e-05,
      "loss": 0.0509,
      "step": 6640
    },
    {
      "epoch": 0.18325617283950618,
      "grad_norm": 39.961822509765625,
      "learning_rate": 1.6335427689594357e-05,
      "loss": 0.2468,
      "step": 6650
    },
    {
      "epoch": 0.18353174603174602,
      "grad_norm": 0.037180814892053604,
      "learning_rate": 1.632991622574956e-05,
      "loss": 0.1946,
      "step": 6660
    },
    {
      "epoch": 0.18380731922398588,
      "grad_norm": 0.03801752254366875,
      "learning_rate": 1.6324404761904764e-05,
      "loss": 0.2549,
      "step": 6670
    },
    {
      "epoch": 0.18408289241622575,
      "grad_norm": 85.1126480102539,
      "learning_rate": 1.6318893298059965e-05,
      "loss": 0.1515,
      "step": 6680
    },
    {
      "epoch": 0.1843584656084656,
      "grad_norm": 1.3060684204101562,
      "learning_rate": 1.631338183421517e-05,
      "loss": 0.0765,
      "step": 6690
    },
    {
      "epoch": 0.18463403880070547,
      "grad_norm": 1.1539655923843384,
      "learning_rate": 1.6307870370370372e-05,
      "loss": 0.1228,
      "step": 6700
    },
    {
      "epoch": 0.18490961199294534,
      "grad_norm": 0.03647667542099953,
      "learning_rate": 1.6302358906525574e-05,
      "loss": 0.0176,
      "step": 6710
    },
    {
      "epoch": 0.18518518518518517,
      "grad_norm": 5.746960163116455,
      "learning_rate": 1.6296847442680775e-05,
      "loss": 0.0971,
      "step": 6720
    },
    {
      "epoch": 0.18546075837742504,
      "grad_norm": 162.26284790039062,
      "learning_rate": 1.629133597883598e-05,
      "loss": 0.2263,
      "step": 6730
    },
    {
      "epoch": 0.1857363315696649,
      "grad_norm": 0.14057672023773193,
      "learning_rate": 1.628582451499118e-05,
      "loss": 0.0172,
      "step": 6740
    },
    {
      "epoch": 0.18601190476190477,
      "grad_norm": 74.31756591796875,
      "learning_rate": 1.6280313051146387e-05,
      "loss": 0.1966,
      "step": 6750
    },
    {
      "epoch": 0.18628747795414463,
      "grad_norm": 1.444978952407837,
      "learning_rate": 1.6274801587301588e-05,
      "loss": 0.2358,
      "step": 6760
    },
    {
      "epoch": 0.18656305114638447,
      "grad_norm": 0.11731351166963577,
      "learning_rate": 1.626929012345679e-05,
      "loss": 0.2358,
      "step": 6770
    },
    {
      "epoch": 0.18683862433862433,
      "grad_norm": 27.793272018432617,
      "learning_rate": 1.6263778659611995e-05,
      "loss": 0.1534,
      "step": 6780
    },
    {
      "epoch": 0.1871141975308642,
      "grad_norm": 0.6900697350502014,
      "learning_rate": 1.6258267195767196e-05,
      "loss": 0.1081,
      "step": 6790
    },
    {
      "epoch": 0.18738977072310406,
      "grad_norm": 0.08426793664693832,
      "learning_rate": 1.62527557319224e-05,
      "loss": 0.0307,
      "step": 6800
    },
    {
      "epoch": 0.18766534391534392,
      "grad_norm": 100.77467346191406,
      "learning_rate": 1.6247244268077603e-05,
      "loss": 0.1429,
      "step": 6810
    },
    {
      "epoch": 0.1879409171075838,
      "grad_norm": 97.15780639648438,
      "learning_rate": 1.6241732804232804e-05,
      "loss": 0.2447,
      "step": 6820
    },
    {
      "epoch": 0.18821649029982362,
      "grad_norm": 0.11016225069761276,
      "learning_rate": 1.6236221340388006e-05,
      "loss": 0.1308,
      "step": 6830
    },
    {
      "epoch": 0.1884920634920635,
      "grad_norm": 0.24573372304439545,
      "learning_rate": 1.623070987654321e-05,
      "loss": 0.1132,
      "step": 6840
    },
    {
      "epoch": 0.18876763668430335,
      "grad_norm": 0.03818359598517418,
      "learning_rate": 1.6225198412698412e-05,
      "loss": 0.1126,
      "step": 6850
    },
    {
      "epoch": 0.18904320987654322,
      "grad_norm": 0.04884874075651169,
      "learning_rate": 1.6219686948853617e-05,
      "loss": 0.1202,
      "step": 6860
    },
    {
      "epoch": 0.18931878306878308,
      "grad_norm": 1.0095545053482056,
      "learning_rate": 1.6214175485008822e-05,
      "loss": 0.0884,
      "step": 6870
    },
    {
      "epoch": 0.18959435626102292,
      "grad_norm": 2.5517919063568115,
      "learning_rate": 1.6208664021164024e-05,
      "loss": 0.3011,
      "step": 6880
    },
    {
      "epoch": 0.18986992945326278,
      "grad_norm": 0.03814885392785072,
      "learning_rate": 1.6203152557319225e-05,
      "loss": 0.0947,
      "step": 6890
    },
    {
      "epoch": 0.19014550264550265,
      "grad_norm": 40.661277770996094,
      "learning_rate": 1.6197641093474427e-05,
      "loss": 0.1349,
      "step": 6900
    },
    {
      "epoch": 0.1904210758377425,
      "grad_norm": 32.127586364746094,
      "learning_rate": 1.6192129629629632e-05,
      "loss": 0.1937,
      "step": 6910
    },
    {
      "epoch": 0.19069664902998237,
      "grad_norm": 0.05117878317832947,
      "learning_rate": 1.6186618165784834e-05,
      "loss": 0.2224,
      "step": 6920
    },
    {
      "epoch": 0.1909722222222222,
      "grad_norm": 0.032303933054208755,
      "learning_rate": 1.618110670194004e-05,
      "loss": 0.1672,
      "step": 6930
    },
    {
      "epoch": 0.19124779541446207,
      "grad_norm": 2.97257924079895,
      "learning_rate": 1.617559523809524e-05,
      "loss": 0.3915,
      "step": 6940
    },
    {
      "epoch": 0.19152336860670194,
      "grad_norm": 91.3352279663086,
      "learning_rate": 1.617008377425044e-05,
      "loss": 0.2897,
      "step": 6950
    },
    {
      "epoch": 0.1917989417989418,
      "grad_norm": 2.861114025115967,
      "learning_rate": 1.6164572310405643e-05,
      "loss": 0.225,
      "step": 6960
    },
    {
      "epoch": 0.19207451499118167,
      "grad_norm": 0.04327753186225891,
      "learning_rate": 1.6159060846560848e-05,
      "loss": 0.2317,
      "step": 6970
    },
    {
      "epoch": 0.19235008818342153,
      "grad_norm": 100.34146118164062,
      "learning_rate": 1.6153549382716053e-05,
      "loss": 0.3246,
      "step": 6980
    },
    {
      "epoch": 0.19262566137566137,
      "grad_norm": 0.06235024333000183,
      "learning_rate": 1.6148037918871255e-05,
      "loss": 0.2124,
      "step": 6990
    },
    {
      "epoch": 0.19290123456790123,
      "grad_norm": 5.724960803985596,
      "learning_rate": 1.6142526455026456e-05,
      "loss": 0.2316,
      "step": 7000
    },
    {
      "epoch": 0.1931768077601411,
      "grad_norm": 16.466203689575195,
      "learning_rate": 1.6137014991181658e-05,
      "loss": 0.0281,
      "step": 7010
    },
    {
      "epoch": 0.19345238095238096,
      "grad_norm": 0.2445182502269745,
      "learning_rate": 1.6131503527336863e-05,
      "loss": 0.1725,
      "step": 7020
    },
    {
      "epoch": 0.19372795414462082,
      "grad_norm": 0.1144706979393959,
      "learning_rate": 1.6125992063492064e-05,
      "loss": 0.2687,
      "step": 7030
    },
    {
      "epoch": 0.19400352733686066,
      "grad_norm": 0.12670469284057617,
      "learning_rate": 1.612048059964727e-05,
      "loss": 0.2332,
      "step": 7040
    },
    {
      "epoch": 0.19427910052910052,
      "grad_norm": 0.38103508949279785,
      "learning_rate": 1.611496913580247e-05,
      "loss": 0.1268,
      "step": 7050
    },
    {
      "epoch": 0.1945546737213404,
      "grad_norm": 0.040503568947315216,
      "learning_rate": 1.6109457671957672e-05,
      "loss": 0.0077,
      "step": 7060
    },
    {
      "epoch": 0.19483024691358025,
      "grad_norm": 0.06461784243583679,
      "learning_rate": 1.6103946208112874e-05,
      "loss": 0.0568,
      "step": 7070
    },
    {
      "epoch": 0.19510582010582012,
      "grad_norm": 0.05336441099643707,
      "learning_rate": 1.609843474426808e-05,
      "loss": 0.3796,
      "step": 7080
    },
    {
      "epoch": 0.19538139329805995,
      "grad_norm": 0.06591349095106125,
      "learning_rate": 1.6092923280423284e-05,
      "loss": 0.1885,
      "step": 7090
    },
    {
      "epoch": 0.19565696649029982,
      "grad_norm": 0.04752245917916298,
      "learning_rate": 1.6087411816578485e-05,
      "loss": 0.0872,
      "step": 7100
    },
    {
      "epoch": 0.19593253968253968,
      "grad_norm": 0.05953918397426605,
      "learning_rate": 1.6081900352733687e-05,
      "loss": 0.0284,
      "step": 7110
    },
    {
      "epoch": 0.19620811287477954,
      "grad_norm": 8.014813423156738,
      "learning_rate": 1.607638888888889e-05,
      "loss": 0.1083,
      "step": 7120
    },
    {
      "epoch": 0.1964836860670194,
      "grad_norm": 36.24344253540039,
      "learning_rate": 1.6070877425044093e-05,
      "loss": 0.2223,
      "step": 7130
    },
    {
      "epoch": 0.19675925925925927,
      "grad_norm": 1.1887993812561035,
      "learning_rate": 1.6065365961199295e-05,
      "loss": 0.1565,
      "step": 7140
    },
    {
      "epoch": 0.1970348324514991,
      "grad_norm": 0.07737832516431808,
      "learning_rate": 1.60598544973545e-05,
      "loss": 0.0838,
      "step": 7150
    },
    {
      "epoch": 0.19731040564373897,
      "grad_norm": 0.04674872010946274,
      "learning_rate": 1.60543430335097e-05,
      "loss": 0.2479,
      "step": 7160
    },
    {
      "epoch": 0.19758597883597884,
      "grad_norm": 0.03802661970257759,
      "learning_rate": 1.6048831569664903e-05,
      "loss": 0.2252,
      "step": 7170
    },
    {
      "epoch": 0.1978615520282187,
      "grad_norm": 42.0389289855957,
      "learning_rate": 1.6043320105820108e-05,
      "loss": 0.2014,
      "step": 7180
    },
    {
      "epoch": 0.19813712522045857,
      "grad_norm": 0.1718631237745285,
      "learning_rate": 1.603780864197531e-05,
      "loss": 0.1478,
      "step": 7190
    },
    {
      "epoch": 0.1984126984126984,
      "grad_norm": 44.64534378051758,
      "learning_rate": 1.6032297178130515e-05,
      "loss": 0.137,
      "step": 7200
    },
    {
      "epoch": 0.19868827160493827,
      "grad_norm": 1.5478956699371338,
      "learning_rate": 1.6026785714285716e-05,
      "loss": 0.0725,
      "step": 7210
    },
    {
      "epoch": 0.19896384479717813,
      "grad_norm": 1.3833590745925903,
      "learning_rate": 1.6021274250440918e-05,
      "loss": 0.0351,
      "step": 7220
    },
    {
      "epoch": 0.199239417989418,
      "grad_norm": 0.15356549620628357,
      "learning_rate": 1.601576278659612e-05,
      "loss": 0.1354,
      "step": 7230
    },
    {
      "epoch": 0.19951499118165786,
      "grad_norm": 2.34800386428833,
      "learning_rate": 1.6010251322751324e-05,
      "loss": 0.0628,
      "step": 7240
    },
    {
      "epoch": 0.1997905643738977,
      "grad_norm": 79.1878890991211,
      "learning_rate": 1.6004739858906526e-05,
      "loss": 0.2441,
      "step": 7250
    },
    {
      "epoch": 0.20006613756613756,
      "grad_norm": 0.03799334540963173,
      "learning_rate": 1.599922839506173e-05,
      "loss": 0.0387,
      "step": 7260
    },
    {
      "epoch": 0.20034171075837742,
      "grad_norm": 36.95280075073242,
      "learning_rate": 1.5993716931216932e-05,
      "loss": 0.1822,
      "step": 7270
    },
    {
      "epoch": 0.2006172839506173,
      "grad_norm": 95.25663757324219,
      "learning_rate": 1.5988205467372134e-05,
      "loss": 0.2422,
      "step": 7280
    },
    {
      "epoch": 0.20089285714285715,
      "grad_norm": 0.03494652360677719,
      "learning_rate": 1.598269400352734e-05,
      "loss": 0.2511,
      "step": 7290
    },
    {
      "epoch": 0.20116843033509701,
      "grad_norm": 0.0401153638958931,
      "learning_rate": 1.597718253968254e-05,
      "loss": 0.0763,
      "step": 7300
    },
    {
      "epoch": 0.20144400352733685,
      "grad_norm": 1.6414023637771606,
      "learning_rate": 1.5971671075837745e-05,
      "loss": 0.0038,
      "step": 7310
    },
    {
      "epoch": 0.20171957671957672,
      "grad_norm": 0.41440343856811523,
      "learning_rate": 1.5966159611992947e-05,
      "loss": 0.0822,
      "step": 7320
    },
    {
      "epoch": 0.20199514991181658,
      "grad_norm": 45.79238510131836,
      "learning_rate": 1.596064814814815e-05,
      "loss": 0.1397,
      "step": 7330
    },
    {
      "epoch": 0.20227072310405644,
      "grad_norm": 0.03651593253016472,
      "learning_rate": 1.595513668430335e-05,
      "loss": 0.0849,
      "step": 7340
    },
    {
      "epoch": 0.2025462962962963,
      "grad_norm": 0.05676575005054474,
      "learning_rate": 1.5949625220458555e-05,
      "loss": 0.035,
      "step": 7350
    },
    {
      "epoch": 0.20282186948853614,
      "grad_norm": 0.038413647562265396,
      "learning_rate": 1.5944113756613757e-05,
      "loss": 0.0526,
      "step": 7360
    },
    {
      "epoch": 0.203097442680776,
      "grad_norm": 114.79791259765625,
      "learning_rate": 1.593860229276896e-05,
      "loss": 0.1544,
      "step": 7370
    },
    {
      "epoch": 0.20337301587301587,
      "grad_norm": 1.358378529548645,
      "learning_rate": 1.5933090828924163e-05,
      "loss": 0.2586,
      "step": 7380
    },
    {
      "epoch": 0.20364858906525574,
      "grad_norm": 0.04382174834609032,
      "learning_rate": 1.5927579365079368e-05,
      "loss": 0.095,
      "step": 7390
    },
    {
      "epoch": 0.2039241622574956,
      "grad_norm": 56.13475036621094,
      "learning_rate": 1.592206790123457e-05,
      "loss": 0.2034,
      "step": 7400
    },
    {
      "epoch": 0.20419973544973544,
      "grad_norm": 0.032174281775951385,
      "learning_rate": 1.591655643738977e-05,
      "loss": 0.2507,
      "step": 7410
    },
    {
      "epoch": 0.2044753086419753,
      "grad_norm": 0.035345807671546936,
      "learning_rate": 1.5911044973544976e-05,
      "loss": 0.1749,
      "step": 7420
    },
    {
      "epoch": 0.20475088183421516,
      "grad_norm": 0.03845781460404396,
      "learning_rate": 1.5905533509700178e-05,
      "loss": 0.0037,
      "step": 7430
    },
    {
      "epoch": 0.20502645502645503,
      "grad_norm": 0.15802232921123505,
      "learning_rate": 1.5900022045855383e-05,
      "loss": 0.1368,
      "step": 7440
    },
    {
      "epoch": 0.2053020282186949,
      "grad_norm": 0.11242037266492844,
      "learning_rate": 1.5894510582010584e-05,
      "loss": 0.0199,
      "step": 7450
    },
    {
      "epoch": 0.20557760141093476,
      "grad_norm": 0.08061011135578156,
      "learning_rate": 1.5888999118165786e-05,
      "loss": 0.0795,
      "step": 7460
    },
    {
      "epoch": 0.2058531746031746,
      "grad_norm": 73.98992919921875,
      "learning_rate": 1.5883487654320987e-05,
      "loss": 0.2517,
      "step": 7470
    },
    {
      "epoch": 0.20612874779541446,
      "grad_norm": 0.23911702632904053,
      "learning_rate": 1.5877976190476192e-05,
      "loss": 0.2455,
      "step": 7480
    },
    {
      "epoch": 0.20640432098765432,
      "grad_norm": 9.67110538482666,
      "learning_rate": 1.5872464726631394e-05,
      "loss": 0.1271,
      "step": 7490
    },
    {
      "epoch": 0.20667989417989419,
      "grad_norm": 167.92213439941406,
      "learning_rate": 1.58669532627866e-05,
      "loss": 0.0299,
      "step": 7500
    },
    {
      "epoch": 0.20695546737213405,
      "grad_norm": 0.040526144206523895,
      "learning_rate": 1.58614417989418e-05,
      "loss": 0.0732,
      "step": 7510
    },
    {
      "epoch": 0.2072310405643739,
      "grad_norm": 2.143131971359253,
      "learning_rate": 1.5855930335097002e-05,
      "loss": 0.1511,
      "step": 7520
    },
    {
      "epoch": 0.20750661375661375,
      "grad_norm": 19.56672477722168,
      "learning_rate": 1.5850418871252207e-05,
      "loss": 0.2051,
      "step": 7530
    },
    {
      "epoch": 0.20778218694885361,
      "grad_norm": 0.42654848098754883,
      "learning_rate": 1.584490740740741e-05,
      "loss": 0.1893,
      "step": 7540
    },
    {
      "epoch": 0.20805776014109348,
      "grad_norm": 0.15088586509227753,
      "learning_rate": 1.5839395943562613e-05,
      "loss": 0.1081,
      "step": 7550
    },
    {
      "epoch": 0.20833333333333334,
      "grad_norm": 0.04394519329071045,
      "learning_rate": 1.5833884479717815e-05,
      "loss": 0.3674,
      "step": 7560
    },
    {
      "epoch": 0.20860890652557318,
      "grad_norm": 0.19426797330379486,
      "learning_rate": 1.5828373015873017e-05,
      "loss": 0.1353,
      "step": 7570
    },
    {
      "epoch": 0.20888447971781304,
      "grad_norm": 0.09172482788562775,
      "learning_rate": 1.5822861552028218e-05,
      "loss": 0.1341,
      "step": 7580
    },
    {
      "epoch": 0.2091600529100529,
      "grad_norm": 19.247880935668945,
      "learning_rate": 1.5817350088183423e-05,
      "loss": 0.1437,
      "step": 7590
    },
    {
      "epoch": 0.20943562610229277,
      "grad_norm": 0.10866256803274155,
      "learning_rate": 1.5811838624338625e-05,
      "loss": 0.0759,
      "step": 7600
    },
    {
      "epoch": 0.20971119929453264,
      "grad_norm": 0.04723439738154411,
      "learning_rate": 1.580632716049383e-05,
      "loss": 0.1443,
      "step": 7610
    },
    {
      "epoch": 0.2099867724867725,
      "grad_norm": 1.1059678792953491,
      "learning_rate": 1.580081569664903e-05,
      "loss": 0.0031,
      "step": 7620
    },
    {
      "epoch": 0.21026234567901234,
      "grad_norm": 5.841726779937744,
      "learning_rate": 1.5795304232804233e-05,
      "loss": 0.1604,
      "step": 7630
    },
    {
      "epoch": 0.2105379188712522,
      "grad_norm": 0.03389224037528038,
      "learning_rate": 1.5789792768959438e-05,
      "loss": 0.0964,
      "step": 7640
    },
    {
      "epoch": 0.21081349206349206,
      "grad_norm": 0.5766028165817261,
      "learning_rate": 1.578428130511464e-05,
      "loss": 0.0576,
      "step": 7650
    },
    {
      "epoch": 0.21108906525573193,
      "grad_norm": 1.2989691495895386,
      "learning_rate": 1.5778769841269844e-05,
      "loss": 0.0658,
      "step": 7660
    },
    {
      "epoch": 0.2113646384479718,
      "grad_norm": 0.03803114965558052,
      "learning_rate": 1.5773258377425046e-05,
      "loss": 0.1905,
      "step": 7670
    },
    {
      "epoch": 0.21164021164021163,
      "grad_norm": 26.702125549316406,
      "learning_rate": 1.5767746913580247e-05,
      "loss": 0.0193,
      "step": 7680
    },
    {
      "epoch": 0.2119157848324515,
      "grad_norm": 0.058061059564352036,
      "learning_rate": 1.576223544973545e-05,
      "loss": 0.1483,
      "step": 7690
    },
    {
      "epoch": 0.21219135802469136,
      "grad_norm": 0.1149030476808548,
      "learning_rate": 1.5756723985890654e-05,
      "loss": 0.052,
      "step": 7700
    },
    {
      "epoch": 0.21246693121693122,
      "grad_norm": 0.07350818812847137,
      "learning_rate": 1.5751212522045855e-05,
      "loss": 0.0817,
      "step": 7710
    },
    {
      "epoch": 0.21274250440917108,
      "grad_norm": 0.17368239164352417,
      "learning_rate": 1.574570105820106e-05,
      "loss": 0.2642,
      "step": 7720
    },
    {
      "epoch": 0.21301807760141092,
      "grad_norm": 1.455336332321167,
      "learning_rate": 1.5740189594356262e-05,
      "loss": 0.1676,
      "step": 7730
    },
    {
      "epoch": 0.21329365079365079,
      "grad_norm": 63.450931549072266,
      "learning_rate": 1.5734678130511463e-05,
      "loss": 0.0852,
      "step": 7740
    },
    {
      "epoch": 0.21356922398589065,
      "grad_norm": 7.088642597198486,
      "learning_rate": 1.572916666666667e-05,
      "loss": 0.1985,
      "step": 7750
    },
    {
      "epoch": 0.2138447971781305,
      "grad_norm": 17.623123168945312,
      "learning_rate": 1.572365520282187e-05,
      "loss": 0.1571,
      "step": 7760
    },
    {
      "epoch": 0.21412037037037038,
      "grad_norm": 39.96504592895508,
      "learning_rate": 1.5718143738977075e-05,
      "loss": 0.2717,
      "step": 7770
    },
    {
      "epoch": 0.21439594356261024,
      "grad_norm": 0.09828443825244904,
      "learning_rate": 1.5712632275132276e-05,
      "loss": 0.163,
      "step": 7780
    },
    {
      "epoch": 0.21467151675485008,
      "grad_norm": 0.04712509363889694,
      "learning_rate": 1.5707120811287478e-05,
      "loss": 0.1413,
      "step": 7790
    },
    {
      "epoch": 0.21494708994708994,
      "grad_norm": 0.3942071199417114,
      "learning_rate": 1.570160934744268e-05,
      "loss": 0.2128,
      "step": 7800
    },
    {
      "epoch": 0.2152226631393298,
      "grad_norm": 0.04582810774445534,
      "learning_rate": 1.5696097883597885e-05,
      "loss": 0.0667,
      "step": 7810
    },
    {
      "epoch": 0.21549823633156967,
      "grad_norm": 0.0996505618095398,
      "learning_rate": 1.5690586419753086e-05,
      "loss": 0.0106,
      "step": 7820
    },
    {
      "epoch": 0.21577380952380953,
      "grad_norm": 94.55094909667969,
      "learning_rate": 1.568507495590829e-05,
      "loss": 0.0922,
      "step": 7830
    },
    {
      "epoch": 0.21604938271604937,
      "grad_norm": 56.41054153442383,
      "learning_rate": 1.5679563492063493e-05,
      "loss": 0.1561,
      "step": 7840
    },
    {
      "epoch": 0.21632495590828923,
      "grad_norm": 0.0314638614654541,
      "learning_rate": 1.5674052028218694e-05,
      "loss": 0.2565,
      "step": 7850
    },
    {
      "epoch": 0.2166005291005291,
      "grad_norm": 0.05245677009224892,
      "learning_rate": 1.56685405643739e-05,
      "loss": 0.0588,
      "step": 7860
    },
    {
      "epoch": 0.21687610229276896,
      "grad_norm": 7.700405597686768,
      "learning_rate": 1.56630291005291e-05,
      "loss": 0.2163,
      "step": 7870
    },
    {
      "epoch": 0.21715167548500883,
      "grad_norm": 8.755144119262695,
      "learning_rate": 1.5657517636684306e-05,
      "loss": 0.2171,
      "step": 7880
    },
    {
      "epoch": 0.21742724867724866,
      "grad_norm": 0.09234726428985596,
      "learning_rate": 1.5652006172839507e-05,
      "loss": 0.1135,
      "step": 7890
    },
    {
      "epoch": 0.21770282186948853,
      "grad_norm": 0.053448740392923355,
      "learning_rate": 1.5646494708994712e-05,
      "loss": 0.1492,
      "step": 7900
    },
    {
      "epoch": 0.2179783950617284,
      "grad_norm": 102.49971771240234,
      "learning_rate": 1.5640983245149914e-05,
      "loss": 0.1686,
      "step": 7910
    },
    {
      "epoch": 0.21825396825396826,
      "grad_norm": 0.03693659231066704,
      "learning_rate": 1.5635471781305115e-05,
      "loss": 0.2069,
      "step": 7920
    },
    {
      "epoch": 0.21852954144620812,
      "grad_norm": 67.54317474365234,
      "learning_rate": 1.562996031746032e-05,
      "loss": 0.1623,
      "step": 7930
    },
    {
      "epoch": 0.21880511463844798,
      "grad_norm": 0.05901176854968071,
      "learning_rate": 1.5624448853615522e-05,
      "loss": 0.0752,
      "step": 7940
    },
    {
      "epoch": 0.21908068783068782,
      "grad_norm": 42.1240234375,
      "learning_rate": 1.5618937389770727e-05,
      "loss": 0.1318,
      "step": 7950
    },
    {
      "epoch": 0.21935626102292768,
      "grad_norm": 0.06108420714735985,
      "learning_rate": 1.561342592592593e-05,
      "loss": 0.0656,
      "step": 7960
    },
    {
      "epoch": 0.21963183421516755,
      "grad_norm": 0.0813969224691391,
      "learning_rate": 1.560791446208113e-05,
      "loss": 0.0966,
      "step": 7970
    },
    {
      "epoch": 0.2199074074074074,
      "grad_norm": 5.529428482055664,
      "learning_rate": 1.560240299823633e-05,
      "loss": 0.0712,
      "step": 7980
    },
    {
      "epoch": 0.22018298059964728,
      "grad_norm": 3.429365873336792,
      "learning_rate": 1.5596891534391536e-05,
      "loss": 0.1882,
      "step": 7990
    },
    {
      "epoch": 0.2204585537918871,
      "grad_norm": 71.8287124633789,
      "learning_rate": 1.5591380070546738e-05,
      "loss": 0.2752,
      "step": 8000
    },
    {
      "epoch": 0.22073412698412698,
      "grad_norm": 81.16069030761719,
      "learning_rate": 1.5585868606701943e-05,
      "loss": 0.0718,
      "step": 8010
    },
    {
      "epoch": 0.22100970017636684,
      "grad_norm": 0.967510461807251,
      "learning_rate": 1.5580357142857145e-05,
      "loss": 0.074,
      "step": 8020
    },
    {
      "epoch": 0.2212852733686067,
      "grad_norm": 75.59449768066406,
      "learning_rate": 1.5574845679012346e-05,
      "loss": 0.0135,
      "step": 8030
    },
    {
      "epoch": 0.22156084656084657,
      "grad_norm": 7.071042060852051,
      "learning_rate": 1.556933421516755e-05,
      "loss": 0.0365,
      "step": 8040
    },
    {
      "epoch": 0.22183641975308643,
      "grad_norm": 0.05621866136789322,
      "learning_rate": 1.5563822751322753e-05,
      "loss": 0.0924,
      "step": 8050
    },
    {
      "epoch": 0.22211199294532627,
      "grad_norm": 0.04620715230703354,
      "learning_rate": 1.5558311287477958e-05,
      "loss": 0.0885,
      "step": 8060
    },
    {
      "epoch": 0.22238756613756613,
      "grad_norm": 6.603892803192139,
      "learning_rate": 1.555279982363316e-05,
      "loss": 0.0941,
      "step": 8070
    },
    {
      "epoch": 0.222663139329806,
      "grad_norm": 0.21405799686908722,
      "learning_rate": 1.554728835978836e-05,
      "loss": 0.0142,
      "step": 8080
    },
    {
      "epoch": 0.22293871252204586,
      "grad_norm": 0.9030508995056152,
      "learning_rate": 1.5541776895943562e-05,
      "loss": 0.0696,
      "step": 8090
    },
    {
      "epoch": 0.22321428571428573,
      "grad_norm": 0.036986663937568665,
      "learning_rate": 1.5536265432098767e-05,
      "loss": 0.0798,
      "step": 8100
    },
    {
      "epoch": 0.22348985890652556,
      "grad_norm": 0.12371253967285156,
      "learning_rate": 1.553075396825397e-05,
      "loss": 0.227,
      "step": 8110
    },
    {
      "epoch": 0.22376543209876543,
      "grad_norm": 42.210350036621094,
      "learning_rate": 1.5525242504409174e-05,
      "loss": 0.0448,
      "step": 8120
    },
    {
      "epoch": 0.2240410052910053,
      "grad_norm": 0.06272336840629578,
      "learning_rate": 1.5519731040564375e-05,
      "loss": 0.0955,
      "step": 8130
    },
    {
      "epoch": 0.22431657848324515,
      "grad_norm": 0.709223747253418,
      "learning_rate": 1.5514219576719577e-05,
      "loss": 0.0265,
      "step": 8140
    },
    {
      "epoch": 0.22459215167548502,
      "grad_norm": 0.027204597368836403,
      "learning_rate": 1.5508708112874782e-05,
      "loss": 0.1149,
      "step": 8150
    },
    {
      "epoch": 0.22486772486772486,
      "grad_norm": 0.19618992507457733,
      "learning_rate": 1.5503196649029983e-05,
      "loss": 0.2625,
      "step": 8160
    },
    {
      "epoch": 0.22514329805996472,
      "grad_norm": 0.031264059245586395,
      "learning_rate": 1.549768518518519e-05,
      "loss": 0.1107,
      "step": 8170
    },
    {
      "epoch": 0.22541887125220458,
      "grad_norm": 0.033240433782339096,
      "learning_rate": 1.549217372134039e-05,
      "loss": 0.1425,
      "step": 8180
    },
    {
      "epoch": 0.22569444444444445,
      "grad_norm": 0.03302665054798126,
      "learning_rate": 1.548666225749559e-05,
      "loss": 0.0349,
      "step": 8190
    },
    {
      "epoch": 0.2259700176366843,
      "grad_norm": 8.00402545928955,
      "learning_rate": 1.5481150793650793e-05,
      "loss": 0.2065,
      "step": 8200
    },
    {
      "epoch": 0.22624559082892418,
      "grad_norm": 0.03497802093625069,
      "learning_rate": 1.5475639329805998e-05,
      "loss": 0.1384,
      "step": 8210
    },
    {
      "epoch": 0.226521164021164,
      "grad_norm": 0.06003542244434357,
      "learning_rate": 1.54701278659612e-05,
      "loss": 0.196,
      "step": 8220
    },
    {
      "epoch": 0.22679673721340388,
      "grad_norm": 0.027762284502387047,
      "learning_rate": 1.5464616402116404e-05,
      "loss": 0.004,
      "step": 8230
    },
    {
      "epoch": 0.22707231040564374,
      "grad_norm": 37.545127868652344,
      "learning_rate": 1.5459104938271606e-05,
      "loss": 0.1162,
      "step": 8240
    },
    {
      "epoch": 0.2273478835978836,
      "grad_norm": 0.15046262741088867,
      "learning_rate": 1.5453593474426808e-05,
      "loss": 0.2053,
      "step": 8250
    },
    {
      "epoch": 0.22762345679012347,
      "grad_norm": 0.029857799410820007,
      "learning_rate": 1.5448082010582013e-05,
      "loss": 0.0142,
      "step": 8260
    },
    {
      "epoch": 0.2278990299823633,
      "grad_norm": 107.14570617675781,
      "learning_rate": 1.5442570546737214e-05,
      "loss": 0.2355,
      "step": 8270
    },
    {
      "epoch": 0.22817460317460317,
      "grad_norm": 4.384498596191406,
      "learning_rate": 1.543705908289242e-05,
      "loss": 0.21,
      "step": 8280
    },
    {
      "epoch": 0.22845017636684303,
      "grad_norm": 0.38630878925323486,
      "learning_rate": 1.543154761904762e-05,
      "loss": 0.1367,
      "step": 8290
    },
    {
      "epoch": 0.2287257495590829,
      "grad_norm": 0.022392122074961662,
      "learning_rate": 1.5426036155202822e-05,
      "loss": 0.2401,
      "step": 8300
    },
    {
      "epoch": 0.22900132275132276,
      "grad_norm": 0.43026965856552124,
      "learning_rate": 1.5420524691358024e-05,
      "loss": 0.0402,
      "step": 8310
    },
    {
      "epoch": 0.2292768959435626,
      "grad_norm": 179.32070922851562,
      "learning_rate": 1.541501322751323e-05,
      "loss": 0.2353,
      "step": 8320
    },
    {
      "epoch": 0.22955246913580246,
      "grad_norm": 56.6312370300293,
      "learning_rate": 1.540950176366843e-05,
      "loss": 0.3407,
      "step": 8330
    },
    {
      "epoch": 0.22982804232804233,
      "grad_norm": 79.0317153930664,
      "learning_rate": 1.5403990299823635e-05,
      "loss": 0.0171,
      "step": 8340
    },
    {
      "epoch": 0.2301036155202822,
      "grad_norm": 0.20972894132137299,
      "learning_rate": 1.5398478835978837e-05,
      "loss": 0.0663,
      "step": 8350
    },
    {
      "epoch": 0.23037918871252205,
      "grad_norm": 120.14359283447266,
      "learning_rate": 1.539296737213404e-05,
      "loss": 0.0779,
      "step": 8360
    },
    {
      "epoch": 0.23065476190476192,
      "grad_norm": 0.9508647918701172,
      "learning_rate": 1.5387455908289243e-05,
      "loss": 0.0977,
      "step": 8370
    },
    {
      "epoch": 0.23093033509700175,
      "grad_norm": 0.6046791076660156,
      "learning_rate": 1.5381944444444445e-05,
      "loss": 0.077,
      "step": 8380
    },
    {
      "epoch": 0.23120590828924162,
      "grad_norm": 0.1328515261411667,
      "learning_rate": 1.537643298059965e-05,
      "loss": 0.0698,
      "step": 8390
    },
    {
      "epoch": 0.23148148148148148,
      "grad_norm": 1.412335753440857,
      "learning_rate": 1.537092151675485e-05,
      "loss": 0.137,
      "step": 8400
    },
    {
      "epoch": 0.23175705467372135,
      "grad_norm": 0.029766123741865158,
      "learning_rate": 1.5365410052910056e-05,
      "loss": 0.1895,
      "step": 8410
    },
    {
      "epoch": 0.2320326278659612,
      "grad_norm": 2.9720523357391357,
      "learning_rate": 1.5359898589065258e-05,
      "loss": 0.0653,
      "step": 8420
    },
    {
      "epoch": 0.23230820105820105,
      "grad_norm": 0.17441071569919586,
      "learning_rate": 1.535438712522046e-05,
      "loss": 0.1264,
      "step": 8430
    },
    {
      "epoch": 0.2325837742504409,
      "grad_norm": 18.984312057495117,
      "learning_rate": 1.534887566137566e-05,
      "loss": 0.2124,
      "step": 8440
    },
    {
      "epoch": 0.23285934744268078,
      "grad_norm": 88.23226165771484,
      "learning_rate": 1.5343364197530866e-05,
      "loss": 0.1745,
      "step": 8450
    },
    {
      "epoch": 0.23313492063492064,
      "grad_norm": 0.040858130902051926,
      "learning_rate": 1.5337852733686068e-05,
      "loss": 0.0564,
      "step": 8460
    },
    {
      "epoch": 0.2334104938271605,
      "grad_norm": 1.1129369735717773,
      "learning_rate": 1.5332341269841273e-05,
      "loss": 0.1395,
      "step": 8470
    },
    {
      "epoch": 0.23368606701940034,
      "grad_norm": 17.573623657226562,
      "learning_rate": 1.5326829805996474e-05,
      "loss": 0.3013,
      "step": 8480
    },
    {
      "epoch": 0.2339616402116402,
      "grad_norm": 22.86162757873535,
      "learning_rate": 1.5321318342151676e-05,
      "loss": 0.1742,
      "step": 8490
    },
    {
      "epoch": 0.23423721340388007,
      "grad_norm": 19.343950271606445,
      "learning_rate": 1.531580687830688e-05,
      "loss": 0.1481,
      "step": 8500
    },
    {
      "epoch": 0.23451278659611993,
      "grad_norm": 0.03321756049990654,
      "learning_rate": 1.5310295414462082e-05,
      "loss": 0.2473,
      "step": 8510
    },
    {
      "epoch": 0.2347883597883598,
      "grad_norm": 0.04935922846198082,
      "learning_rate": 1.5304783950617287e-05,
      "loss": 0.0932,
      "step": 8520
    },
    {
      "epoch": 0.23506393298059966,
      "grad_norm": 0.11395981162786484,
      "learning_rate": 1.529927248677249e-05,
      "loss": 0.1492,
      "step": 8530
    },
    {
      "epoch": 0.2353395061728395,
      "grad_norm": 1.616886854171753,
      "learning_rate": 1.529376102292769e-05,
      "loss": 0.0754,
      "step": 8540
    },
    {
      "epoch": 0.23561507936507936,
      "grad_norm": 136.29664611816406,
      "learning_rate": 1.5288249559082892e-05,
      "loss": 0.1416,
      "step": 8550
    },
    {
      "epoch": 0.23589065255731922,
      "grad_norm": 0.28156575560569763,
      "learning_rate": 1.5282738095238097e-05,
      "loss": 0.1488,
      "step": 8560
    },
    {
      "epoch": 0.2361662257495591,
      "grad_norm": 0.036821383982896805,
      "learning_rate": 1.52772266313933e-05,
      "loss": 0.0355,
      "step": 8570
    },
    {
      "epoch": 0.23644179894179895,
      "grad_norm": 0.04918554797768593,
      "learning_rate": 1.5271715167548503e-05,
      "loss": 0.0181,
      "step": 8580
    },
    {
      "epoch": 0.2367173721340388,
      "grad_norm": 0.03282642364501953,
      "learning_rate": 1.5266203703703705e-05,
      "loss": 0.1016,
      "step": 8590
    },
    {
      "epoch": 0.23699294532627865,
      "grad_norm": 67.8595962524414,
      "learning_rate": 1.5260692239858906e-05,
      "loss": 0.1743,
      "step": 8600
    },
    {
      "epoch": 0.23726851851851852,
      "grad_norm": 0.5942448973655701,
      "learning_rate": 1.5255180776014111e-05,
      "loss": 0.0135,
      "step": 8610
    },
    {
      "epoch": 0.23754409171075838,
      "grad_norm": 0.03277002274990082,
      "learning_rate": 1.5249669312169313e-05,
      "loss": 0.1859,
      "step": 8620
    },
    {
      "epoch": 0.23781966490299825,
      "grad_norm": 0.034987665712833405,
      "learning_rate": 1.5244157848324516e-05,
      "loss": 0.1319,
      "step": 8630
    },
    {
      "epoch": 0.23809523809523808,
      "grad_norm": 29.009553909301758,
      "learning_rate": 1.5238646384479718e-05,
      "loss": 0.1541,
      "step": 8640
    },
    {
      "epoch": 0.23837081128747795,
      "grad_norm": 0.06820648163557053,
      "learning_rate": 1.5233134920634923e-05,
      "loss": 0.1565,
      "step": 8650
    },
    {
      "epoch": 0.2386463844797178,
      "grad_norm": 0.056510310620069504,
      "learning_rate": 1.5227623456790124e-05,
      "loss": 0.1024,
      "step": 8660
    },
    {
      "epoch": 0.23892195767195767,
      "grad_norm": 0.04767525568604469,
      "learning_rate": 1.5222111992945328e-05,
      "loss": 0.1171,
      "step": 8670
    },
    {
      "epoch": 0.23919753086419754,
      "grad_norm": 0.0903518944978714,
      "learning_rate": 1.521660052910053e-05,
      "loss": 0.1899,
      "step": 8680
    },
    {
      "epoch": 0.2394731040564374,
      "grad_norm": 0.20960378646850586,
      "learning_rate": 1.5211089065255732e-05,
      "loss": 0.2018,
      "step": 8690
    },
    {
      "epoch": 0.23974867724867724,
      "grad_norm": 4.792473316192627,
      "learning_rate": 1.5205577601410937e-05,
      "loss": 0.1524,
      "step": 8700
    },
    {
      "epoch": 0.2400242504409171,
      "grad_norm": 0.06459466367959976,
      "learning_rate": 1.5200066137566139e-05,
      "loss": 0.1805,
      "step": 8710
    },
    {
      "epoch": 0.24029982363315697,
      "grad_norm": 0.08703923970460892,
      "learning_rate": 1.5194554673721342e-05,
      "loss": 0.0653,
      "step": 8720
    },
    {
      "epoch": 0.24057539682539683,
      "grad_norm": 109.26681518554688,
      "learning_rate": 1.5189043209876544e-05,
      "loss": 0.2594,
      "step": 8730
    },
    {
      "epoch": 0.2408509700176367,
      "grad_norm": 0.18205790221691132,
      "learning_rate": 1.5183531746031747e-05,
      "loss": 0.0364,
      "step": 8740
    },
    {
      "epoch": 0.24112654320987653,
      "grad_norm": 4.818272113800049,
      "learning_rate": 1.5178020282186949e-05,
      "loss": 0.126,
      "step": 8750
    },
    {
      "epoch": 0.2414021164021164,
      "grad_norm": 0.04060852527618408,
      "learning_rate": 1.5172508818342153e-05,
      "loss": 0.209,
      "step": 8760
    },
    {
      "epoch": 0.24167768959435626,
      "grad_norm": 0.11707890033721924,
      "learning_rate": 1.5166997354497355e-05,
      "loss": 0.262,
      "step": 8770
    },
    {
      "epoch": 0.24195326278659612,
      "grad_norm": 0.037421807646751404,
      "learning_rate": 1.5161485890652558e-05,
      "loss": 0.1629,
      "step": 8780
    },
    {
      "epoch": 0.242228835978836,
      "grad_norm": 0.03226866573095322,
      "learning_rate": 1.5155974426807763e-05,
      "loss": 0.2763,
      "step": 8790
    },
    {
      "epoch": 0.24250440917107582,
      "grad_norm": 0.030263055115938187,
      "learning_rate": 1.5150462962962965e-05,
      "loss": 0.1736,
      "step": 8800
    },
    {
      "epoch": 0.2427799823633157,
      "grad_norm": 0.20655734837055206,
      "learning_rate": 1.5144951499118168e-05,
      "loss": 0.2739,
      "step": 8810
    },
    {
      "epoch": 0.24305555555555555,
      "grad_norm": 16.43254280090332,
      "learning_rate": 1.513944003527337e-05,
      "loss": 0.217,
      "step": 8820
    },
    {
      "epoch": 0.24333112874779542,
      "grad_norm": 0.03202617168426514,
      "learning_rate": 1.5133928571428573e-05,
      "loss": 0.006,
      "step": 8830
    },
    {
      "epoch": 0.24360670194003528,
      "grad_norm": 22.051513671875,
      "learning_rate": 1.5128417107583774e-05,
      "loss": 0.1429,
      "step": 8840
    },
    {
      "epoch": 0.24388227513227514,
      "grad_norm": 6.661655426025391,
      "learning_rate": 1.512290564373898e-05,
      "loss": 0.1259,
      "step": 8850
    },
    {
      "epoch": 0.24415784832451498,
      "grad_norm": 0.14067722856998444,
      "learning_rate": 1.5117394179894181e-05,
      "loss": 0.0811,
      "step": 8860
    },
    {
      "epoch": 0.24443342151675485,
      "grad_norm": 0.04055643454194069,
      "learning_rate": 1.5111882716049384e-05,
      "loss": 0.0196,
      "step": 8870
    },
    {
      "epoch": 0.2447089947089947,
      "grad_norm": 0.26007986068725586,
      "learning_rate": 1.5106371252204586e-05,
      "loss": 0.0967,
      "step": 8880
    },
    {
      "epoch": 0.24498456790123457,
      "grad_norm": 0.05384933948516846,
      "learning_rate": 1.5100859788359789e-05,
      "loss": 0.121,
      "step": 8890
    },
    {
      "epoch": 0.24526014109347444,
      "grad_norm": 63.46101379394531,
      "learning_rate": 1.5095348324514994e-05,
      "loss": 0.0121,
      "step": 8900
    },
    {
      "epoch": 0.24553571428571427,
      "grad_norm": 0.08567783236503601,
      "learning_rate": 1.5089836860670196e-05,
      "loss": 0.2,
      "step": 8910
    },
    {
      "epoch": 0.24581128747795414,
      "grad_norm": 0.08314923197031021,
      "learning_rate": 1.5084325396825399e-05,
      "loss": 0.0874,
      "step": 8920
    },
    {
      "epoch": 0.246086860670194,
      "grad_norm": 0.031759075820446014,
      "learning_rate": 1.50788139329806e-05,
      "loss": 0.1184,
      "step": 8930
    },
    {
      "epoch": 0.24636243386243387,
      "grad_norm": 0.037313323467969894,
      "learning_rate": 1.5073302469135804e-05,
      "loss": 0.0024,
      "step": 8940
    },
    {
      "epoch": 0.24663800705467373,
      "grad_norm": 1.59908127784729,
      "learning_rate": 1.5067791005291005e-05,
      "loss": 0.1247,
      "step": 8950
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 0.050389956682920456,
      "learning_rate": 1.506227954144621e-05,
      "loss": 0.2546,
      "step": 8960
    },
    {
      "epoch": 0.24718915343915343,
      "grad_norm": 0.07909447699785233,
      "learning_rate": 1.5056768077601412e-05,
      "loss": 0.0816,
      "step": 8970
    },
    {
      "epoch": 0.2474647266313933,
      "grad_norm": 1.09756338596344,
      "learning_rate": 1.5051256613756615e-05,
      "loss": 0.1069,
      "step": 8980
    },
    {
      "epoch": 0.24774029982363316,
      "grad_norm": 0.05775550380349159,
      "learning_rate": 1.5045745149911817e-05,
      "loss": 0.066,
      "step": 8990
    },
    {
      "epoch": 0.24801587301587302,
      "grad_norm": 2.3018457889556885,
      "learning_rate": 1.504023368606702e-05,
      "loss": 0.0592,
      "step": 9000
    },
    {
      "epoch": 0.2482914462081129,
      "grad_norm": 0.05844256654381752,
      "learning_rate": 1.5034722222222225e-05,
      "loss": 0.1393,
      "step": 9010
    },
    {
      "epoch": 0.24856701940035272,
      "grad_norm": 18.83705711364746,
      "learning_rate": 1.5029210758377426e-05,
      "loss": 0.245,
      "step": 9020
    },
    {
      "epoch": 0.2488425925925926,
      "grad_norm": 0.02818099595606327,
      "learning_rate": 1.502369929453263e-05,
      "loss": 0.4097,
      "step": 9030
    },
    {
      "epoch": 0.24911816578483245,
      "grad_norm": 0.048250094056129456,
      "learning_rate": 1.5018187830687831e-05,
      "loss": 0.3785,
      "step": 9040
    },
    {
      "epoch": 0.24939373897707232,
      "grad_norm": 110.27848052978516,
      "learning_rate": 1.5012676366843036e-05,
      "loss": 0.1773,
      "step": 9050
    },
    {
      "epoch": 0.24966931216931218,
      "grad_norm": 0.03775840252637863,
      "learning_rate": 1.5007164902998238e-05,
      "loss": 0.1098,
      "step": 9060
    },
    {
      "epoch": 0.24994488536155202,
      "grad_norm": 76.12902069091797,
      "learning_rate": 1.5001653439153441e-05,
      "loss": 0.0834,
      "step": 9070
    },
    {
      "epoch": 0.2502204585537919,
      "grad_norm": 0.04078006371855736,
      "learning_rate": 1.4996141975308642e-05,
      "loss": 0.0776,
      "step": 9080
    },
    {
      "epoch": 0.25049603174603174,
      "grad_norm": 0.04166170582175255,
      "learning_rate": 1.4990630511463846e-05,
      "loss": 0.0899,
      "step": 9090
    },
    {
      "epoch": 0.2507716049382716,
      "grad_norm": 0.05248059332370758,
      "learning_rate": 1.4985119047619047e-05,
      "loss": 0.0221,
      "step": 9100
    },
    {
      "epoch": 0.2510471781305115,
      "grad_norm": 122.94986724853516,
      "learning_rate": 1.4979607583774252e-05,
      "loss": 0.194,
      "step": 9110
    },
    {
      "epoch": 0.25132275132275134,
      "grad_norm": 10.775205612182617,
      "learning_rate": 1.4974096119929456e-05,
      "loss": 0.0724,
      "step": 9120
    },
    {
      "epoch": 0.2515983245149912,
      "grad_norm": 11.172750473022461,
      "learning_rate": 1.4968584656084657e-05,
      "loss": 0.0745,
      "step": 9130
    },
    {
      "epoch": 0.25187389770723106,
      "grad_norm": 0.05199790373444557,
      "learning_rate": 1.496307319223986e-05,
      "loss": 0.0125,
      "step": 9140
    },
    {
      "epoch": 0.2521494708994709,
      "grad_norm": 0.09723856300115585,
      "learning_rate": 1.4957561728395062e-05,
      "loss": 0.1578,
      "step": 9150
    },
    {
      "epoch": 0.25242504409171074,
      "grad_norm": 0.4936153292655945,
      "learning_rate": 1.4952050264550267e-05,
      "loss": 0.0705,
      "step": 9160
    },
    {
      "epoch": 0.2527006172839506,
      "grad_norm": 0.06943850219249725,
      "learning_rate": 1.4946538800705468e-05,
      "loss": 0.0023,
      "step": 9170
    },
    {
      "epoch": 0.25297619047619047,
      "grad_norm": 83.21712493896484,
      "learning_rate": 1.4941027336860672e-05,
      "loss": 0.1248,
      "step": 9180
    },
    {
      "epoch": 0.25325176366843033,
      "grad_norm": 96.02444458007812,
      "learning_rate": 1.4935515873015873e-05,
      "loss": 0.065,
      "step": 9190
    },
    {
      "epoch": 0.2535273368606702,
      "grad_norm": 0.0362018346786499,
      "learning_rate": 1.4930004409171077e-05,
      "loss": 0.0762,
      "step": 9200
    },
    {
      "epoch": 0.25380291005291006,
      "grad_norm": 0.11703301221132278,
      "learning_rate": 1.4924492945326278e-05,
      "loss": 0.3283,
      "step": 9210
    },
    {
      "epoch": 0.2540784832451499,
      "grad_norm": 0.6124374866485596,
      "learning_rate": 1.4918981481481483e-05,
      "loss": 0.1573,
      "step": 9220
    },
    {
      "epoch": 0.2543540564373898,
      "grad_norm": 0.09418048709630966,
      "learning_rate": 1.4913470017636686e-05,
      "loss": 0.0964,
      "step": 9230
    },
    {
      "epoch": 0.25462962962962965,
      "grad_norm": 0.2576640546321869,
      "learning_rate": 1.4907958553791888e-05,
      "loss": 0.0029,
      "step": 9240
    },
    {
      "epoch": 0.2549052028218695,
      "grad_norm": 0.03251892328262329,
      "learning_rate": 1.4902447089947091e-05,
      "loss": 0.2236,
      "step": 9250
    },
    {
      "epoch": 0.2551807760141093,
      "grad_norm": 2.505192279815674,
      "learning_rate": 1.4896935626102293e-05,
      "loss": 0.2669,
      "step": 9260
    },
    {
      "epoch": 0.2554563492063492,
      "grad_norm": 111.5595474243164,
      "learning_rate": 1.4891424162257498e-05,
      "loss": 0.0877,
      "step": 9270
    },
    {
      "epoch": 0.25573192239858905,
      "grad_norm": 0.027623137459158897,
      "learning_rate": 1.48859126984127e-05,
      "loss": 0.0656,
      "step": 9280
    },
    {
      "epoch": 0.2560074955908289,
      "grad_norm": 0.1374809741973877,
      "learning_rate": 1.4880401234567902e-05,
      "loss": 0.104,
      "step": 9290
    },
    {
      "epoch": 0.2562830687830688,
      "grad_norm": 126.4664535522461,
      "learning_rate": 1.4874889770723104e-05,
      "loss": 0.09,
      "step": 9300
    },
    {
      "epoch": 0.25655864197530864,
      "grad_norm": 145.93406677246094,
      "learning_rate": 1.4869378306878309e-05,
      "loss": 0.1616,
      "step": 9310
    },
    {
      "epoch": 0.2568342151675485,
      "grad_norm": 1.0561997890472412,
      "learning_rate": 1.4863866843033512e-05,
      "loss": 0.0022,
      "step": 9320
    },
    {
      "epoch": 0.25710978835978837,
      "grad_norm": 0.05109281465411186,
      "learning_rate": 1.4858355379188714e-05,
      "loss": 0.1021,
      "step": 9330
    },
    {
      "epoch": 0.25738536155202824,
      "grad_norm": 24.185340881347656,
      "learning_rate": 1.4852843915343917e-05,
      "loss": 0.3221,
      "step": 9340
    },
    {
      "epoch": 0.2576609347442681,
      "grad_norm": 0.026642724871635437,
      "learning_rate": 1.4847332451499119e-05,
      "loss": 0.1339,
      "step": 9350
    },
    {
      "epoch": 0.25793650793650796,
      "grad_norm": 50.47445297241211,
      "learning_rate": 1.4841820987654324e-05,
      "loss": 0.0172,
      "step": 9360
    },
    {
      "epoch": 0.2582120811287478,
      "grad_norm": 112.7108383178711,
      "learning_rate": 1.4836309523809525e-05,
      "loss": 0.0447,
      "step": 9370
    },
    {
      "epoch": 0.25848765432098764,
      "grad_norm": 0.029718080535531044,
      "learning_rate": 1.4830798059964728e-05,
      "loss": 0.0646,
      "step": 9380
    },
    {
      "epoch": 0.2587632275132275,
      "grad_norm": 0.029151085764169693,
      "learning_rate": 1.482528659611993e-05,
      "loss": 0.1431,
      "step": 9390
    },
    {
      "epoch": 0.25903880070546736,
      "grad_norm": 0.08929701149463654,
      "learning_rate": 1.4819775132275133e-05,
      "loss": 0.3641,
      "step": 9400
    },
    {
      "epoch": 0.25931437389770723,
      "grad_norm": 4.98059606552124,
      "learning_rate": 1.4814263668430335e-05,
      "loss": 0.126,
      "step": 9410
    },
    {
      "epoch": 0.2595899470899471,
      "grad_norm": 0.044370923191308975,
      "learning_rate": 1.480875220458554e-05,
      "loss": 0.1057,
      "step": 9420
    },
    {
      "epoch": 0.25986552028218696,
      "grad_norm": 0.24127168953418732,
      "learning_rate": 1.4803240740740743e-05,
      "loss": 0.0314,
      "step": 9430
    },
    {
      "epoch": 0.2601410934744268,
      "grad_norm": 0.12694668769836426,
      "learning_rate": 1.4797729276895945e-05,
      "loss": 0.1565,
      "step": 9440
    },
    {
      "epoch": 0.2604166666666667,
      "grad_norm": 11.65156078338623,
      "learning_rate": 1.4792217813051148e-05,
      "loss": 0.2192,
      "step": 9450
    },
    {
      "epoch": 0.26069223985890655,
      "grad_norm": 0.05849674716591835,
      "learning_rate": 1.478670634920635e-05,
      "loss": 0.2591,
      "step": 9460
    },
    {
      "epoch": 0.26096781305114636,
      "grad_norm": 0.04567835107445717,
      "learning_rate": 1.4781194885361554e-05,
      "loss": 0.2103,
      "step": 9470
    },
    {
      "epoch": 0.2612433862433862,
      "grad_norm": 92.2568359375,
      "learning_rate": 1.4775683421516756e-05,
      "loss": 0.1771,
      "step": 9480
    },
    {
      "epoch": 0.2615189594356261,
      "grad_norm": 0.025131402537226677,
      "learning_rate": 1.4770171957671959e-05,
      "loss": 0.0842,
      "step": 9490
    },
    {
      "epoch": 0.26179453262786595,
      "grad_norm": 0.15238890051841736,
      "learning_rate": 1.476466049382716e-05,
      "loss": 0.1405,
      "step": 9500
    },
    {
      "epoch": 0.2620701058201058,
      "grad_norm": 0.09112222492694855,
      "learning_rate": 1.4759149029982364e-05,
      "loss": 0.0202,
      "step": 9510
    },
    {
      "epoch": 0.2623456790123457,
      "grad_norm": 0.043899569660425186,
      "learning_rate": 1.4753637566137566e-05,
      "loss": 0.0028,
      "step": 9520
    },
    {
      "epoch": 0.26262125220458554,
      "grad_norm": 0.07832485437393188,
      "learning_rate": 1.474812610229277e-05,
      "loss": 0.093,
      "step": 9530
    },
    {
      "epoch": 0.2628968253968254,
      "grad_norm": 0.04530365765094757,
      "learning_rate": 1.4742614638447974e-05,
      "loss": 0.1717,
      "step": 9540
    },
    {
      "epoch": 0.26317239858906527,
      "grad_norm": 75.08981323242188,
      "learning_rate": 1.4737103174603175e-05,
      "loss": 0.0244,
      "step": 9550
    },
    {
      "epoch": 0.26344797178130513,
      "grad_norm": 0.03150749206542969,
      "learning_rate": 1.473159171075838e-05,
      "loss": 0.141,
      "step": 9560
    },
    {
      "epoch": 0.263723544973545,
      "grad_norm": 0.06366221606731415,
      "learning_rate": 1.4726080246913582e-05,
      "loss": 0.1358,
      "step": 9570
    },
    {
      "epoch": 0.2639991181657848,
      "grad_norm": 0.08388683944940567,
      "learning_rate": 1.4720568783068785e-05,
      "loss": 0.0672,
      "step": 9580
    },
    {
      "epoch": 0.26427469135802467,
      "grad_norm": 0.06512529402971268,
      "learning_rate": 1.4715057319223987e-05,
      "loss": 0.1687,
      "step": 9590
    },
    {
      "epoch": 0.26455026455026454,
      "grad_norm": 0.12649792432785034,
      "learning_rate": 1.470954585537919e-05,
      "loss": 0.1147,
      "step": 9600
    },
    {
      "epoch": 0.2648258377425044,
      "grad_norm": 0.24469347298145294,
      "learning_rate": 1.4704034391534391e-05,
      "loss": 0.0245,
      "step": 9610
    },
    {
      "epoch": 0.26510141093474426,
      "grad_norm": 0.1612132340669632,
      "learning_rate": 1.4698522927689596e-05,
      "loss": 0.0844,
      "step": 9620
    },
    {
      "epoch": 0.26537698412698413,
      "grad_norm": 4.404020309448242,
      "learning_rate": 1.4693011463844798e-05,
      "loss": 0.1566,
      "step": 9630
    },
    {
      "epoch": 0.265652557319224,
      "grad_norm": 118.09564208984375,
      "learning_rate": 1.4687500000000001e-05,
      "loss": 0.1019,
      "step": 9640
    },
    {
      "epoch": 0.26592813051146386,
      "grad_norm": 0.033528029918670654,
      "learning_rate": 1.4681988536155205e-05,
      "loss": 0.2468,
      "step": 9650
    },
    {
      "epoch": 0.2662037037037037,
      "grad_norm": 0.05548016354441643,
      "learning_rate": 1.4676477072310406e-05,
      "loss": 0.1395,
      "step": 9660
    },
    {
      "epoch": 0.2664792768959436,
      "grad_norm": 0.14046961069107056,
      "learning_rate": 1.4670965608465611e-05,
      "loss": 0.2395,
      "step": 9670
    },
    {
      "epoch": 0.26675485008818345,
      "grad_norm": 1.6057088375091553,
      "learning_rate": 1.4665454144620813e-05,
      "loss": 0.1672,
      "step": 9680
    },
    {
      "epoch": 0.26703042328042326,
      "grad_norm": 0.03402438014745712,
      "learning_rate": 1.4659942680776016e-05,
      "loss": 0.2353,
      "step": 9690
    },
    {
      "epoch": 0.2673059964726631,
      "grad_norm": 1.1681976318359375,
      "learning_rate": 1.4654431216931217e-05,
      "loss": 0.1212,
      "step": 9700
    },
    {
      "epoch": 0.267581569664903,
      "grad_norm": 0.06559853255748749,
      "learning_rate": 1.464891975308642e-05,
      "loss": 0.1759,
      "step": 9710
    },
    {
      "epoch": 0.26785714285714285,
      "grad_norm": 0.03685703128576279,
      "learning_rate": 1.4643408289241622e-05,
      "loss": 0.1978,
      "step": 9720
    },
    {
      "epoch": 0.2681327160493827,
      "grad_norm": 74.35182189941406,
      "learning_rate": 1.4637896825396827e-05,
      "loss": 0.1431,
      "step": 9730
    },
    {
      "epoch": 0.2684082892416226,
      "grad_norm": 0.04589146748185158,
      "learning_rate": 1.4632385361552029e-05,
      "loss": 0.089,
      "step": 9740
    },
    {
      "epoch": 0.26868386243386244,
      "grad_norm": 0.03049636073410511,
      "learning_rate": 1.4626873897707232e-05,
      "loss": 0.2608,
      "step": 9750
    },
    {
      "epoch": 0.2689594356261023,
      "grad_norm": 0.8682631254196167,
      "learning_rate": 1.4621362433862435e-05,
      "loss": 0.0492,
      "step": 9760
    },
    {
      "epoch": 0.26923500881834217,
      "grad_norm": 6.539223670959473,
      "learning_rate": 1.4615850970017637e-05,
      "loss": 0.2081,
      "step": 9770
    },
    {
      "epoch": 0.26951058201058203,
      "grad_norm": 7.909769058227539,
      "learning_rate": 1.4610339506172842e-05,
      "loss": 0.2876,
      "step": 9780
    },
    {
      "epoch": 0.26978615520282184,
      "grad_norm": 0.2497694045305252,
      "learning_rate": 1.4604828042328043e-05,
      "loss": 0.0612,
      "step": 9790
    },
    {
      "epoch": 0.2700617283950617,
      "grad_norm": 0.03199814632534981,
      "learning_rate": 1.4599316578483247e-05,
      "loss": 0.0846,
      "step": 9800
    },
    {
      "epoch": 0.27033730158730157,
      "grad_norm": 0.03861991688609123,
      "learning_rate": 1.4593805114638448e-05,
      "loss": 0.047,
      "step": 9810
    },
    {
      "epoch": 0.27061287477954143,
      "grad_norm": 0.040752410888671875,
      "learning_rate": 1.4588293650793653e-05,
      "loss": 0.1432,
      "step": 9820
    },
    {
      "epoch": 0.2708884479717813,
      "grad_norm": 0.03827482834458351,
      "learning_rate": 1.4582782186948855e-05,
      "loss": 0.1237,
      "step": 9830
    },
    {
      "epoch": 0.27116402116402116,
      "grad_norm": 0.05802254006266594,
      "learning_rate": 1.4577270723104058e-05,
      "loss": 0.0165,
      "step": 9840
    },
    {
      "epoch": 0.271439594356261,
      "grad_norm": 0.08432766050100327,
      "learning_rate": 1.457175925925926e-05,
      "loss": 0.1835,
      "step": 9850
    },
    {
      "epoch": 0.2717151675485009,
      "grad_norm": 0.08483054488897324,
      "learning_rate": 1.4566247795414463e-05,
      "loss": 0.0064,
      "step": 9860
    },
    {
      "epoch": 0.27199074074074076,
      "grad_norm": 0.06202234327793121,
      "learning_rate": 1.4560736331569668e-05,
      "loss": 0.0728,
      "step": 9870
    },
    {
      "epoch": 0.2722663139329806,
      "grad_norm": 0.04454009607434273,
      "learning_rate": 1.455522486772487e-05,
      "loss": 0.1294,
      "step": 9880
    },
    {
      "epoch": 0.2725418871252205,
      "grad_norm": 13.997681617736816,
      "learning_rate": 1.4549713403880073e-05,
      "loss": 0.2123,
      "step": 9890
    },
    {
      "epoch": 0.2728174603174603,
      "grad_norm": 4.568706512451172,
      "learning_rate": 1.4544201940035274e-05,
      "loss": 0.1061,
      "step": 9900
    },
    {
      "epoch": 0.27309303350970016,
      "grad_norm": 0.06066864728927612,
      "learning_rate": 1.4538690476190477e-05,
      "loss": 0.0672,
      "step": 9910
    },
    {
      "epoch": 0.27336860670194,
      "grad_norm": 93.04930114746094,
      "learning_rate": 1.4533179012345679e-05,
      "loss": 0.2067,
      "step": 9920
    },
    {
      "epoch": 0.2736441798941799,
      "grad_norm": 0.03697557747364044,
      "learning_rate": 1.4527667548500884e-05,
      "loss": 0.0803,
      "step": 9930
    },
    {
      "epoch": 0.27391975308641975,
      "grad_norm": 0.02922617457807064,
      "learning_rate": 1.4522156084656085e-05,
      "loss": 0.1296,
      "step": 9940
    },
    {
      "epoch": 0.2741953262786596,
      "grad_norm": 3.2624013423919678,
      "learning_rate": 1.4516644620811289e-05,
      "loss": 0.1227,
      "step": 9950
    },
    {
      "epoch": 0.2744708994708995,
      "grad_norm": 0.04633145406842232,
      "learning_rate": 1.451113315696649e-05,
      "loss": 0.1635,
      "step": 9960
    },
    {
      "epoch": 0.27474647266313934,
      "grad_norm": 0.11115480214357376,
      "learning_rate": 1.4505621693121694e-05,
      "loss": 0.1069,
      "step": 9970
    },
    {
      "epoch": 0.2750220458553792,
      "grad_norm": 34.67721176147461,
      "learning_rate": 1.4500110229276898e-05,
      "loss": 0.3128,
      "step": 9980
    },
    {
      "epoch": 0.27529761904761907,
      "grad_norm": 1.154077172279358,
      "learning_rate": 1.44945987654321e-05,
      "loss": 0.1306,
      "step": 9990
    },
    {
      "epoch": 0.27557319223985893,
      "grad_norm": 2.607126235961914,
      "learning_rate": 1.4489087301587303e-05,
      "loss": 0.1842,
      "step": 10000
    },
    {
      "epoch": 0.27584876543209874,
      "grad_norm": 0.03735971078276634,
      "learning_rate": 1.4483575837742505e-05,
      "loss": 0.1103,
      "step": 10010
    },
    {
      "epoch": 0.2761243386243386,
      "grad_norm": 149.9569549560547,
      "learning_rate": 1.4478064373897708e-05,
      "loss": 0.036,
      "step": 10020
    },
    {
      "epoch": 0.27639991181657847,
      "grad_norm": 0.44306039810180664,
      "learning_rate": 1.447255291005291e-05,
      "loss": 0.1503,
      "step": 10030
    },
    {
      "epoch": 0.27667548500881833,
      "grad_norm": 0.09422712028026581,
      "learning_rate": 1.4467041446208115e-05,
      "loss": 0.0976,
      "step": 10040
    },
    {
      "epoch": 0.2769510582010582,
      "grad_norm": 0.035327017307281494,
      "learning_rate": 1.4461529982363316e-05,
      "loss": 0.0106,
      "step": 10050
    },
    {
      "epoch": 0.27722663139329806,
      "grad_norm": 2.8085153102874756,
      "learning_rate": 1.445601851851852e-05,
      "loss": 0.1613,
      "step": 10060
    },
    {
      "epoch": 0.2775022045855379,
      "grad_norm": 0.030729522928595543,
      "learning_rate": 1.4450507054673724e-05,
      "loss": 0.0815,
      "step": 10070
    },
    {
      "epoch": 0.2777777777777778,
      "grad_norm": 0.05200198292732239,
      "learning_rate": 1.4444995590828926e-05,
      "loss": 0.0517,
      "step": 10080
    },
    {
      "epoch": 0.27805335097001765,
      "grad_norm": 10.290596961975098,
      "learning_rate": 1.443948412698413e-05,
      "loss": 0.1219,
      "step": 10090
    },
    {
      "epoch": 0.2783289241622575,
      "grad_norm": 0.06762460619211197,
      "learning_rate": 1.443397266313933e-05,
      "loss": 0.1247,
      "step": 10100
    },
    {
      "epoch": 0.2786044973544973,
      "grad_norm": 0.4791697859764099,
      "learning_rate": 1.4428461199294534e-05,
      "loss": 0.1856,
      "step": 10110
    },
    {
      "epoch": 0.2788800705467372,
      "grad_norm": 0.058020491153001785,
      "learning_rate": 1.4422949735449736e-05,
      "loss": 0.1452,
      "step": 10120
    },
    {
      "epoch": 0.27915564373897706,
      "grad_norm": 45.80044937133789,
      "learning_rate": 1.441743827160494e-05,
      "loss": 0.179,
      "step": 10130
    },
    {
      "epoch": 0.2794312169312169,
      "grad_norm": 45.696136474609375,
      "learning_rate": 1.4411926807760142e-05,
      "loss": 0.3107,
      "step": 10140
    },
    {
      "epoch": 0.2797067901234568,
      "grad_norm": 25.927682876586914,
      "learning_rate": 1.4406415343915345e-05,
      "loss": 0.1905,
      "step": 10150
    },
    {
      "epoch": 0.27998236331569665,
      "grad_norm": 1.0415500402450562,
      "learning_rate": 1.4400903880070547e-05,
      "loss": 0.1184,
      "step": 10160
    },
    {
      "epoch": 0.2802579365079365,
      "grad_norm": 0.05309494957327843,
      "learning_rate": 1.439539241622575e-05,
      "loss": 0.1187,
      "step": 10170
    },
    {
      "epoch": 0.2805335097001764,
      "grad_norm": 1.1671264171600342,
      "learning_rate": 1.4389880952380955e-05,
      "loss": 0.2734,
      "step": 10180
    },
    {
      "epoch": 0.28080908289241624,
      "grad_norm": 0.1613786667585373,
      "learning_rate": 1.4384369488536157e-05,
      "loss": 0.0123,
      "step": 10190
    },
    {
      "epoch": 0.2810846560846561,
      "grad_norm": 22.993404388427734,
      "learning_rate": 1.437885802469136e-05,
      "loss": 0.1517,
      "step": 10200
    },
    {
      "epoch": 0.28136022927689597,
      "grad_norm": 6.211454391479492,
      "learning_rate": 1.4373346560846562e-05,
      "loss": 0.2213,
      "step": 10210
    },
    {
      "epoch": 0.2816358024691358,
      "grad_norm": 0.04675121232867241,
      "learning_rate": 1.4367835097001765e-05,
      "loss": 0.1209,
      "step": 10220
    },
    {
      "epoch": 0.28191137566137564,
      "grad_norm": 0.4754639267921448,
      "learning_rate": 1.4362323633156966e-05,
      "loss": 0.0374,
      "step": 10230
    },
    {
      "epoch": 0.2821869488536155,
      "grad_norm": 0.6939693689346313,
      "learning_rate": 1.4356812169312171e-05,
      "loss": 0.086,
      "step": 10240
    },
    {
      "epoch": 0.28246252204585537,
      "grad_norm": 12.457474708557129,
      "learning_rate": 1.4351300705467373e-05,
      "loss": 0.0045,
      "step": 10250
    },
    {
      "epoch": 0.28273809523809523,
      "grad_norm": 0.0763845220208168,
      "learning_rate": 1.4345789241622576e-05,
      "loss": 0.0041,
      "step": 10260
    },
    {
      "epoch": 0.2830136684303351,
      "grad_norm": 150.5141143798828,
      "learning_rate": 1.4340277777777778e-05,
      "loss": 0.2286,
      "step": 10270
    },
    {
      "epoch": 0.28328924162257496,
      "grad_norm": 0.02941168285906315,
      "learning_rate": 1.4334766313932981e-05,
      "loss": 0.2028,
      "step": 10280
    },
    {
      "epoch": 0.2835648148148148,
      "grad_norm": 0.44803327322006226,
      "learning_rate": 1.4329254850088186e-05,
      "loss": 0.1906,
      "step": 10290
    },
    {
      "epoch": 0.2838403880070547,
      "grad_norm": 0.027493063360452652,
      "learning_rate": 1.4323743386243388e-05,
      "loss": 0.1848,
      "step": 10300
    },
    {
      "epoch": 0.28411596119929455,
      "grad_norm": 27.855253219604492,
      "learning_rate": 1.431823192239859e-05,
      "loss": 0.0057,
      "step": 10310
    },
    {
      "epoch": 0.2843915343915344,
      "grad_norm": 0.13152626156806946,
      "learning_rate": 1.4312720458553792e-05,
      "loss": 0.1218,
      "step": 10320
    },
    {
      "epoch": 0.2846671075837742,
      "grad_norm": 0.03502557426691055,
      "learning_rate": 1.4307208994708997e-05,
      "loss": 0.0752,
      "step": 10330
    },
    {
      "epoch": 0.2849426807760141,
      "grad_norm": 2.3725380897521973,
      "learning_rate": 1.4301697530864199e-05,
      "loss": 0.0223,
      "step": 10340
    },
    {
      "epoch": 0.28521825396825395,
      "grad_norm": 0.2280697226524353,
      "learning_rate": 1.4296186067019402e-05,
      "loss": 0.004,
      "step": 10350
    },
    {
      "epoch": 0.2854938271604938,
      "grad_norm": 0.040505725890398026,
      "learning_rate": 1.4290674603174604e-05,
      "loss": 0.1839,
      "step": 10360
    },
    {
      "epoch": 0.2857694003527337,
      "grad_norm": 23.36438751220703,
      "learning_rate": 1.4285163139329807e-05,
      "loss": 0.1439,
      "step": 10370
    },
    {
      "epoch": 0.28604497354497355,
      "grad_norm": 156.8368682861328,
      "learning_rate": 1.4279651675485009e-05,
      "loss": 0.223,
      "step": 10380
    },
    {
      "epoch": 0.2863205467372134,
      "grad_norm": 0.08613623678684235,
      "learning_rate": 1.4274140211640213e-05,
      "loss": 0.0712,
      "step": 10390
    },
    {
      "epoch": 0.2865961199294533,
      "grad_norm": 1.244386076927185,
      "learning_rate": 1.4268628747795417e-05,
      "loss": 0.288,
      "step": 10400
    },
    {
      "epoch": 0.28687169312169314,
      "grad_norm": 60.082698822021484,
      "learning_rate": 1.4263117283950618e-05,
      "loss": 0.1921,
      "step": 10410
    },
    {
      "epoch": 0.287147266313933,
      "grad_norm": 0.04549175873398781,
      "learning_rate": 1.4257605820105822e-05,
      "loss": 0.0986,
      "step": 10420
    },
    {
      "epoch": 0.28742283950617287,
      "grad_norm": 109.21761322021484,
      "learning_rate": 1.4252094356261023e-05,
      "loss": 0.0254,
      "step": 10430
    },
    {
      "epoch": 0.2876984126984127,
      "grad_norm": 7.060558795928955,
      "learning_rate": 1.4246582892416228e-05,
      "loss": 0.1063,
      "step": 10440
    },
    {
      "epoch": 0.28797398589065254,
      "grad_norm": 14.156786918640137,
      "learning_rate": 1.424107142857143e-05,
      "loss": 0.2043,
      "step": 10450
    },
    {
      "epoch": 0.2882495590828924,
      "grad_norm": 0.028273042291402817,
      "learning_rate": 1.4235559964726633e-05,
      "loss": 0.0704,
      "step": 10460
    },
    {
      "epoch": 0.28852513227513227,
      "grad_norm": 0.19937865436077118,
      "learning_rate": 1.4230048500881834e-05,
      "loss": 0.1233,
      "step": 10470
    },
    {
      "epoch": 0.28880070546737213,
      "grad_norm": 0.033296193927526474,
      "learning_rate": 1.4224537037037038e-05,
      "loss": 0.1099,
      "step": 10480
    },
    {
      "epoch": 0.289076278659612,
      "grad_norm": 82.79309844970703,
      "learning_rate": 1.421902557319224e-05,
      "loss": 0.1675,
      "step": 10490
    },
    {
      "epoch": 0.28935185185185186,
      "grad_norm": 45.413238525390625,
      "learning_rate": 1.4213514109347444e-05,
      "loss": 0.0069,
      "step": 10500
    },
    {
      "epoch": 0.2896274250440917,
      "grad_norm": 0.02552603930234909,
      "learning_rate": 1.4208002645502647e-05,
      "loss": 0.2081,
      "step": 10510
    },
    {
      "epoch": 0.2899029982363316,
      "grad_norm": 76.23731994628906,
      "learning_rate": 1.4202491181657849e-05,
      "loss": 0.1903,
      "step": 10520
    },
    {
      "epoch": 0.29017857142857145,
      "grad_norm": 1.5380626916885376,
      "learning_rate": 1.4196979717813052e-05,
      "loss": 0.026,
      "step": 10530
    },
    {
      "epoch": 0.29045414462081126,
      "grad_norm": 0.4214450418949127,
      "learning_rate": 1.4191468253968254e-05,
      "loss": 0.044,
      "step": 10540
    },
    {
      "epoch": 0.2907297178130511,
      "grad_norm": 18.14927101135254,
      "learning_rate": 1.4185956790123459e-05,
      "loss": 0.1598,
      "step": 10550
    },
    {
      "epoch": 0.291005291005291,
      "grad_norm": 153.91793823242188,
      "learning_rate": 1.418044532627866e-05,
      "loss": 0.0399,
      "step": 10560
    },
    {
      "epoch": 0.29128086419753085,
      "grad_norm": 94.46695709228516,
      "learning_rate": 1.4174933862433864e-05,
      "loss": 0.0436,
      "step": 10570
    },
    {
      "epoch": 0.2915564373897707,
      "grad_norm": 0.5754363536834717,
      "learning_rate": 1.4169422398589065e-05,
      "loss": 0.031,
      "step": 10580
    },
    {
      "epoch": 0.2918320105820106,
      "grad_norm": 0.023753579705953598,
      "learning_rate": 1.416391093474427e-05,
      "loss": 0.0402,
      "step": 10590
    },
    {
      "epoch": 0.29210758377425045,
      "grad_norm": 0.030583038926124573,
      "learning_rate": 1.4158399470899472e-05,
      "loss": 0.1676,
      "step": 10600
    },
    {
      "epoch": 0.2923831569664903,
      "grad_norm": 0.041010163724422455,
      "learning_rate": 1.4152888007054675e-05,
      "loss": 0.0849,
      "step": 10610
    },
    {
      "epoch": 0.2926587301587302,
      "grad_norm": 0.046867165714502335,
      "learning_rate": 1.4147376543209878e-05,
      "loss": 0.0286,
      "step": 10620
    },
    {
      "epoch": 0.29293430335097004,
      "grad_norm": 0.04755089432001114,
      "learning_rate": 1.414186507936508e-05,
      "loss": 0.2397,
      "step": 10630
    },
    {
      "epoch": 0.2932098765432099,
      "grad_norm": 32.16276931762695,
      "learning_rate": 1.4136353615520285e-05,
      "loss": 0.2608,
      "step": 10640
    },
    {
      "epoch": 0.2934854497354497,
      "grad_norm": 0.04122549295425415,
      "learning_rate": 1.4130842151675486e-05,
      "loss": 0.0033,
      "step": 10650
    },
    {
      "epoch": 0.2937610229276896,
      "grad_norm": 0.33214259147644043,
      "learning_rate": 1.412533068783069e-05,
      "loss": 0.2164,
      "step": 10660
    },
    {
      "epoch": 0.29403659611992944,
      "grad_norm": 0.033128656446933746,
      "learning_rate": 1.4119819223985891e-05,
      "loss": 0.078,
      "step": 10670
    },
    {
      "epoch": 0.2943121693121693,
      "grad_norm": 15.995179176330566,
      "learning_rate": 1.4114307760141094e-05,
      "loss": 0.1007,
      "step": 10680
    },
    {
      "epoch": 0.29458774250440917,
      "grad_norm": 0.040683310478925705,
      "learning_rate": 1.4108796296296296e-05,
      "loss": 0.3102,
      "step": 10690
    },
    {
      "epoch": 0.29486331569664903,
      "grad_norm": 0.03658795356750488,
      "learning_rate": 1.4103284832451501e-05,
      "loss": 0.002,
      "step": 10700
    },
    {
      "epoch": 0.2951388888888889,
      "grad_norm": 0.023893460631370544,
      "learning_rate": 1.4097773368606704e-05,
      "loss": 0.0831,
      "step": 10710
    },
    {
      "epoch": 0.29541446208112876,
      "grad_norm": 83.74134063720703,
      "learning_rate": 1.4092261904761906e-05,
      "loss": 0.0685,
      "step": 10720
    },
    {
      "epoch": 0.2956900352733686,
      "grad_norm": 0.028905801475048065,
      "learning_rate": 1.4086750440917109e-05,
      "loss": 0.1341,
      "step": 10730
    },
    {
      "epoch": 0.2959656084656085,
      "grad_norm": 65.1828842163086,
      "learning_rate": 1.408123897707231e-05,
      "loss": 0.2549,
      "step": 10740
    },
    {
      "epoch": 0.29624118165784835,
      "grad_norm": 6.445282936096191,
      "learning_rate": 1.4075727513227516e-05,
      "loss": 0.2048,
      "step": 10750
    },
    {
      "epoch": 0.29651675485008816,
      "grad_norm": 0.04821867123246193,
      "learning_rate": 1.4070216049382717e-05,
      "loss": 0.002,
      "step": 10760
    },
    {
      "epoch": 0.296792328042328,
      "grad_norm": 0.38011834025382996,
      "learning_rate": 1.406470458553792e-05,
      "loss": 0.0021,
      "step": 10770
    },
    {
      "epoch": 0.2970679012345679,
      "grad_norm": 0.451715886592865,
      "learning_rate": 1.4059193121693122e-05,
      "loss": 0.1486,
      "step": 10780
    },
    {
      "epoch": 0.29734347442680775,
      "grad_norm": 0.1613117754459381,
      "learning_rate": 1.4053681657848325e-05,
      "loss": 0.0386,
      "step": 10790
    },
    {
      "epoch": 0.2976190476190476,
      "grad_norm": 25.35698890686035,
      "learning_rate": 1.4048170194003527e-05,
      "loss": 0.2129,
      "step": 10800
    },
    {
      "epoch": 0.2978946208112875,
      "grad_norm": 0.7091489434242249,
      "learning_rate": 1.4042658730158732e-05,
      "loss": 0.1806,
      "step": 10810
    },
    {
      "epoch": 0.29817019400352734,
      "grad_norm": 106.55783081054688,
      "learning_rate": 1.4037147266313935e-05,
      "loss": 0.1281,
      "step": 10820
    },
    {
      "epoch": 0.2984457671957672,
      "grad_norm": 0.03725207969546318,
      "learning_rate": 1.4031635802469137e-05,
      "loss": 0.0046,
      "step": 10830
    },
    {
      "epoch": 0.2987213403880071,
      "grad_norm": 0.028966231271624565,
      "learning_rate": 1.4026124338624341e-05,
      "loss": 0.1106,
      "step": 10840
    },
    {
      "epoch": 0.29899691358024694,
      "grad_norm": 82.02652740478516,
      "learning_rate": 1.4020612874779543e-05,
      "loss": 0.1508,
      "step": 10850
    },
    {
      "epoch": 0.29927248677248675,
      "grad_norm": 121.96764373779297,
      "learning_rate": 1.4015101410934746e-05,
      "loss": 0.1737,
      "step": 10860
    },
    {
      "epoch": 0.2995480599647266,
      "grad_norm": 11.888428688049316,
      "learning_rate": 1.4009589947089948e-05,
      "loss": 0.2161,
      "step": 10870
    },
    {
      "epoch": 0.2998236331569665,
      "grad_norm": 0.09437195956707001,
      "learning_rate": 1.4004078483245151e-05,
      "loss": 0.0288,
      "step": 10880
    },
    {
      "epoch": 0.30009920634920634,
      "grad_norm": 0.03410490229725838,
      "learning_rate": 1.3998567019400353e-05,
      "loss": 0.0152,
      "step": 10890
    },
    {
      "epoch": 0.3003747795414462,
      "grad_norm": 0.1194969043135643,
      "learning_rate": 1.3993055555555558e-05,
      "loss": 0.0391,
      "step": 10900
    },
    {
      "epoch": 0.30065035273368607,
      "grad_norm": 0.05139094591140747,
      "learning_rate": 1.398754409171076e-05,
      "loss": 0.0726,
      "step": 10910
    },
    {
      "epoch": 0.30092592592592593,
      "grad_norm": 0.018887866288423538,
      "learning_rate": 1.3982032627865962e-05,
      "loss": 0.1717,
      "step": 10920
    },
    {
      "epoch": 0.3012014991181658,
      "grad_norm": 47.078983306884766,
      "learning_rate": 1.3976521164021166e-05,
      "loss": 0.2156,
      "step": 10930
    },
    {
      "epoch": 0.30147707231040566,
      "grad_norm": 0.09265638142824173,
      "learning_rate": 1.3971009700176367e-05,
      "loss": 0.0898,
      "step": 10940
    },
    {
      "epoch": 0.3017526455026455,
      "grad_norm": 0.03730065003037453,
      "learning_rate": 1.3965498236331572e-05,
      "loss": 0.0033,
      "step": 10950
    },
    {
      "epoch": 0.3020282186948854,
      "grad_norm": 154.42007446289062,
      "learning_rate": 1.3959986772486774e-05,
      "loss": 0.2913,
      "step": 10960
    },
    {
      "epoch": 0.3023037918871252,
      "grad_norm": 0.021613167598843575,
      "learning_rate": 1.3954475308641977e-05,
      "loss": 0.0662,
      "step": 10970
    },
    {
      "epoch": 0.30257936507936506,
      "grad_norm": 0.15972767770290375,
      "learning_rate": 1.3948963844797179e-05,
      "loss": 0.061,
      "step": 10980
    },
    {
      "epoch": 0.3028549382716049,
      "grad_norm": 0.41852912306785583,
      "learning_rate": 1.3943452380952382e-05,
      "loss": 0.1162,
      "step": 10990
    },
    {
      "epoch": 0.3031305114638448,
      "grad_norm": 0.02318023145198822,
      "learning_rate": 1.3937940917107583e-05,
      "loss": 0.3359,
      "step": 11000
    },
    {
      "epoch": 0.30340608465608465,
      "grad_norm": 9.161425590515137,
      "learning_rate": 1.3932429453262788e-05,
      "loss": 0.1831,
      "step": 11010
    },
    {
      "epoch": 0.3036816578483245,
      "grad_norm": 17.847667694091797,
      "learning_rate": 1.392691798941799e-05,
      "loss": 0.0611,
      "step": 11020
    },
    {
      "epoch": 0.3039572310405644,
      "grad_norm": 0.021284550428390503,
      "learning_rate": 1.3921406525573193e-05,
      "loss": 0.0779,
      "step": 11030
    },
    {
      "epoch": 0.30423280423280424,
      "grad_norm": 0.5384649634361267,
      "learning_rate": 1.3915895061728396e-05,
      "loss": 0.1685,
      "step": 11040
    },
    {
      "epoch": 0.3045083774250441,
      "grad_norm": 0.0883338525891304,
      "learning_rate": 1.3910383597883598e-05,
      "loss": 0.0032,
      "step": 11050
    },
    {
      "epoch": 0.30478395061728397,
      "grad_norm": 107.33055114746094,
      "learning_rate": 1.3904872134038803e-05,
      "loss": 0.2212,
      "step": 11060
    },
    {
      "epoch": 0.30505952380952384,
      "grad_norm": 0.1259864866733551,
      "learning_rate": 1.3899360670194005e-05,
      "loss": 0.2224,
      "step": 11070
    },
    {
      "epoch": 0.30533509700176364,
      "grad_norm": 0.8066208958625793,
      "learning_rate": 1.3893849206349208e-05,
      "loss": 0.059,
      "step": 11080
    },
    {
      "epoch": 0.3056106701940035,
      "grad_norm": 0.03323270380496979,
      "learning_rate": 1.388833774250441e-05,
      "loss": 0.075,
      "step": 11090
    },
    {
      "epoch": 0.3058862433862434,
      "grad_norm": 0.06511250883340836,
      "learning_rate": 1.3882826278659614e-05,
      "loss": 0.0418,
      "step": 11100
    },
    {
      "epoch": 0.30616181657848324,
      "grad_norm": 3.9747440814971924,
      "learning_rate": 1.3877314814814816e-05,
      "loss": 0.1113,
      "step": 11110
    },
    {
      "epoch": 0.3064373897707231,
      "grad_norm": 0.09527750313282013,
      "learning_rate": 1.3871803350970019e-05,
      "loss": 0.2262,
      "step": 11120
    },
    {
      "epoch": 0.30671296296296297,
      "grad_norm": 0.03823454678058624,
      "learning_rate": 1.386629188712522e-05,
      "loss": 0.1567,
      "step": 11130
    },
    {
      "epoch": 0.30698853615520283,
      "grad_norm": 0.03751829266548157,
      "learning_rate": 1.3860780423280424e-05,
      "loss": 0.1642,
      "step": 11140
    },
    {
      "epoch": 0.3072641093474427,
      "grad_norm": 47.04704666137695,
      "learning_rate": 1.3855268959435629e-05,
      "loss": 0.1666,
      "step": 11150
    },
    {
      "epoch": 0.30753968253968256,
      "grad_norm": 113.19039154052734,
      "learning_rate": 1.384975749559083e-05,
      "loss": 0.1015,
      "step": 11160
    },
    {
      "epoch": 0.3078152557319224,
      "grad_norm": 0.12123260647058487,
      "learning_rate": 1.3844246031746034e-05,
      "loss": 0.0664,
      "step": 11170
    },
    {
      "epoch": 0.30809082892416223,
      "grad_norm": 0.140791654586792,
      "learning_rate": 1.3838734567901235e-05,
      "loss": 0.1027,
      "step": 11180
    },
    {
      "epoch": 0.3083664021164021,
      "grad_norm": 0.04411851987242699,
      "learning_rate": 1.3833223104056439e-05,
      "loss": 0.1178,
      "step": 11190
    },
    {
      "epoch": 0.30864197530864196,
      "grad_norm": 0.0322076678276062,
      "learning_rate": 1.382771164021164e-05,
      "loss": 0.0195,
      "step": 11200
    },
    {
      "epoch": 0.3089175485008818,
      "grad_norm": 0.0679151862859726,
      "learning_rate": 1.3822200176366845e-05,
      "loss": 0.0798,
      "step": 11210
    },
    {
      "epoch": 0.3091931216931217,
      "grad_norm": 0.07252183556556702,
      "learning_rate": 1.3816688712522047e-05,
      "loss": 0.015,
      "step": 11220
    },
    {
      "epoch": 0.30946869488536155,
      "grad_norm": 0.5855779647827148,
      "learning_rate": 1.381117724867725e-05,
      "loss": 0.1346,
      "step": 11230
    },
    {
      "epoch": 0.3097442680776014,
      "grad_norm": 0.08123273402452469,
      "learning_rate": 1.3805665784832451e-05,
      "loss": 0.093,
      "step": 11240
    },
    {
      "epoch": 0.3100198412698413,
      "grad_norm": 0.06609100103378296,
      "learning_rate": 1.3800154320987655e-05,
      "loss": 0.1617,
      "step": 11250
    },
    {
      "epoch": 0.31029541446208114,
      "grad_norm": 2.597989559173584,
      "learning_rate": 1.379464285714286e-05,
      "loss": 0.2115,
      "step": 11260
    },
    {
      "epoch": 0.310570987654321,
      "grad_norm": 111.22080993652344,
      "learning_rate": 1.3789131393298061e-05,
      "loss": 0.1895,
      "step": 11270
    },
    {
      "epoch": 0.31084656084656087,
      "grad_norm": 0.030142998322844505,
      "learning_rate": 1.3783619929453265e-05,
      "loss": 0.1059,
      "step": 11280
    },
    {
      "epoch": 0.3111221340388007,
      "grad_norm": 0.03180786967277527,
      "learning_rate": 1.3778108465608466e-05,
      "loss": 0.1665,
      "step": 11290
    },
    {
      "epoch": 0.31139770723104054,
      "grad_norm": 33.16112518310547,
      "learning_rate": 1.377259700176367e-05,
      "loss": 0.0077,
      "step": 11300
    },
    {
      "epoch": 0.3116732804232804,
      "grad_norm": 64.4091567993164,
      "learning_rate": 1.3767085537918871e-05,
      "loss": 0.0408,
      "step": 11310
    },
    {
      "epoch": 0.31194885361552027,
      "grad_norm": 0.026682265102863312,
      "learning_rate": 1.3761574074074076e-05,
      "loss": 0.1482,
      "step": 11320
    },
    {
      "epoch": 0.31222442680776014,
      "grad_norm": 61.192630767822266,
      "learning_rate": 1.3756062610229277e-05,
      "loss": 0.2736,
      "step": 11330
    },
    {
      "epoch": 0.3125,
      "grad_norm": 51.19356918334961,
      "learning_rate": 1.375055114638448e-05,
      "loss": 0.094,
      "step": 11340
    },
    {
      "epoch": 0.31277557319223986,
      "grad_norm": 0.05404045060276985,
      "learning_rate": 1.3745039682539682e-05,
      "loss": 0.0467,
      "step": 11350
    },
    {
      "epoch": 0.31305114638447973,
      "grad_norm": 0.05090950056910515,
      "learning_rate": 1.3739528218694887e-05,
      "loss": 0.15,
      "step": 11360
    },
    {
      "epoch": 0.3133267195767196,
      "grad_norm": 0.49389028549194336,
      "learning_rate": 1.373401675485009e-05,
      "loss": 0.0954,
      "step": 11370
    },
    {
      "epoch": 0.31360229276895946,
      "grad_norm": 0.03303787484765053,
      "learning_rate": 1.3728505291005292e-05,
      "loss": 0.0489,
      "step": 11380
    },
    {
      "epoch": 0.3138778659611993,
      "grad_norm": 0.05126658082008362,
      "learning_rate": 1.3722993827160495e-05,
      "loss": 0.1533,
      "step": 11390
    },
    {
      "epoch": 0.31415343915343913,
      "grad_norm": 19.44227409362793,
      "learning_rate": 1.3717482363315697e-05,
      "loss": 0.004,
      "step": 11400
    },
    {
      "epoch": 0.314429012345679,
      "grad_norm": 0.02433948777616024,
      "learning_rate": 1.3711970899470902e-05,
      "loss": 0.0017,
      "step": 11410
    },
    {
      "epoch": 0.31470458553791886,
      "grad_norm": 22.987152099609375,
      "learning_rate": 1.3706459435626103e-05,
      "loss": 0.1821,
      "step": 11420
    },
    {
      "epoch": 0.3149801587301587,
      "grad_norm": 94.78428649902344,
      "learning_rate": 1.3700947971781307e-05,
      "loss": 0.0764,
      "step": 11430
    },
    {
      "epoch": 0.3152557319223986,
      "grad_norm": 126.8586654663086,
      "learning_rate": 1.3695436507936508e-05,
      "loss": 0.1159,
      "step": 11440
    },
    {
      "epoch": 0.31553130511463845,
      "grad_norm": 0.06909869611263275,
      "learning_rate": 1.3689925044091711e-05,
      "loss": 0.0805,
      "step": 11450
    },
    {
      "epoch": 0.3158068783068783,
      "grad_norm": 104.58987426757812,
      "learning_rate": 1.3684413580246916e-05,
      "loss": 0.0615,
      "step": 11460
    },
    {
      "epoch": 0.3160824514991182,
      "grad_norm": 0.02514040470123291,
      "learning_rate": 1.3678902116402118e-05,
      "loss": 0.1485,
      "step": 11470
    },
    {
      "epoch": 0.31635802469135804,
      "grad_norm": 1.733433485031128,
      "learning_rate": 1.3673390652557321e-05,
      "loss": 0.1742,
      "step": 11480
    },
    {
      "epoch": 0.3166335978835979,
      "grad_norm": 0.027487657964229584,
      "learning_rate": 1.3667879188712523e-05,
      "loss": 0.2773,
      "step": 11490
    },
    {
      "epoch": 0.31690917107583777,
      "grad_norm": 3.47442626953125,
      "learning_rate": 1.3662367724867726e-05,
      "loss": 0.0972,
      "step": 11500
    },
    {
      "epoch": 0.3171847442680776,
      "grad_norm": 85.9154052734375,
      "learning_rate": 1.3656856261022928e-05,
      "loss": 0.1404,
      "step": 11510
    },
    {
      "epoch": 0.31746031746031744,
      "grad_norm": 0.19474735856056213,
      "learning_rate": 1.3651344797178133e-05,
      "loss": 0.0023,
      "step": 11520
    },
    {
      "epoch": 0.3177358906525573,
      "grad_norm": 10.359160423278809,
      "learning_rate": 1.3645833333333334e-05,
      "loss": 0.2569,
      "step": 11530
    },
    {
      "epoch": 0.31801146384479717,
      "grad_norm": 5.000150680541992,
      "learning_rate": 1.3640321869488537e-05,
      "loss": 0.0246,
      "step": 11540
    },
    {
      "epoch": 0.31828703703703703,
      "grad_norm": 89.77106475830078,
      "learning_rate": 1.3634810405643739e-05,
      "loss": 0.2152,
      "step": 11550
    },
    {
      "epoch": 0.3185626102292769,
      "grad_norm": 6.450171947479248,
      "learning_rate": 1.3629298941798942e-05,
      "loss": 0.2103,
      "step": 11560
    },
    {
      "epoch": 0.31883818342151676,
      "grad_norm": 0.04112831875681877,
      "learning_rate": 1.3623787477954147e-05,
      "loss": 0.2543,
      "step": 11570
    },
    {
      "epoch": 0.3191137566137566,
      "grad_norm": 0.047198422253131866,
      "learning_rate": 1.3618276014109349e-05,
      "loss": 0.0866,
      "step": 11580
    },
    {
      "epoch": 0.3193893298059965,
      "grad_norm": 0.02109440416097641,
      "learning_rate": 1.3612764550264552e-05,
      "loss": 0.0147,
      "step": 11590
    },
    {
      "epoch": 0.31966490299823636,
      "grad_norm": 0.03114437684416771,
      "learning_rate": 1.3607253086419754e-05,
      "loss": 0.0707,
      "step": 11600
    },
    {
      "epoch": 0.31994047619047616,
      "grad_norm": 0.026323027908802032,
      "learning_rate": 1.3601741622574958e-05,
      "loss": 0.2154,
      "step": 11610
    },
    {
      "epoch": 0.32021604938271603,
      "grad_norm": 0.6713343858718872,
      "learning_rate": 1.359623015873016e-05,
      "loss": 0.1917,
      "step": 11620
    },
    {
      "epoch": 0.3204916225749559,
      "grad_norm": 0.02715832181274891,
      "learning_rate": 1.3590718694885363e-05,
      "loss": 0.0836,
      "step": 11630
    },
    {
      "epoch": 0.32076719576719576,
      "grad_norm": 0.9208006262779236,
      "learning_rate": 1.3585207231040565e-05,
      "loss": 0.0863,
      "step": 11640
    },
    {
      "epoch": 0.3210427689594356,
      "grad_norm": 216.12306213378906,
      "learning_rate": 1.3579695767195768e-05,
      "loss": 0.1337,
      "step": 11650
    },
    {
      "epoch": 0.3213183421516755,
      "grad_norm": 150.46214294433594,
      "learning_rate": 1.357418430335097e-05,
      "loss": 0.099,
      "step": 11660
    },
    {
      "epoch": 0.32159391534391535,
      "grad_norm": 0.0371144562959671,
      "learning_rate": 1.3568672839506175e-05,
      "loss": 0.1013,
      "step": 11670
    },
    {
      "epoch": 0.3218694885361552,
      "grad_norm": 0.023633500561118126,
      "learning_rate": 1.3563161375661378e-05,
      "loss": 0.0534,
      "step": 11680
    },
    {
      "epoch": 0.3221450617283951,
      "grad_norm": 0.06386295706033707,
      "learning_rate": 1.355764991181658e-05,
      "loss": 0.065,
      "step": 11690
    },
    {
      "epoch": 0.32242063492063494,
      "grad_norm": 6.139651298522949,
      "learning_rate": 1.3552138447971783e-05,
      "loss": 0.0159,
      "step": 11700
    },
    {
      "epoch": 0.3226962081128748,
      "grad_norm": 0.02433861792087555,
      "learning_rate": 1.3546626984126984e-05,
      "loss": 0.0791,
      "step": 11710
    },
    {
      "epoch": 0.3229717813051146,
      "grad_norm": 0.19763454794883728,
      "learning_rate": 1.354111552028219e-05,
      "loss": 0.0881,
      "step": 11720
    },
    {
      "epoch": 0.3232473544973545,
      "grad_norm": 18.039478302001953,
      "learning_rate": 1.353560405643739e-05,
      "loss": 0.2621,
      "step": 11730
    },
    {
      "epoch": 0.32352292768959434,
      "grad_norm": 0.0474243201315403,
      "learning_rate": 1.3530092592592594e-05,
      "loss": 0.0985,
      "step": 11740
    },
    {
      "epoch": 0.3237985008818342,
      "grad_norm": 0.8378705382347107,
      "learning_rate": 1.3524581128747796e-05,
      "loss": 0.0885,
      "step": 11750
    },
    {
      "epoch": 0.32407407407407407,
      "grad_norm": 0.02799200266599655,
      "learning_rate": 1.3519069664902999e-05,
      "loss": 0.0017,
      "step": 11760
    },
    {
      "epoch": 0.32434964726631393,
      "grad_norm": 0.06642569601535797,
      "learning_rate": 1.35135582010582e-05,
      "loss": 0.1172,
      "step": 11770
    },
    {
      "epoch": 0.3246252204585538,
      "grad_norm": 0.04076765850186348,
      "learning_rate": 1.3508046737213405e-05,
      "loss": 0.1361,
      "step": 11780
    },
    {
      "epoch": 0.32490079365079366,
      "grad_norm": 0.07789250463247299,
      "learning_rate": 1.3502535273368609e-05,
      "loss": 0.0653,
      "step": 11790
    },
    {
      "epoch": 0.3251763668430335,
      "grad_norm": 133.88209533691406,
      "learning_rate": 1.349702380952381e-05,
      "loss": 0.1057,
      "step": 11800
    },
    {
      "epoch": 0.3254519400352734,
      "grad_norm": 0.2617793083190918,
      "learning_rate": 1.3491512345679013e-05,
      "loss": 0.1658,
      "step": 11810
    },
    {
      "epoch": 0.32572751322751325,
      "grad_norm": 106.7090072631836,
      "learning_rate": 1.3486000881834215e-05,
      "loss": 0.1412,
      "step": 11820
    },
    {
      "epoch": 0.32600308641975306,
      "grad_norm": 0.030355922877788544,
      "learning_rate": 1.348048941798942e-05,
      "loss": 0.2953,
      "step": 11830
    },
    {
      "epoch": 0.3262786596119929,
      "grad_norm": 0.03814661130309105,
      "learning_rate": 1.3474977954144622e-05,
      "loss": 0.1332,
      "step": 11840
    },
    {
      "epoch": 0.3265542328042328,
      "grad_norm": 4.283719062805176,
      "learning_rate": 1.3469466490299825e-05,
      "loss": 0.1275,
      "step": 11850
    },
    {
      "epoch": 0.32682980599647266,
      "grad_norm": 84.18909454345703,
      "learning_rate": 1.3463955026455026e-05,
      "loss": 0.2111,
      "step": 11860
    },
    {
      "epoch": 0.3271053791887125,
      "grad_norm": 0.34667742252349854,
      "learning_rate": 1.3458443562610231e-05,
      "loss": 0.3792,
      "step": 11870
    },
    {
      "epoch": 0.3273809523809524,
      "grad_norm": 1.9447591304779053,
      "learning_rate": 1.3452932098765433e-05,
      "loss": 0.0108,
      "step": 11880
    },
    {
      "epoch": 0.32765652557319225,
      "grad_norm": 0.4589683413505554,
      "learning_rate": 1.3447420634920636e-05,
      "loss": 0.0823,
      "step": 11890
    },
    {
      "epoch": 0.3279320987654321,
      "grad_norm": 0.03686415031552315,
      "learning_rate": 1.344190917107584e-05,
      "loss": 0.124,
      "step": 11900
    },
    {
      "epoch": 0.328207671957672,
      "grad_norm": 0.025625284761190414,
      "learning_rate": 1.3436397707231041e-05,
      "loss": 0.0239,
      "step": 11910
    },
    {
      "epoch": 0.32848324514991184,
      "grad_norm": 0.035922616720199585,
      "learning_rate": 1.3430886243386246e-05,
      "loss": 0.2976,
      "step": 11920
    },
    {
      "epoch": 0.32875881834215165,
      "grad_norm": 0.02925752103328705,
      "learning_rate": 1.3425374779541448e-05,
      "loss": 0.0157,
      "step": 11930
    },
    {
      "epoch": 0.3290343915343915,
      "grad_norm": 0.1754300445318222,
      "learning_rate": 1.341986331569665e-05,
      "loss": 0.0515,
      "step": 11940
    },
    {
      "epoch": 0.3293099647266314,
      "grad_norm": 0.5957933664321899,
      "learning_rate": 1.3414351851851852e-05,
      "loss": 0.2062,
      "step": 11950
    },
    {
      "epoch": 0.32958553791887124,
      "grad_norm": 42.548583984375,
      "learning_rate": 1.3408840388007056e-05,
      "loss": 0.1813,
      "step": 11960
    },
    {
      "epoch": 0.3298611111111111,
      "grad_norm": 106.88874816894531,
      "learning_rate": 1.3403328924162257e-05,
      "loss": 0.2252,
      "step": 11970
    },
    {
      "epoch": 0.33013668430335097,
      "grad_norm": 0.0638682171702385,
      "learning_rate": 1.3397817460317462e-05,
      "loss": 0.1103,
      "step": 11980
    },
    {
      "epoch": 0.33041225749559083,
      "grad_norm": 0.05148265138268471,
      "learning_rate": 1.3392305996472664e-05,
      "loss": 0.1041,
      "step": 11990
    },
    {
      "epoch": 0.3306878306878307,
      "grad_norm": 5.847717761993408,
      "learning_rate": 1.3386794532627867e-05,
      "loss": 0.1605,
      "step": 12000
    },
    {
      "epoch": 0.33096340388007056,
      "grad_norm": 0.8219833374023438,
      "learning_rate": 1.338128306878307e-05,
      "loss": 0.0186,
      "step": 12010
    },
    {
      "epoch": 0.3312389770723104,
      "grad_norm": 44.90829086303711,
      "learning_rate": 1.3375771604938272e-05,
      "loss": 0.1429,
      "step": 12020
    },
    {
      "epoch": 0.3315145502645503,
      "grad_norm": 0.26940879225730896,
      "learning_rate": 1.3370260141093477e-05,
      "loss": 0.0033,
      "step": 12030
    },
    {
      "epoch": 0.3317901234567901,
      "grad_norm": 57.105918884277344,
      "learning_rate": 1.3364748677248678e-05,
      "loss": 0.2162,
      "step": 12040
    },
    {
      "epoch": 0.33206569664902996,
      "grad_norm": 0.05103868618607521,
      "learning_rate": 1.3359237213403882e-05,
      "loss": 0.0839,
      "step": 12050
    },
    {
      "epoch": 0.3323412698412698,
      "grad_norm": 0.6471331119537354,
      "learning_rate": 1.3353725749559083e-05,
      "loss": 0.0108,
      "step": 12060
    },
    {
      "epoch": 0.3326168430335097,
      "grad_norm": 0.8551211953163147,
      "learning_rate": 1.3348214285714286e-05,
      "loss": 0.0615,
      "step": 12070
    },
    {
      "epoch": 0.33289241622574955,
      "grad_norm": 0.02831994742155075,
      "learning_rate": 1.3342702821869488e-05,
      "loss": 0.0239,
      "step": 12080
    },
    {
      "epoch": 0.3331679894179894,
      "grad_norm": 0.05975860357284546,
      "learning_rate": 1.3337191358024693e-05,
      "loss": 0.0357,
      "step": 12090
    },
    {
      "epoch": 0.3334435626102293,
      "grad_norm": 66.16009521484375,
      "learning_rate": 1.3331679894179894e-05,
      "loss": 0.1639,
      "step": 12100
    },
    {
      "epoch": 0.33371913580246915,
      "grad_norm": 0.5249175429344177,
      "learning_rate": 1.3326168430335098e-05,
      "loss": 0.074,
      "step": 12110
    },
    {
      "epoch": 0.333994708994709,
      "grad_norm": 5.3515238761901855,
      "learning_rate": 1.3320656966490303e-05,
      "loss": 0.2108,
      "step": 12120
    },
    {
      "epoch": 0.3342702821869489,
      "grad_norm": 0.02341577224433422,
      "learning_rate": 1.3315145502645504e-05,
      "loss": 0.0625,
      "step": 12130
    },
    {
      "epoch": 0.33454585537918874,
      "grad_norm": 22.738353729248047,
      "learning_rate": 1.3309634038800707e-05,
      "loss": 0.106,
      "step": 12140
    },
    {
      "epoch": 0.33482142857142855,
      "grad_norm": 0.048439305275678635,
      "learning_rate": 1.3304122574955909e-05,
      "loss": 0.0448,
      "step": 12150
    },
    {
      "epoch": 0.3350970017636684,
      "grad_norm": 0.026413293555378914,
      "learning_rate": 1.3298611111111112e-05,
      "loss": 0.0846,
      "step": 12160
    },
    {
      "epoch": 0.3353725749559083,
      "grad_norm": 0.023215586319565773,
      "learning_rate": 1.3293099647266314e-05,
      "loss": 0.0997,
      "step": 12170
    },
    {
      "epoch": 0.33564814814814814,
      "grad_norm": 148.8963623046875,
      "learning_rate": 1.3287588183421519e-05,
      "loss": 0.1113,
      "step": 12180
    },
    {
      "epoch": 0.335923721340388,
      "grad_norm": 0.6125712990760803,
      "learning_rate": 1.328207671957672e-05,
      "loss": 0.1019,
      "step": 12190
    },
    {
      "epoch": 0.33619929453262787,
      "grad_norm": 0.03004496730864048,
      "learning_rate": 1.3276565255731924e-05,
      "loss": 0.2872,
      "step": 12200
    },
    {
      "epoch": 0.33647486772486773,
      "grad_norm": 2.5512571334838867,
      "learning_rate": 1.3271053791887127e-05,
      "loss": 0.2221,
      "step": 12210
    },
    {
      "epoch": 0.3367504409171076,
      "grad_norm": 4.4842939376831055,
      "learning_rate": 1.3265542328042328e-05,
      "loss": 0.0758,
      "step": 12220
    },
    {
      "epoch": 0.33702601410934746,
      "grad_norm": 13.130629539489746,
      "learning_rate": 1.3260030864197533e-05,
      "loss": 0.1666,
      "step": 12230
    },
    {
      "epoch": 0.3373015873015873,
      "grad_norm": 10.080059051513672,
      "learning_rate": 1.3254519400352735e-05,
      "loss": 0.1338,
      "step": 12240
    },
    {
      "epoch": 0.33757716049382713,
      "grad_norm": 0.027230126783251762,
      "learning_rate": 1.3249007936507938e-05,
      "loss": 0.0855,
      "step": 12250
    },
    {
      "epoch": 0.337852733686067,
      "grad_norm": 121.90682220458984,
      "learning_rate": 1.324349647266314e-05,
      "loss": 0.1848,
      "step": 12260
    },
    {
      "epoch": 0.33812830687830686,
      "grad_norm": 67.68790435791016,
      "learning_rate": 1.3237985008818343e-05,
      "loss": 0.2039,
      "step": 12270
    },
    {
      "epoch": 0.3384038800705467,
      "grad_norm": 0.03412783890962601,
      "learning_rate": 1.3232473544973545e-05,
      "loss": 0.3633,
      "step": 12280
    },
    {
      "epoch": 0.3386794532627866,
      "grad_norm": 0.07194478064775467,
      "learning_rate": 1.322696208112875e-05,
      "loss": 0.0931,
      "step": 12290
    },
    {
      "epoch": 0.33895502645502645,
      "grad_norm": 0.04605589807033539,
      "learning_rate": 1.3221450617283951e-05,
      "loss": 0.1522,
      "step": 12300
    },
    {
      "epoch": 0.3392305996472663,
      "grad_norm": 0.030415674671530724,
      "learning_rate": 1.3215939153439154e-05,
      "loss": 0.2286,
      "step": 12310
    },
    {
      "epoch": 0.3395061728395062,
      "grad_norm": 29.8393611907959,
      "learning_rate": 1.3210427689594358e-05,
      "loss": 0.0543,
      "step": 12320
    },
    {
      "epoch": 0.33978174603174605,
      "grad_norm": 0.5662255883216858,
      "learning_rate": 1.320491622574956e-05,
      "loss": 0.0104,
      "step": 12330
    },
    {
      "epoch": 0.3400573192239859,
      "grad_norm": 0.02154596708714962,
      "learning_rate": 1.3199404761904764e-05,
      "loss": 0.217,
      "step": 12340
    },
    {
      "epoch": 0.3403328924162258,
      "grad_norm": 0.02193526364862919,
      "learning_rate": 1.3193893298059966e-05,
      "loss": 0.1525,
      "step": 12350
    },
    {
      "epoch": 0.3406084656084656,
      "grad_norm": 2.0988192558288574,
      "learning_rate": 1.3188381834215169e-05,
      "loss": 0.1171,
      "step": 12360
    },
    {
      "epoch": 0.34088403880070545,
      "grad_norm": 29.138805389404297,
      "learning_rate": 1.318287037037037e-05,
      "loss": 0.1211,
      "step": 12370
    },
    {
      "epoch": 0.3411596119929453,
      "grad_norm": 0.021374845877289772,
      "learning_rate": 1.3177358906525576e-05,
      "loss": 0.1643,
      "step": 12380
    },
    {
      "epoch": 0.3414351851851852,
      "grad_norm": 0.024716036394238472,
      "learning_rate": 1.3171847442680777e-05,
      "loss": 0.292,
      "step": 12390
    },
    {
      "epoch": 0.34171075837742504,
      "grad_norm": 59.36734390258789,
      "learning_rate": 1.316633597883598e-05,
      "loss": 0.1277,
      "step": 12400
    },
    {
      "epoch": 0.3419863315696649,
      "grad_norm": 43.95828628540039,
      "learning_rate": 1.3160824514991182e-05,
      "loss": 0.0799,
      "step": 12410
    },
    {
      "epoch": 0.34226190476190477,
      "grad_norm": 1.0138565301895142,
      "learning_rate": 1.3155313051146385e-05,
      "loss": 0.128,
      "step": 12420
    },
    {
      "epoch": 0.34253747795414463,
      "grad_norm": 0.026024213060736656,
      "learning_rate": 1.314980158730159e-05,
      "loss": 0.0658,
      "step": 12430
    },
    {
      "epoch": 0.3428130511463845,
      "grad_norm": 0.02727731689810753,
      "learning_rate": 1.3144290123456792e-05,
      "loss": 0.0431,
      "step": 12440
    },
    {
      "epoch": 0.34308862433862436,
      "grad_norm": 0.026460761204361916,
      "learning_rate": 1.3138778659611995e-05,
      "loss": 0.1376,
      "step": 12450
    },
    {
      "epoch": 0.3433641975308642,
      "grad_norm": 0.028666747733950615,
      "learning_rate": 1.3133267195767197e-05,
      "loss": 0.173,
      "step": 12460
    },
    {
      "epoch": 0.34363977072310403,
      "grad_norm": 0.022439267486333847,
      "learning_rate": 1.31277557319224e-05,
      "loss": 0.1934,
      "step": 12470
    },
    {
      "epoch": 0.3439153439153439,
      "grad_norm": 0.23863741755485535,
      "learning_rate": 1.3122244268077601e-05,
      "loss": 0.1174,
      "step": 12480
    },
    {
      "epoch": 0.34419091710758376,
      "grad_norm": 58.98365783691406,
      "learning_rate": 1.3116732804232806e-05,
      "loss": 0.1278,
      "step": 12490
    },
    {
      "epoch": 0.3444664902998236,
      "grad_norm": 46.547794342041016,
      "learning_rate": 1.3111221340388008e-05,
      "loss": 0.142,
      "step": 12500
    },
    {
      "epoch": 0.3447420634920635,
      "grad_norm": 0.03069816716015339,
      "learning_rate": 1.3105709876543211e-05,
      "loss": 0.1244,
      "step": 12510
    },
    {
      "epoch": 0.34501763668430335,
      "grad_norm": 20.119813919067383,
      "learning_rate": 1.3100198412698413e-05,
      "loss": 0.0452,
      "step": 12520
    },
    {
      "epoch": 0.3452932098765432,
      "grad_norm": 0.02090330421924591,
      "learning_rate": 1.3094686948853616e-05,
      "loss": 0.0331,
      "step": 12530
    },
    {
      "epoch": 0.3455687830687831,
      "grad_norm": 0.040914371609687805,
      "learning_rate": 1.3089175485008821e-05,
      "loss": 0.0627,
      "step": 12540
    },
    {
      "epoch": 0.34584435626102294,
      "grad_norm": 32.281280517578125,
      "learning_rate": 1.3083664021164022e-05,
      "loss": 0.2776,
      "step": 12550
    },
    {
      "epoch": 0.3461199294532628,
      "grad_norm": 53.915828704833984,
      "learning_rate": 1.3078152557319226e-05,
      "loss": 0.1241,
      "step": 12560
    },
    {
      "epoch": 0.3463955026455027,
      "grad_norm": 0.1065935343503952,
      "learning_rate": 1.3072641093474427e-05,
      "loss": 0.0291,
      "step": 12570
    },
    {
      "epoch": 0.3466710758377425,
      "grad_norm": 46.70022964477539,
      "learning_rate": 1.306712962962963e-05,
      "loss": 0.0664,
      "step": 12580
    },
    {
      "epoch": 0.34694664902998235,
      "grad_norm": 0.052004341036081314,
      "learning_rate": 1.3061618165784832e-05,
      "loss": 0.0628,
      "step": 12590
    },
    {
      "epoch": 0.3472222222222222,
      "grad_norm": 0.03282590210437775,
      "learning_rate": 1.3056106701940037e-05,
      "loss": 0.0777,
      "step": 12600
    },
    {
      "epoch": 0.3474977954144621,
      "grad_norm": 0.14710916578769684,
      "learning_rate": 1.3050595238095239e-05,
      "loss": 0.003,
      "step": 12610
    },
    {
      "epoch": 0.34777336860670194,
      "grad_norm": 0.03907039761543274,
      "learning_rate": 1.3045083774250442e-05,
      "loss": 0.1287,
      "step": 12620
    },
    {
      "epoch": 0.3480489417989418,
      "grad_norm": 0.09192097187042236,
      "learning_rate": 1.3039572310405643e-05,
      "loss": 0.1177,
      "step": 12630
    },
    {
      "epoch": 0.34832451499118167,
      "grad_norm": 0.09893406182527542,
      "learning_rate": 1.3034060846560848e-05,
      "loss": 0.2233,
      "step": 12640
    },
    {
      "epoch": 0.34860008818342153,
      "grad_norm": 0.02168334275484085,
      "learning_rate": 1.3028549382716052e-05,
      "loss": 0.0398,
      "step": 12650
    },
    {
      "epoch": 0.3488756613756614,
      "grad_norm": 36.21457290649414,
      "learning_rate": 1.3023037918871253e-05,
      "loss": 0.1646,
      "step": 12660
    },
    {
      "epoch": 0.34915123456790126,
      "grad_norm": 0.0289152804762125,
      "learning_rate": 1.3017526455026456e-05,
      "loss": 0.0698,
      "step": 12670
    },
    {
      "epoch": 0.34942680776014107,
      "grad_norm": 0.4541836082935333,
      "learning_rate": 1.3012014991181658e-05,
      "loss": 0.0513,
      "step": 12680
    },
    {
      "epoch": 0.34970238095238093,
      "grad_norm": 0.024503884837031364,
      "learning_rate": 1.3006503527336863e-05,
      "loss": 0.1542,
      "step": 12690
    },
    {
      "epoch": 0.3499779541446208,
      "grad_norm": 0.05412012338638306,
      "learning_rate": 1.3000992063492065e-05,
      "loss": 0.079,
      "step": 12700
    },
    {
      "epoch": 0.35025352733686066,
      "grad_norm": 92.77584075927734,
      "learning_rate": 1.2995480599647268e-05,
      "loss": 0.102,
      "step": 12710
    },
    {
      "epoch": 0.3505291005291005,
      "grad_norm": 0.20589333772659302,
      "learning_rate": 1.298996913580247e-05,
      "loss": 0.3433,
      "step": 12720
    },
    {
      "epoch": 0.3508046737213404,
      "grad_norm": 0.07933172583580017,
      "learning_rate": 1.2984457671957673e-05,
      "loss": 0.0806,
      "step": 12730
    },
    {
      "epoch": 0.35108024691358025,
      "grad_norm": 65.92891693115234,
      "learning_rate": 1.2978946208112874e-05,
      "loss": 0.2908,
      "step": 12740
    },
    {
      "epoch": 0.3513558201058201,
      "grad_norm": 0.028905851766467094,
      "learning_rate": 1.2973434744268079e-05,
      "loss": 0.0153,
      "step": 12750
    },
    {
      "epoch": 0.35163139329806,
      "grad_norm": 0.024956006556749344,
      "learning_rate": 1.2967923280423282e-05,
      "loss": 0.0023,
      "step": 12760
    },
    {
      "epoch": 0.35190696649029984,
      "grad_norm": 0.026985645294189453,
      "learning_rate": 1.2962411816578484e-05,
      "loss": 0.0267,
      "step": 12770
    },
    {
      "epoch": 0.3521825396825397,
      "grad_norm": 0.3851166069507599,
      "learning_rate": 1.2956900352733687e-05,
      "loss": 0.0137,
      "step": 12780
    },
    {
      "epoch": 0.3524581128747795,
      "grad_norm": 23.301088333129883,
      "learning_rate": 1.2951388888888889e-05,
      "loss": 0.1921,
      "step": 12790
    },
    {
      "epoch": 0.3527336860670194,
      "grad_norm": 0.0316898450255394,
      "learning_rate": 1.2945877425044094e-05,
      "loss": 0.0615,
      "step": 12800
    },
    {
      "epoch": 0.35300925925925924,
      "grad_norm": 61.91754150390625,
      "learning_rate": 1.2940365961199295e-05,
      "loss": 0.0507,
      "step": 12810
    },
    {
      "epoch": 0.3532848324514991,
      "grad_norm": 0.07893707603216171,
      "learning_rate": 1.2934854497354499e-05,
      "loss": 0.0608,
      "step": 12820
    },
    {
      "epoch": 0.353560405643739,
      "grad_norm": 0.024310488253831863,
      "learning_rate": 1.29293430335097e-05,
      "loss": 0.1609,
      "step": 12830
    },
    {
      "epoch": 0.35383597883597884,
      "grad_norm": 0.01868441514670849,
      "learning_rate": 1.2923831569664903e-05,
      "loss": 0.0022,
      "step": 12840
    },
    {
      "epoch": 0.3541115520282187,
      "grad_norm": 0.02224559895694256,
      "learning_rate": 1.2918320105820108e-05,
      "loss": 0.0246,
      "step": 12850
    },
    {
      "epoch": 0.35438712522045857,
      "grad_norm": 0.03280065581202507,
      "learning_rate": 1.291280864197531e-05,
      "loss": 0.1453,
      "step": 12860
    },
    {
      "epoch": 0.35466269841269843,
      "grad_norm": 0.21320664882659912,
      "learning_rate": 1.2907297178130513e-05,
      "loss": 0.2292,
      "step": 12870
    },
    {
      "epoch": 0.3549382716049383,
      "grad_norm": 0.03206566348671913,
      "learning_rate": 1.2901785714285715e-05,
      "loss": 0.0667,
      "step": 12880
    },
    {
      "epoch": 0.35521384479717816,
      "grad_norm": 0.04852588102221489,
      "learning_rate": 1.289627425044092e-05,
      "loss": 0.1742,
      "step": 12890
    },
    {
      "epoch": 0.35548941798941797,
      "grad_norm": 96.82398223876953,
      "learning_rate": 1.2890762786596121e-05,
      "loss": 0.0962,
      "step": 12900
    },
    {
      "epoch": 0.35576499118165783,
      "grad_norm": 0.028809906914830208,
      "learning_rate": 1.2885251322751325e-05,
      "loss": 0.1756,
      "step": 12910
    },
    {
      "epoch": 0.3560405643738977,
      "grad_norm": 48.03437805175781,
      "learning_rate": 1.2879739858906526e-05,
      "loss": 0.2306,
      "step": 12920
    },
    {
      "epoch": 0.35631613756613756,
      "grad_norm": 0.030741212889552116,
      "learning_rate": 1.287422839506173e-05,
      "loss": 0.0939,
      "step": 12930
    },
    {
      "epoch": 0.3565917107583774,
      "grad_norm": 76.79158782958984,
      "learning_rate": 1.2868716931216931e-05,
      "loss": 0.2243,
      "step": 12940
    },
    {
      "epoch": 0.3568672839506173,
      "grad_norm": 0.019293779507279396,
      "learning_rate": 1.2863205467372136e-05,
      "loss": 0.0019,
      "step": 12950
    },
    {
      "epoch": 0.35714285714285715,
      "grad_norm": 2.397568702697754,
      "learning_rate": 1.2857694003527339e-05,
      "loss": 0.002,
      "step": 12960
    },
    {
      "epoch": 0.357418430335097,
      "grad_norm": 0.0764041617512703,
      "learning_rate": 1.285218253968254e-05,
      "loss": 0.0036,
      "step": 12970
    },
    {
      "epoch": 0.3576940035273369,
      "grad_norm": 0.05388248711824417,
      "learning_rate": 1.2846671075837744e-05,
      "loss": 0.0335,
      "step": 12980
    },
    {
      "epoch": 0.35796957671957674,
      "grad_norm": 0.028431160375475883,
      "learning_rate": 1.2841159611992945e-05,
      "loss": 0.0495,
      "step": 12990
    },
    {
      "epoch": 0.35824514991181655,
      "grad_norm": 0.015210540033876896,
      "learning_rate": 1.283564814814815e-05,
      "loss": 0.1783,
      "step": 13000
    },
    {
      "epoch": 0.3585207231040564,
      "grad_norm": 150.13998413085938,
      "learning_rate": 1.2830136684303352e-05,
      "loss": 0.0516,
      "step": 13010
    },
    {
      "epoch": 0.3587962962962963,
      "grad_norm": 0.023533277213573456,
      "learning_rate": 1.2824625220458555e-05,
      "loss": 0.0948,
      "step": 13020
    },
    {
      "epoch": 0.35907186948853614,
      "grad_norm": 0.019372770562767982,
      "learning_rate": 1.2819113756613757e-05,
      "loss": 0.0758,
      "step": 13030
    },
    {
      "epoch": 0.359347442680776,
      "grad_norm": 0.03697402402758598,
      "learning_rate": 1.281360229276896e-05,
      "loss": 0.1716,
      "step": 13040
    },
    {
      "epoch": 0.35962301587301587,
      "grad_norm": 59.804908752441406,
      "learning_rate": 1.2808090828924162e-05,
      "loss": 0.2478,
      "step": 13050
    },
    {
      "epoch": 0.35989858906525574,
      "grad_norm": 157.26707458496094,
      "learning_rate": 1.2802579365079367e-05,
      "loss": 0.0717,
      "step": 13060
    },
    {
      "epoch": 0.3601741622574956,
      "grad_norm": 119.7796401977539,
      "learning_rate": 1.279706790123457e-05,
      "loss": 0.0584,
      "step": 13070
    },
    {
      "epoch": 0.36044973544973546,
      "grad_norm": 2.34792160987854,
      "learning_rate": 1.2791556437389771e-05,
      "loss": 0.0408,
      "step": 13080
    },
    {
      "epoch": 0.36072530864197533,
      "grad_norm": 1.365070104598999,
      "learning_rate": 1.2786044973544975e-05,
      "loss": 0.1728,
      "step": 13090
    },
    {
      "epoch": 0.3610008818342152,
      "grad_norm": 0.026135796681046486,
      "learning_rate": 1.2780533509700176e-05,
      "loss": 0.0067,
      "step": 13100
    },
    {
      "epoch": 0.361276455026455,
      "grad_norm": 90.37748718261719,
      "learning_rate": 1.2775022045855381e-05,
      "loss": 0.0804,
      "step": 13110
    },
    {
      "epoch": 0.36155202821869487,
      "grad_norm": 7.361839771270752,
      "learning_rate": 1.2769510582010583e-05,
      "loss": 0.0383,
      "step": 13120
    },
    {
      "epoch": 0.36182760141093473,
      "grad_norm": 42.14433670043945,
      "learning_rate": 1.2763999118165786e-05,
      "loss": 0.1928,
      "step": 13130
    },
    {
      "epoch": 0.3621031746031746,
      "grad_norm": 2.690669298171997,
      "learning_rate": 1.2758487654320988e-05,
      "loss": 0.1429,
      "step": 13140
    },
    {
      "epoch": 0.36237874779541446,
      "grad_norm": 0.1899677813053131,
      "learning_rate": 1.2752976190476193e-05,
      "loss": 0.007,
      "step": 13150
    },
    {
      "epoch": 0.3626543209876543,
      "grad_norm": 0.027738049626350403,
      "learning_rate": 1.2747464726631394e-05,
      "loss": 0.0984,
      "step": 13160
    },
    {
      "epoch": 0.3629298941798942,
      "grad_norm": 0.10462909191846848,
      "learning_rate": 1.2741953262786597e-05,
      "loss": 0.1501,
      "step": 13170
    },
    {
      "epoch": 0.36320546737213405,
      "grad_norm": 0.1829536408185959,
      "learning_rate": 1.27364417989418e-05,
      "loss": 0.1888,
      "step": 13180
    },
    {
      "epoch": 0.3634810405643739,
      "grad_norm": 3.5875184535980225,
      "learning_rate": 1.2730930335097002e-05,
      "loss": 0.2174,
      "step": 13190
    },
    {
      "epoch": 0.3637566137566138,
      "grad_norm": 0.9994794130325317,
      "learning_rate": 1.2725418871252207e-05,
      "loss": 0.0808,
      "step": 13200
    },
    {
      "epoch": 0.36403218694885364,
      "grad_norm": 0.03611958771944046,
      "learning_rate": 1.2719907407407409e-05,
      "loss": 0.158,
      "step": 13210
    },
    {
      "epoch": 0.36430776014109345,
      "grad_norm": 0.025933897122740746,
      "learning_rate": 1.2714395943562612e-05,
      "loss": 0.0847,
      "step": 13220
    },
    {
      "epoch": 0.3645833333333333,
      "grad_norm": 33.83580017089844,
      "learning_rate": 1.2708884479717814e-05,
      "loss": 0.1164,
      "step": 13230
    },
    {
      "epoch": 0.3648589065255732,
      "grad_norm": 20.188650131225586,
      "learning_rate": 1.2703373015873017e-05,
      "loss": 0.1107,
      "step": 13240
    },
    {
      "epoch": 0.36513447971781304,
      "grad_norm": 0.11298199743032455,
      "learning_rate": 1.2697861552028218e-05,
      "loss": 0.1402,
      "step": 13250
    },
    {
      "epoch": 0.3654100529100529,
      "grad_norm": 1.652173399925232,
      "learning_rate": 1.2692350088183423e-05,
      "loss": 0.173,
      "step": 13260
    },
    {
      "epoch": 0.36568562610229277,
      "grad_norm": 1.270902395248413,
      "learning_rate": 1.2686838624338625e-05,
      "loss": 0.0666,
      "step": 13270
    },
    {
      "epoch": 0.36596119929453264,
      "grad_norm": 0.481436163187027,
      "learning_rate": 1.2681327160493828e-05,
      "loss": 0.0751,
      "step": 13280
    },
    {
      "epoch": 0.3662367724867725,
      "grad_norm": 65.35901641845703,
      "learning_rate": 1.2675815696649031e-05,
      "loss": 0.1537,
      "step": 13290
    },
    {
      "epoch": 0.36651234567901236,
      "grad_norm": 0.2993212938308716,
      "learning_rate": 1.2670304232804233e-05,
      "loss": 0.0099,
      "step": 13300
    },
    {
      "epoch": 0.3667879188712522,
      "grad_norm": 0.04746369272470474,
      "learning_rate": 1.2664792768959438e-05,
      "loss": 0.0656,
      "step": 13310
    },
    {
      "epoch": 0.36706349206349204,
      "grad_norm": 0.05877053737640381,
      "learning_rate": 1.265928130511464e-05,
      "loss": 0.0164,
      "step": 13320
    },
    {
      "epoch": 0.3673390652557319,
      "grad_norm": 0.027482038363814354,
      "learning_rate": 1.2653769841269843e-05,
      "loss": 0.1456,
      "step": 13330
    },
    {
      "epoch": 0.36761463844797176,
      "grad_norm": 68.57767486572266,
      "learning_rate": 1.2648258377425044e-05,
      "loss": 0.1909,
      "step": 13340
    },
    {
      "epoch": 0.36789021164021163,
      "grad_norm": 0.035130519419908524,
      "learning_rate": 1.2642746913580248e-05,
      "loss": 0.0696,
      "step": 13350
    },
    {
      "epoch": 0.3681657848324515,
      "grad_norm": 0.02865380235016346,
      "learning_rate": 1.2637235449735449e-05,
      "loss": 0.218,
      "step": 13360
    },
    {
      "epoch": 0.36844135802469136,
      "grad_norm": 10.310015678405762,
      "learning_rate": 1.2631723985890654e-05,
      "loss": 0.2286,
      "step": 13370
    },
    {
      "epoch": 0.3687169312169312,
      "grad_norm": 0.04007631912827492,
      "learning_rate": 1.2626212522045856e-05,
      "loss": 0.1186,
      "step": 13380
    },
    {
      "epoch": 0.3689925044091711,
      "grad_norm": 0.24594461917877197,
      "learning_rate": 1.2620701058201059e-05,
      "loss": 0.0896,
      "step": 13390
    },
    {
      "epoch": 0.36926807760141095,
      "grad_norm": 0.017975972965359688,
      "learning_rate": 1.2615189594356264e-05,
      "loss": 0.0742,
      "step": 13400
    },
    {
      "epoch": 0.3695436507936508,
      "grad_norm": 0.03096354939043522,
      "learning_rate": 1.2609678130511465e-05,
      "loss": 0.209,
      "step": 13410
    },
    {
      "epoch": 0.3698192239858907,
      "grad_norm": 0.033535879105329514,
      "learning_rate": 1.2604166666666669e-05,
      "loss": 0.177,
      "step": 13420
    },
    {
      "epoch": 0.3700947971781305,
      "grad_norm": 0.6206998825073242,
      "learning_rate": 1.259865520282187e-05,
      "loss": 0.2239,
      "step": 13430
    },
    {
      "epoch": 0.37037037037037035,
      "grad_norm": 0.05984894558787346,
      "learning_rate": 1.2593143738977073e-05,
      "loss": 0.1576,
      "step": 13440
    },
    {
      "epoch": 0.3706459435626102,
      "grad_norm": 0.04724805802106857,
      "learning_rate": 1.2587632275132275e-05,
      "loss": 0.01,
      "step": 13450
    },
    {
      "epoch": 0.3709215167548501,
      "grad_norm": 0.040226977318525314,
      "learning_rate": 1.258212081128748e-05,
      "loss": 0.0084,
      "step": 13460
    },
    {
      "epoch": 0.37119708994708994,
      "grad_norm": 0.10272447764873505,
      "learning_rate": 1.2576609347442682e-05,
      "loss": 0.1271,
      "step": 13470
    },
    {
      "epoch": 0.3714726631393298,
      "grad_norm": 0.3780028223991394,
      "learning_rate": 1.2571097883597885e-05,
      "loss": 0.1245,
      "step": 13480
    },
    {
      "epoch": 0.37174823633156967,
      "grad_norm": 0.07381066679954529,
      "learning_rate": 1.2565586419753086e-05,
      "loss": 0.045,
      "step": 13490
    },
    {
      "epoch": 0.37202380952380953,
      "grad_norm": 0.035087957978248596,
      "learning_rate": 1.256007495590829e-05,
      "loss": 0.0741,
      "step": 13500
    },
    {
      "epoch": 0.3722993827160494,
      "grad_norm": 0.016703402623534203,
      "learning_rate": 1.2554563492063495e-05,
      "loss": 0.0016,
      "step": 13510
    },
    {
      "epoch": 0.37257495590828926,
      "grad_norm": 0.08615736663341522,
      "learning_rate": 1.2549052028218696e-05,
      "loss": 0.1097,
      "step": 13520
    },
    {
      "epoch": 0.3728505291005291,
      "grad_norm": 0.018483351916074753,
      "learning_rate": 1.25435405643739e-05,
      "loss": 0.0048,
      "step": 13530
    },
    {
      "epoch": 0.37312610229276894,
      "grad_norm": 0.023279843851923943,
      "learning_rate": 1.2538029100529101e-05,
      "loss": 0.0484,
      "step": 13540
    },
    {
      "epoch": 0.3734016754850088,
      "grad_norm": 0.28790706396102905,
      "learning_rate": 1.2532517636684304e-05,
      "loss": 0.0323,
      "step": 13550
    },
    {
      "epoch": 0.37367724867724866,
      "grad_norm": 0.13808408379554749,
      "learning_rate": 1.2527006172839506e-05,
      "loss": 0.0108,
      "step": 13560
    },
    {
      "epoch": 0.3739528218694885,
      "grad_norm": 0.021624842658638954,
      "learning_rate": 1.252149470899471e-05,
      "loss": 0.2519,
      "step": 13570
    },
    {
      "epoch": 0.3742283950617284,
      "grad_norm": 0.02316278964281082,
      "learning_rate": 1.2515983245149912e-05,
      "loss": 0.1068,
      "step": 13580
    },
    {
      "epoch": 0.37450396825396826,
      "grad_norm": 0.03449857234954834,
      "learning_rate": 1.2510471781305116e-05,
      "loss": 0.1469,
      "step": 13590
    },
    {
      "epoch": 0.3747795414462081,
      "grad_norm": 13.330695152282715,
      "learning_rate": 1.2504960317460319e-05,
      "loss": 0.0798,
      "step": 13600
    },
    {
      "epoch": 0.375055114638448,
      "grad_norm": 0.056974198669195175,
      "learning_rate": 1.249944885361552e-05,
      "loss": 0.1668,
      "step": 13610
    },
    {
      "epoch": 0.37533068783068785,
      "grad_norm": 115.67140197753906,
      "learning_rate": 1.2493937389770725e-05,
      "loss": 0.1107,
      "step": 13620
    },
    {
      "epoch": 0.3756062610229277,
      "grad_norm": 23.631011962890625,
      "learning_rate": 1.2488425925925927e-05,
      "loss": 0.0304,
      "step": 13630
    },
    {
      "epoch": 0.3758818342151676,
      "grad_norm": 0.030920296907424927,
      "learning_rate": 1.248291446208113e-05,
      "loss": 0.1061,
      "step": 13640
    },
    {
      "epoch": 0.3761574074074074,
      "grad_norm": 58.88142013549805,
      "learning_rate": 1.2477402998236332e-05,
      "loss": 0.1157,
      "step": 13650
    },
    {
      "epoch": 0.37643298059964725,
      "grad_norm": 48.478782653808594,
      "learning_rate": 1.2471891534391537e-05,
      "loss": 0.1164,
      "step": 13660
    },
    {
      "epoch": 0.3767085537918871,
      "grad_norm": 0.11437482386827469,
      "learning_rate": 1.2466380070546738e-05,
      "loss": 0.0304,
      "step": 13670
    },
    {
      "epoch": 0.376984126984127,
      "grad_norm": 0.060034286230802536,
      "learning_rate": 1.2460868606701942e-05,
      "loss": 0.0716,
      "step": 13680
    },
    {
      "epoch": 0.37725970017636684,
      "grad_norm": 0.26606500148773193,
      "learning_rate": 1.2455357142857143e-05,
      "loss": 0.0041,
      "step": 13690
    },
    {
      "epoch": 0.3775352733686067,
      "grad_norm": 0.1579357236623764,
      "learning_rate": 1.2449845679012346e-05,
      "loss": 0.0065,
      "step": 13700
    },
    {
      "epoch": 0.37781084656084657,
      "grad_norm": 0.30657511949539185,
      "learning_rate": 1.2444334215167551e-05,
      "loss": 0.0859,
      "step": 13710
    },
    {
      "epoch": 0.37808641975308643,
      "grad_norm": 1.1985313892364502,
      "learning_rate": 1.2438822751322753e-05,
      "loss": 0.1435,
      "step": 13720
    },
    {
      "epoch": 0.3783619929453263,
      "grad_norm": 4.0314106941223145,
      "learning_rate": 1.2433311287477956e-05,
      "loss": 0.2352,
      "step": 13730
    },
    {
      "epoch": 0.37863756613756616,
      "grad_norm": 9.613512992858887,
      "learning_rate": 1.2427799823633158e-05,
      "loss": 0.097,
      "step": 13740
    },
    {
      "epoch": 0.37891313932980597,
      "grad_norm": 4.233458042144775,
      "learning_rate": 1.2422288359788361e-05,
      "loss": 0.0664,
      "step": 13750
    },
    {
      "epoch": 0.37918871252204583,
      "grad_norm": 0.04286370053887367,
      "learning_rate": 1.2416776895943563e-05,
      "loss": 0.1702,
      "step": 13760
    },
    {
      "epoch": 0.3794642857142857,
      "grad_norm": 0.030777646228671074,
      "learning_rate": 1.2411265432098767e-05,
      "loss": 0.0421,
      "step": 13770
    },
    {
      "epoch": 0.37973985890652556,
      "grad_norm": 40.84487533569336,
      "learning_rate": 1.2405753968253969e-05,
      "loss": 0.0725,
      "step": 13780
    },
    {
      "epoch": 0.3800154320987654,
      "grad_norm": 1.1034691333770752,
      "learning_rate": 1.2400242504409172e-05,
      "loss": 0.1126,
      "step": 13790
    },
    {
      "epoch": 0.3802910052910053,
      "grad_norm": 0.026276445016264915,
      "learning_rate": 1.2394731040564374e-05,
      "loss": 0.0311,
      "step": 13800
    },
    {
      "epoch": 0.38056657848324515,
      "grad_norm": 21.022201538085938,
      "learning_rate": 1.2389219576719577e-05,
      "loss": 0.1608,
      "step": 13810
    },
    {
      "epoch": 0.380842151675485,
      "grad_norm": 129.420166015625,
      "learning_rate": 1.2383708112874782e-05,
      "loss": 0.1422,
      "step": 13820
    },
    {
      "epoch": 0.3811177248677249,
      "grad_norm": 0.01742827519774437,
      "learning_rate": 1.2378196649029984e-05,
      "loss": 0.1118,
      "step": 13830
    },
    {
      "epoch": 0.38139329805996475,
      "grad_norm": 0.061823733150959015,
      "learning_rate": 1.2372685185185187e-05,
      "loss": 0.2612,
      "step": 13840
    },
    {
      "epoch": 0.3816688712522046,
      "grad_norm": 0.02434881590306759,
      "learning_rate": 1.2367173721340388e-05,
      "loss": 0.149,
      "step": 13850
    },
    {
      "epoch": 0.3819444444444444,
      "grad_norm": 30.084970474243164,
      "learning_rate": 1.2361662257495592e-05,
      "loss": 0.0815,
      "step": 13860
    },
    {
      "epoch": 0.3822200176366843,
      "grad_norm": 0.02661570906639099,
      "learning_rate": 1.2356150793650793e-05,
      "loss": 0.1055,
      "step": 13870
    },
    {
      "epoch": 0.38249559082892415,
      "grad_norm": 0.03967936709523201,
      "learning_rate": 1.2350639329805998e-05,
      "loss": 0.1624,
      "step": 13880
    },
    {
      "epoch": 0.382771164021164,
      "grad_norm": 0.05568770691752434,
      "learning_rate": 1.23451278659612e-05,
      "loss": 0.0935,
      "step": 13890
    },
    {
      "epoch": 0.3830467372134039,
      "grad_norm": 0.11631575971841812,
      "learning_rate": 1.2339616402116403e-05,
      "loss": 0.1692,
      "step": 13900
    },
    {
      "epoch": 0.38332231040564374,
      "grad_norm": 0.028309518471360207,
      "learning_rate": 1.2334104938271605e-05,
      "loss": 0.0906,
      "step": 13910
    },
    {
      "epoch": 0.3835978835978836,
      "grad_norm": 2.171567440032959,
      "learning_rate": 1.232859347442681e-05,
      "loss": 0.3932,
      "step": 13920
    },
    {
      "epoch": 0.38387345679012347,
      "grad_norm": 20.44271469116211,
      "learning_rate": 1.2323082010582013e-05,
      "loss": 0.2485,
      "step": 13930
    },
    {
      "epoch": 0.38414902998236333,
      "grad_norm": 41.46831512451172,
      "learning_rate": 1.2317570546737214e-05,
      "loss": 0.1238,
      "step": 13940
    },
    {
      "epoch": 0.3844246031746032,
      "grad_norm": 105.96001434326172,
      "learning_rate": 1.2312059082892418e-05,
      "loss": 0.0455,
      "step": 13950
    },
    {
      "epoch": 0.38470017636684306,
      "grad_norm": 10.439916610717773,
      "learning_rate": 1.230654761904762e-05,
      "loss": 0.1545,
      "step": 13960
    },
    {
      "epoch": 0.38497574955908287,
      "grad_norm": 0.0833546444773674,
      "learning_rate": 1.2301036155202824e-05,
      "loss": 0.1782,
      "step": 13970
    },
    {
      "epoch": 0.38525132275132273,
      "grad_norm": 0.023731429129838943,
      "learning_rate": 1.2295524691358026e-05,
      "loss": 0.0701,
      "step": 13980
    },
    {
      "epoch": 0.3855268959435626,
      "grad_norm": 0.07141493260860443,
      "learning_rate": 1.2290013227513229e-05,
      "loss": 0.1442,
      "step": 13990
    },
    {
      "epoch": 0.38580246913580246,
      "grad_norm": 0.029753895476460457,
      "learning_rate": 1.228450176366843e-05,
      "loss": 0.1895,
      "step": 14000
    },
    {
      "epoch": 0.3860780423280423,
      "grad_norm": 72.58519744873047,
      "learning_rate": 1.2278990299823634e-05,
      "loss": 0.246,
      "step": 14010
    },
    {
      "epoch": 0.3863536155202822,
      "grad_norm": 0.023988425731658936,
      "learning_rate": 1.2273478835978835e-05,
      "loss": 0.051,
      "step": 14020
    },
    {
      "epoch": 0.38662918871252205,
      "grad_norm": 0.028840715065598488,
      "learning_rate": 1.226796737213404e-05,
      "loss": 0.1052,
      "step": 14030
    },
    {
      "epoch": 0.3869047619047619,
      "grad_norm": 0.06460153311491013,
      "learning_rate": 1.2262455908289244e-05,
      "loss": 0.07,
      "step": 14040
    },
    {
      "epoch": 0.3871803350970018,
      "grad_norm": 0.05993203446269035,
      "learning_rate": 1.2256944444444445e-05,
      "loss": 0.0615,
      "step": 14050
    },
    {
      "epoch": 0.38745590828924165,
      "grad_norm": 0.08949200063943863,
      "learning_rate": 1.2251432980599648e-05,
      "loss": 0.1533,
      "step": 14060
    },
    {
      "epoch": 0.38773148148148145,
      "grad_norm": 0.030180804431438446,
      "learning_rate": 1.224592151675485e-05,
      "loss": 0.0038,
      "step": 14070
    },
    {
      "epoch": 0.3880070546737213,
      "grad_norm": 30.95631217956543,
      "learning_rate": 1.2240410052910055e-05,
      "loss": 0.0928,
      "step": 14080
    },
    {
      "epoch": 0.3882826278659612,
      "grad_norm": 1.6447628736495972,
      "learning_rate": 1.2234898589065256e-05,
      "loss": 0.0018,
      "step": 14090
    },
    {
      "epoch": 0.38855820105820105,
      "grad_norm": 0.07748713344335556,
      "learning_rate": 1.222938712522046e-05,
      "loss": 0.1836,
      "step": 14100
    },
    {
      "epoch": 0.3888337742504409,
      "grad_norm": 0.04926958307623863,
      "learning_rate": 1.2223875661375661e-05,
      "loss": 0.1316,
      "step": 14110
    },
    {
      "epoch": 0.3891093474426808,
      "grad_norm": 0.024008342996239662,
      "learning_rate": 1.2218364197530865e-05,
      "loss": 0.1716,
      "step": 14120
    },
    {
      "epoch": 0.38938492063492064,
      "grad_norm": 0.02242320403456688,
      "learning_rate": 1.2212852733686066e-05,
      "loss": 0.1823,
      "step": 14130
    },
    {
      "epoch": 0.3896604938271605,
      "grad_norm": 155.9515380859375,
      "learning_rate": 1.2207341269841271e-05,
      "loss": 0.203,
      "step": 14140
    },
    {
      "epoch": 0.38993606701940037,
      "grad_norm": 0.032878149300813675,
      "learning_rate": 1.2201829805996474e-05,
      "loss": 0.1459,
      "step": 14150
    },
    {
      "epoch": 0.39021164021164023,
      "grad_norm": 0.024529216811060905,
      "learning_rate": 1.2196318342151676e-05,
      "loss": 0.0583,
      "step": 14160
    },
    {
      "epoch": 0.3904872134038801,
      "grad_norm": 0.03400084003806114,
      "learning_rate": 1.2190806878306881e-05,
      "loss": 0.1071,
      "step": 14170
    },
    {
      "epoch": 0.3907627865961199,
      "grad_norm": 0.04847927764058113,
      "learning_rate": 1.2185295414462082e-05,
      "loss": 0.1012,
      "step": 14180
    },
    {
      "epoch": 0.39103835978835977,
      "grad_norm": 13.93795394897461,
      "learning_rate": 1.2179783950617286e-05,
      "loss": 0.2581,
      "step": 14190
    },
    {
      "epoch": 0.39131393298059963,
      "grad_norm": 88.89727020263672,
      "learning_rate": 1.2174272486772487e-05,
      "loss": 0.2074,
      "step": 14200
    },
    {
      "epoch": 0.3915895061728395,
      "grad_norm": 0.6797550320625305,
      "learning_rate": 1.216876102292769e-05,
      "loss": 0.2467,
      "step": 14210
    },
    {
      "epoch": 0.39186507936507936,
      "grad_norm": 0.023024385794997215,
      "learning_rate": 1.2163249559082892e-05,
      "loss": 0.0476,
      "step": 14220
    },
    {
      "epoch": 0.3921406525573192,
      "grad_norm": 102.83180236816406,
      "learning_rate": 1.2157738095238097e-05,
      "loss": 0.0827,
      "step": 14230
    },
    {
      "epoch": 0.3924162257495591,
      "grad_norm": 0.5691784620285034,
      "learning_rate": 1.2152226631393299e-05,
      "loss": 0.0029,
      "step": 14240
    },
    {
      "epoch": 0.39269179894179895,
      "grad_norm": 0.08507323265075684,
      "learning_rate": 1.2146715167548502e-05,
      "loss": 0.1381,
      "step": 14250
    },
    {
      "epoch": 0.3929673721340388,
      "grad_norm": 11.152108192443848,
      "learning_rate": 1.2141203703703705e-05,
      "loss": 0.0024,
      "step": 14260
    },
    {
      "epoch": 0.3932429453262787,
      "grad_norm": 0.03664644807577133,
      "learning_rate": 1.2135692239858907e-05,
      "loss": 0.14,
      "step": 14270
    },
    {
      "epoch": 0.39351851851851855,
      "grad_norm": 0.0454934798181057,
      "learning_rate": 1.2130180776014112e-05,
      "loss": 0.0735,
      "step": 14280
    },
    {
      "epoch": 0.39379409171075835,
      "grad_norm": 7.236422061920166,
      "learning_rate": 1.2124669312169313e-05,
      "loss": 0.1492,
      "step": 14290
    },
    {
      "epoch": 0.3940696649029982,
      "grad_norm": 1.598907232284546,
      "learning_rate": 1.2119157848324516e-05,
      "loss": 0.2014,
      "step": 14300
    },
    {
      "epoch": 0.3943452380952381,
      "grad_norm": 0.019396129995584488,
      "learning_rate": 1.2113646384479718e-05,
      "loss": 0.0061,
      "step": 14310
    },
    {
      "epoch": 0.39462081128747795,
      "grad_norm": 0.020589511841535568,
      "learning_rate": 1.2108134920634921e-05,
      "loss": 0.0037,
      "step": 14320
    },
    {
      "epoch": 0.3948963844797178,
      "grad_norm": 0.018527129665017128,
      "learning_rate": 1.2102623456790123e-05,
      "loss": 0.0551,
      "step": 14330
    },
    {
      "epoch": 0.3951719576719577,
      "grad_norm": 84.12403869628906,
      "learning_rate": 1.2097111992945328e-05,
      "loss": 0.0514,
      "step": 14340
    },
    {
      "epoch": 0.39544753086419754,
      "grad_norm": 0.021333739161491394,
      "learning_rate": 1.2091600529100531e-05,
      "loss": 0.2142,
      "step": 14350
    },
    {
      "epoch": 0.3957231040564374,
      "grad_norm": 14.227781295776367,
      "learning_rate": 1.2086089065255733e-05,
      "loss": 0.089,
      "step": 14360
    },
    {
      "epoch": 0.39599867724867727,
      "grad_norm": 0.03534088656306267,
      "learning_rate": 1.2080577601410936e-05,
      "loss": 0.0862,
      "step": 14370
    },
    {
      "epoch": 0.39627425044091713,
      "grad_norm": 0.30883967876434326,
      "learning_rate": 1.2075066137566137e-05,
      "loss": 0.0706,
      "step": 14380
    },
    {
      "epoch": 0.39654982363315694,
      "grad_norm": 0.05988992378115654,
      "learning_rate": 1.2069554673721342e-05,
      "loss": 0.1459,
      "step": 14390
    },
    {
      "epoch": 0.3968253968253968,
      "grad_norm": 2.338806629180908,
      "learning_rate": 1.2064043209876544e-05,
      "loss": 0.1321,
      "step": 14400
    },
    {
      "epoch": 0.39710097001763667,
      "grad_norm": 0.2393355369567871,
      "learning_rate": 1.2058531746031747e-05,
      "loss": 0.1821,
      "step": 14410
    },
    {
      "epoch": 0.39737654320987653,
      "grad_norm": 0.01708671636879444,
      "learning_rate": 1.2053020282186949e-05,
      "loss": 0.07,
      "step": 14420
    },
    {
      "epoch": 0.3976521164021164,
      "grad_norm": 40.10631561279297,
      "learning_rate": 1.2047508818342154e-05,
      "loss": 0.0722,
      "step": 14430
    },
    {
      "epoch": 0.39792768959435626,
      "grad_norm": 11.7268705368042,
      "learning_rate": 1.2041997354497355e-05,
      "loss": 0.1835,
      "step": 14440
    },
    {
      "epoch": 0.3982032627865961,
      "grad_norm": 0.02474568970501423,
      "learning_rate": 1.2036485890652559e-05,
      "loss": 0.0053,
      "step": 14450
    },
    {
      "epoch": 0.398478835978836,
      "grad_norm": 32.01184844970703,
      "learning_rate": 1.2030974426807762e-05,
      "loss": 0.0694,
      "step": 14460
    },
    {
      "epoch": 0.39875440917107585,
      "grad_norm": 0.029175162315368652,
      "learning_rate": 1.2025462962962963e-05,
      "loss": 0.1238,
      "step": 14470
    },
    {
      "epoch": 0.3990299823633157,
      "grad_norm": 0.041819144040346146,
      "learning_rate": 1.2019951499118168e-05,
      "loss": 0.1981,
      "step": 14480
    },
    {
      "epoch": 0.3993055555555556,
      "grad_norm": 0.050450388342142105,
      "learning_rate": 1.201444003527337e-05,
      "loss": 0.2121,
      "step": 14490
    },
    {
      "epoch": 0.3995811287477954,
      "grad_norm": 0.27104055881500244,
      "learning_rate": 1.2008928571428573e-05,
      "loss": 0.0676,
      "step": 14500
    },
    {
      "epoch": 0.39985670194003525,
      "grad_norm": 0.040857940912246704,
      "learning_rate": 1.2003417107583775e-05,
      "loss": 0.02,
      "step": 14510
    },
    {
      "epoch": 0.4001322751322751,
      "grad_norm": 0.018013769760727882,
      "learning_rate": 1.1997905643738978e-05,
      "loss": 0.1656,
      "step": 14520
    },
    {
      "epoch": 0.400407848324515,
      "grad_norm": 58.539306640625,
      "learning_rate": 1.199239417989418e-05,
      "loss": 0.1031,
      "step": 14530
    },
    {
      "epoch": 0.40068342151675485,
      "grad_norm": 0.020768627524375916,
      "learning_rate": 1.1986882716049384e-05,
      "loss": 0.1594,
      "step": 14540
    },
    {
      "epoch": 0.4009589947089947,
      "grad_norm": 141.9150848388672,
      "learning_rate": 1.1981371252204586e-05,
      "loss": 0.1611,
      "step": 14550
    },
    {
      "epoch": 0.4012345679012346,
      "grad_norm": 210.47438049316406,
      "learning_rate": 1.197585978835979e-05,
      "loss": 0.165,
      "step": 14560
    },
    {
      "epoch": 0.40151014109347444,
      "grad_norm": 0.2870761752128601,
      "learning_rate": 1.1970348324514993e-05,
      "loss": 0.0014,
      "step": 14570
    },
    {
      "epoch": 0.4017857142857143,
      "grad_norm": 1.554994821548462,
      "learning_rate": 1.1964836860670194e-05,
      "loss": 0.0862,
      "step": 14580
    },
    {
      "epoch": 0.40206128747795417,
      "grad_norm": 94.93737030029297,
      "learning_rate": 1.1959325396825399e-05,
      "loss": 0.1291,
      "step": 14590
    },
    {
      "epoch": 0.40233686067019403,
      "grad_norm": 0.022338835522532463,
      "learning_rate": 1.19538139329806e-05,
      "loss": 0.1263,
      "step": 14600
    },
    {
      "epoch": 0.40261243386243384,
      "grad_norm": 138.76925659179688,
      "learning_rate": 1.1948302469135804e-05,
      "loss": 0.0197,
      "step": 14610
    },
    {
      "epoch": 0.4028880070546737,
      "grad_norm": 5.108794212341309,
      "learning_rate": 1.1942791005291005e-05,
      "loss": 0.0839,
      "step": 14620
    },
    {
      "epoch": 0.40316358024691357,
      "grad_norm": 39.39045715332031,
      "learning_rate": 1.1937279541446209e-05,
      "loss": 0.1305,
      "step": 14630
    },
    {
      "epoch": 0.40343915343915343,
      "grad_norm": 117.01091003417969,
      "learning_rate": 1.193176807760141e-05,
      "loss": 0.1813,
      "step": 14640
    },
    {
      "epoch": 0.4037147266313933,
      "grad_norm": 34.380435943603516,
      "learning_rate": 1.1926256613756615e-05,
      "loss": 0.2029,
      "step": 14650
    },
    {
      "epoch": 0.40399029982363316,
      "grad_norm": 0.1394442915916443,
      "learning_rate": 1.1920745149911817e-05,
      "loss": 0.0778,
      "step": 14660
    },
    {
      "epoch": 0.404265873015873,
      "grad_norm": 0.21788732707500458,
      "learning_rate": 1.191523368606702e-05,
      "loss": 0.0404,
      "step": 14670
    },
    {
      "epoch": 0.4045414462081129,
      "grad_norm": 0.03626329079270363,
      "learning_rate": 1.1909722222222223e-05,
      "loss": 0.1385,
      "step": 14680
    },
    {
      "epoch": 0.40481701940035275,
      "grad_norm": 86.7984619140625,
      "learning_rate": 1.1904210758377427e-05,
      "loss": 0.1136,
      "step": 14690
    },
    {
      "epoch": 0.4050925925925926,
      "grad_norm": 0.027022816240787506,
      "learning_rate": 1.189869929453263e-05,
      "loss": 0.0974,
      "step": 14700
    },
    {
      "epoch": 0.4053681657848324,
      "grad_norm": 0.017704587429761887,
      "learning_rate": 1.1893187830687831e-05,
      "loss": 0.0301,
      "step": 14710
    },
    {
      "epoch": 0.4056437389770723,
      "grad_norm": 0.030442526564002037,
      "learning_rate": 1.1887676366843035e-05,
      "loss": 0.0019,
      "step": 14720
    },
    {
      "epoch": 0.40591931216931215,
      "grad_norm": 77.17879486083984,
      "learning_rate": 1.1882164902998236e-05,
      "loss": 0.0896,
      "step": 14730
    },
    {
      "epoch": 0.406194885361552,
      "grad_norm": 0.02272145450115204,
      "learning_rate": 1.1876653439153441e-05,
      "loss": 0.0744,
      "step": 14740
    },
    {
      "epoch": 0.4064704585537919,
      "grad_norm": 159.80555725097656,
      "learning_rate": 1.1871141975308643e-05,
      "loss": 0.0687,
      "step": 14750
    },
    {
      "epoch": 0.40674603174603174,
      "grad_norm": 166.4212646484375,
      "learning_rate": 1.1865630511463846e-05,
      "loss": 0.2283,
      "step": 14760
    },
    {
      "epoch": 0.4070216049382716,
      "grad_norm": 0.014877749606966972,
      "learning_rate": 1.1860119047619048e-05,
      "loss": 0.0792,
      "step": 14770
    },
    {
      "epoch": 0.4072971781305115,
      "grad_norm": 0.023189006373286247,
      "learning_rate": 1.185460758377425e-05,
      "loss": 0.0848,
      "step": 14780
    },
    {
      "epoch": 0.40757275132275134,
      "grad_norm": 131.44918823242188,
      "learning_rate": 1.1849096119929456e-05,
      "loss": 0.1287,
      "step": 14790
    },
    {
      "epoch": 0.4078483245149912,
      "grad_norm": 0.019620049744844437,
      "learning_rate": 1.1843584656084657e-05,
      "loss": 0.0636,
      "step": 14800
    },
    {
      "epoch": 0.40812389770723106,
      "grad_norm": 0.07379448413848877,
      "learning_rate": 1.183807319223986e-05,
      "loss": 0.088,
      "step": 14810
    },
    {
      "epoch": 0.4083994708994709,
      "grad_norm": 0.18099986016750336,
      "learning_rate": 1.1832561728395062e-05,
      "loss": 0.3479,
      "step": 14820
    },
    {
      "epoch": 0.40867504409171074,
      "grad_norm": 0.047522518783807755,
      "learning_rate": 1.1827050264550265e-05,
      "loss": 0.0525,
      "step": 14830
    },
    {
      "epoch": 0.4089506172839506,
      "grad_norm": 0.023761996999382973,
      "learning_rate": 1.1821538800705467e-05,
      "loss": 0.154,
      "step": 14840
    },
    {
      "epoch": 0.40922619047619047,
      "grad_norm": 0.035268884152173996,
      "learning_rate": 1.1816027336860672e-05,
      "loss": 0.1983,
      "step": 14850
    },
    {
      "epoch": 0.40950176366843033,
      "grad_norm": 0.021765144541859627,
      "learning_rate": 1.1810515873015874e-05,
      "loss": 0.0835,
      "step": 14860
    },
    {
      "epoch": 0.4097773368606702,
      "grad_norm": 0.034363873302936554,
      "learning_rate": 1.1805004409171077e-05,
      "loss": 0.0034,
      "step": 14870
    },
    {
      "epoch": 0.41005291005291006,
      "grad_norm": 0.23947890102863312,
      "learning_rate": 1.1799492945326278e-05,
      "loss": 0.1017,
      "step": 14880
    },
    {
      "epoch": 0.4103284832451499,
      "grad_norm": 153.55970764160156,
      "learning_rate": 1.1793981481481482e-05,
      "loss": 0.0364,
      "step": 14890
    },
    {
      "epoch": 0.4106040564373898,
      "grad_norm": 0.015607889741659164,
      "learning_rate": 1.1788470017636687e-05,
      "loss": 0.0094,
      "step": 14900
    },
    {
      "epoch": 0.41087962962962965,
      "grad_norm": 3.5949411392211914,
      "learning_rate": 1.1782958553791888e-05,
      "loss": 0.1683,
      "step": 14910
    },
    {
      "epoch": 0.4111552028218695,
      "grad_norm": 0.41764214634895325,
      "learning_rate": 1.1777447089947091e-05,
      "loss": 0.0014,
      "step": 14920
    },
    {
      "epoch": 0.4114307760141093,
      "grad_norm": 10.718482971191406,
      "learning_rate": 1.1771935626102293e-05,
      "loss": 0.0895,
      "step": 14930
    },
    {
      "epoch": 0.4117063492063492,
      "grad_norm": 0.11105078458786011,
      "learning_rate": 1.1766424162257498e-05,
      "loss": 0.0904,
      "step": 14940
    },
    {
      "epoch": 0.41198192239858905,
      "grad_norm": 8.659733772277832,
      "learning_rate": 1.17609126984127e-05,
      "loss": 0.2954,
      "step": 14950
    },
    {
      "epoch": 0.4122574955908289,
      "grad_norm": 0.016235623508691788,
      "learning_rate": 1.1755401234567903e-05,
      "loss": 0.0478,
      "step": 14960
    },
    {
      "epoch": 0.4125330687830688,
      "grad_norm": 0.015107601881027222,
      "learning_rate": 1.1749889770723104e-05,
      "loss": 0.1035,
      "step": 14970
    },
    {
      "epoch": 0.41280864197530864,
      "grad_norm": 0.6015961170196533,
      "learning_rate": 1.1744378306878308e-05,
      "loss": 0.106,
      "step": 14980
    },
    {
      "epoch": 0.4130842151675485,
      "grad_norm": 0.024982625618577003,
      "learning_rate": 1.1738866843033513e-05,
      "loss": 0.0352,
      "step": 14990
    },
    {
      "epoch": 0.41335978835978837,
      "grad_norm": 0.038298167288303375,
      "learning_rate": 1.1733355379188714e-05,
      "loss": 0.0779,
      "step": 15000
    },
    {
      "epoch": 0.41363536155202824,
      "grad_norm": 69.296142578125,
      "learning_rate": 1.1727843915343917e-05,
      "loss": 0.0756,
      "step": 15010
    },
    {
      "epoch": 0.4139109347442681,
      "grad_norm": 0.018992479890584946,
      "learning_rate": 1.1722332451499119e-05,
      "loss": 0.2697,
      "step": 15020
    },
    {
      "epoch": 0.41418650793650796,
      "grad_norm": 0.936854898929596,
      "learning_rate": 1.1716820987654322e-05,
      "loss": 0.0599,
      "step": 15030
    },
    {
      "epoch": 0.4144620811287478,
      "grad_norm": 143.63645935058594,
      "learning_rate": 1.1711309523809524e-05,
      "loss": 0.0387,
      "step": 15040
    },
    {
      "epoch": 0.41473765432098764,
      "grad_norm": 149.6968231201172,
      "learning_rate": 1.1705798059964729e-05,
      "loss": 0.0474,
      "step": 15050
    },
    {
      "epoch": 0.4150132275132275,
      "grad_norm": 0.021360015496611595,
      "learning_rate": 1.170028659611993e-05,
      "loss": 0.2416,
      "step": 15060
    },
    {
      "epoch": 0.41528880070546736,
      "grad_norm": 0.02895704284310341,
      "learning_rate": 1.1694775132275133e-05,
      "loss": 0.2472,
      "step": 15070
    },
    {
      "epoch": 0.41556437389770723,
      "grad_norm": 1.2451789379119873,
      "learning_rate": 1.1689263668430335e-05,
      "loss": 0.0837,
      "step": 15080
    },
    {
      "epoch": 0.4158399470899471,
      "grad_norm": 0.5722481608390808,
      "learning_rate": 1.1683752204585538e-05,
      "loss": 0.0773,
      "step": 15090
    },
    {
      "epoch": 0.41611552028218696,
      "grad_norm": 0.02100289985537529,
      "learning_rate": 1.1678240740740743e-05,
      "loss": 0.0078,
      "step": 15100
    },
    {
      "epoch": 0.4163910934744268,
      "grad_norm": 5.681981563568115,
      "learning_rate": 1.1672729276895945e-05,
      "loss": 0.2028,
      "step": 15110
    },
    {
      "epoch": 0.4166666666666667,
      "grad_norm": 154.5922393798828,
      "learning_rate": 1.1667217813051148e-05,
      "loss": 0.0828,
      "step": 15120
    },
    {
      "epoch": 0.41694223985890655,
      "grad_norm": 0.016068408265709877,
      "learning_rate": 1.166170634920635e-05,
      "loss": 0.0811,
      "step": 15130
    },
    {
      "epoch": 0.41721781305114636,
      "grad_norm": 6.2886481285095215,
      "learning_rate": 1.1656194885361553e-05,
      "loss": 0.1025,
      "step": 15140
    },
    {
      "epoch": 0.4174933862433862,
      "grad_norm": 0.01870562694966793,
      "learning_rate": 1.1650683421516754e-05,
      "loss": 0.0721,
      "step": 15150
    },
    {
      "epoch": 0.4177689594356261,
      "grad_norm": 1.804351806640625,
      "learning_rate": 1.164517195767196e-05,
      "loss": 0.1327,
      "step": 15160
    },
    {
      "epoch": 0.41804453262786595,
      "grad_norm": 0.02181832678616047,
      "learning_rate": 1.1639660493827161e-05,
      "loss": 0.2129,
      "step": 15170
    },
    {
      "epoch": 0.4183201058201058,
      "grad_norm": 0.187088280916214,
      "learning_rate": 1.1634149029982364e-05,
      "loss": 0.1539,
      "step": 15180
    },
    {
      "epoch": 0.4185956790123457,
      "grad_norm": 0.020762849599123,
      "learning_rate": 1.1628637566137566e-05,
      "loss": 0.2665,
      "step": 15190
    },
    {
      "epoch": 0.41887125220458554,
      "grad_norm": 0.05438864231109619,
      "learning_rate": 1.162312610229277e-05,
      "loss": 0.1028,
      "step": 15200
    },
    {
      "epoch": 0.4191468253968254,
      "grad_norm": 0.14512647688388824,
      "learning_rate": 1.1617614638447974e-05,
      "loss": 0.1501,
      "step": 15210
    },
    {
      "epoch": 0.41942239858906527,
      "grad_norm": 96.82110595703125,
      "learning_rate": 1.1612103174603176e-05,
      "loss": 0.3418,
      "step": 15220
    },
    {
      "epoch": 0.41969797178130513,
      "grad_norm": 20.26034164428711,
      "learning_rate": 1.1606591710758379e-05,
      "loss": 0.0817,
      "step": 15230
    },
    {
      "epoch": 0.419973544973545,
      "grad_norm": 21.112897872924805,
      "learning_rate": 1.160108024691358e-05,
      "loss": 0.2169,
      "step": 15240
    },
    {
      "epoch": 0.4202491181657848,
      "grad_norm": 23.87104034423828,
      "learning_rate": 1.1595568783068785e-05,
      "loss": 0.0734,
      "step": 15250
    },
    {
      "epoch": 0.42052469135802467,
      "grad_norm": 0.07056088745594025,
      "learning_rate": 1.1590057319223987e-05,
      "loss": 0.0425,
      "step": 15260
    },
    {
      "epoch": 0.42080026455026454,
      "grad_norm": 0.02801140956580639,
      "learning_rate": 1.158454585537919e-05,
      "loss": 0.1666,
      "step": 15270
    },
    {
      "epoch": 0.4210758377425044,
      "grad_norm": 0.21078120172023773,
      "learning_rate": 1.1579034391534392e-05,
      "loss": 0.0041,
      "step": 15280
    },
    {
      "epoch": 0.42135141093474426,
      "grad_norm": 0.019138220697641373,
      "learning_rate": 1.1573522927689595e-05,
      "loss": 0.0202,
      "step": 15290
    },
    {
      "epoch": 0.42162698412698413,
      "grad_norm": 0.018318522721529007,
      "learning_rate": 1.1568011463844797e-05,
      "loss": 0.2105,
      "step": 15300
    },
    {
      "epoch": 0.421902557319224,
      "grad_norm": 1.8794142007827759,
      "learning_rate": 1.1562500000000002e-05,
      "loss": 0.0036,
      "step": 15310
    },
    {
      "epoch": 0.42217813051146386,
      "grad_norm": 0.19777368009090424,
      "learning_rate": 1.1556988536155205e-05,
      "loss": 0.0018,
      "step": 15320
    },
    {
      "epoch": 0.4224537037037037,
      "grad_norm": 0.019924016669392586,
      "learning_rate": 1.1551477072310406e-05,
      "loss": 0.0484,
      "step": 15330
    },
    {
      "epoch": 0.4227292768959436,
      "grad_norm": 0.016301777213811874,
      "learning_rate": 1.154596560846561e-05,
      "loss": 0.1038,
      "step": 15340
    },
    {
      "epoch": 0.42300485008818345,
      "grad_norm": 0.1831578016281128,
      "learning_rate": 1.1540454144620811e-05,
      "loss": 0.1307,
      "step": 15350
    },
    {
      "epoch": 0.42328042328042326,
      "grad_norm": 0.5154680609703064,
      "learning_rate": 1.1534942680776016e-05,
      "loss": 0.1767,
      "step": 15360
    },
    {
      "epoch": 0.4235559964726631,
      "grad_norm": 0.013869233429431915,
      "learning_rate": 1.1529431216931218e-05,
      "loss": 0.1796,
      "step": 15370
    },
    {
      "epoch": 0.423831569664903,
      "grad_norm": 0.040575895458459854,
      "learning_rate": 1.1523919753086421e-05,
      "loss": 0.1445,
      "step": 15380
    },
    {
      "epoch": 0.42410714285714285,
      "grad_norm": 0.16751030087471008,
      "learning_rate": 1.1518408289241623e-05,
      "loss": 0.0598,
      "step": 15390
    },
    {
      "epoch": 0.4243827160493827,
      "grad_norm": 0.14021040499210358,
      "learning_rate": 1.1512896825396826e-05,
      "loss": 0.2319,
      "step": 15400
    },
    {
      "epoch": 0.4246582892416226,
      "grad_norm": 3.9779293537139893,
      "learning_rate": 1.1507385361552027e-05,
      "loss": 0.0027,
      "step": 15410
    },
    {
      "epoch": 0.42493386243386244,
      "grad_norm": 0.024073351174592972,
      "learning_rate": 1.1501873897707232e-05,
      "loss": 0.0174,
      "step": 15420
    },
    {
      "epoch": 0.4252094356261023,
      "grad_norm": 3.4914519786834717,
      "learning_rate": 1.1496362433862436e-05,
      "loss": 0.0023,
      "step": 15430
    },
    {
      "epoch": 0.42548500881834217,
      "grad_norm": 0.23514333367347717,
      "learning_rate": 1.1490850970017637e-05,
      "loss": 0.055,
      "step": 15440
    },
    {
      "epoch": 0.42576058201058203,
      "grad_norm": 128.03697204589844,
      "learning_rate": 1.148533950617284e-05,
      "loss": 0.1566,
      "step": 15450
    },
    {
      "epoch": 0.42603615520282184,
      "grad_norm": 0.046301908791065216,
      "learning_rate": 1.1479828042328044e-05,
      "loss": 0.0011,
      "step": 15460
    },
    {
      "epoch": 0.4263117283950617,
      "grad_norm": 142.1400604248047,
      "learning_rate": 1.1474316578483247e-05,
      "loss": 0.1368,
      "step": 15470
    },
    {
      "epoch": 0.42658730158730157,
      "grad_norm": 0.014789994806051254,
      "learning_rate": 1.1468805114638448e-05,
      "loss": 0.1523,
      "step": 15480
    },
    {
      "epoch": 0.42686287477954143,
      "grad_norm": 75.9791488647461,
      "learning_rate": 1.1463293650793652e-05,
      "loss": 0.1901,
      "step": 15490
    },
    {
      "epoch": 0.4271384479717813,
      "grad_norm": 13.164896965026855,
      "learning_rate": 1.1457782186948853e-05,
      "loss": 0.1786,
      "step": 15500
    },
    {
      "epoch": 0.42741402116402116,
      "grad_norm": 0.02634342759847641,
      "learning_rate": 1.1452270723104058e-05,
      "loss": 0.0039,
      "step": 15510
    },
    {
      "epoch": 0.427689594356261,
      "grad_norm": 0.016043944284319878,
      "learning_rate": 1.144675925925926e-05,
      "loss": 0.1356,
      "step": 15520
    },
    {
      "epoch": 0.4279651675485009,
      "grad_norm": 0.015237139537930489,
      "learning_rate": 1.1441247795414463e-05,
      "loss": 0.1566,
      "step": 15530
    },
    {
      "epoch": 0.42824074074074076,
      "grad_norm": 0.17390598356723785,
      "learning_rate": 1.1435736331569666e-05,
      "loss": 0.1491,
      "step": 15540
    },
    {
      "epoch": 0.4285163139329806,
      "grad_norm": 0.015029160305857658,
      "learning_rate": 1.1430224867724868e-05,
      "loss": 0.1621,
      "step": 15550
    },
    {
      "epoch": 0.4287918871252205,
      "grad_norm": 0.07972170412540436,
      "learning_rate": 1.1424713403880073e-05,
      "loss": 0.0809,
      "step": 15560
    },
    {
      "epoch": 0.4290674603174603,
      "grad_norm": 0.03574885055422783,
      "learning_rate": 1.1419201940035274e-05,
      "loss": 0.0021,
      "step": 15570
    },
    {
      "epoch": 0.42934303350970016,
      "grad_norm": 93.94816589355469,
      "learning_rate": 1.1413690476190478e-05,
      "loss": 0.1584,
      "step": 15580
    },
    {
      "epoch": 0.42961860670194,
      "grad_norm": 83.80692291259766,
      "learning_rate": 1.140817901234568e-05,
      "loss": 0.0129,
      "step": 15590
    },
    {
      "epoch": 0.4298941798941799,
      "grad_norm": 0.7496775388717651,
      "learning_rate": 1.1402667548500882e-05,
      "loss": 0.0145,
      "step": 15600
    },
    {
      "epoch": 0.43016975308641975,
      "grad_norm": 0.020725736394524574,
      "learning_rate": 1.1397156084656084e-05,
      "loss": 0.0829,
      "step": 15610
    },
    {
      "epoch": 0.4304453262786596,
      "grad_norm": 0.24497348070144653,
      "learning_rate": 1.1391644620811289e-05,
      "loss": 0.1468,
      "step": 15620
    },
    {
      "epoch": 0.4307208994708995,
      "grad_norm": 122.50065612792969,
      "learning_rate": 1.138613315696649e-05,
      "loss": 0.1002,
      "step": 15630
    },
    {
      "epoch": 0.43099647266313934,
      "grad_norm": 0.013732892461121082,
      "learning_rate": 1.1380621693121694e-05,
      "loss": 0.162,
      "step": 15640
    },
    {
      "epoch": 0.4312720458553792,
      "grad_norm": 0.037375133484601974,
      "learning_rate": 1.1375110229276897e-05,
      "loss": 0.0042,
      "step": 15650
    },
    {
      "epoch": 0.43154761904761907,
      "grad_norm": 0.01874118484556675,
      "learning_rate": 1.1369598765432099e-05,
      "loss": 0.0449,
      "step": 15660
    },
    {
      "epoch": 0.43182319223985893,
      "grad_norm": 0.02080649882555008,
      "learning_rate": 1.1364087301587304e-05,
      "loss": 0.0034,
      "step": 15670
    },
    {
      "epoch": 0.43209876543209874,
      "grad_norm": 1.779608964920044,
      "learning_rate": 1.1358575837742505e-05,
      "loss": 0.0967,
      "step": 15680
    },
    {
      "epoch": 0.4323743386243386,
      "grad_norm": 0.02000698819756508,
      "learning_rate": 1.1353064373897708e-05,
      "loss": 0.0074,
      "step": 15690
    },
    {
      "epoch": 0.43264991181657847,
      "grad_norm": 0.03155453875660896,
      "learning_rate": 1.134755291005291e-05,
      "loss": 0.124,
      "step": 15700
    },
    {
      "epoch": 0.43292548500881833,
      "grad_norm": 20.073637008666992,
      "learning_rate": 1.1342041446208115e-05,
      "loss": 0.1237,
      "step": 15710
    },
    {
      "epoch": 0.4332010582010582,
      "grad_norm": 0.015855856239795685,
      "learning_rate": 1.1336529982363316e-05,
      "loss": 0.2063,
      "step": 15720
    },
    {
      "epoch": 0.43347663139329806,
      "grad_norm": 63.430118560791016,
      "learning_rate": 1.133101851851852e-05,
      "loss": 0.1496,
      "step": 15730
    },
    {
      "epoch": 0.4337522045855379,
      "grad_norm": 0.02741529978811741,
      "learning_rate": 1.1325507054673723e-05,
      "loss": 0.1037,
      "step": 15740
    },
    {
      "epoch": 0.4340277777777778,
      "grad_norm": 0.015729675069451332,
      "learning_rate": 1.1319995590828925e-05,
      "loss": 0.23,
      "step": 15750
    },
    {
      "epoch": 0.43430335097001765,
      "grad_norm": 0.036818958818912506,
      "learning_rate": 1.131448412698413e-05,
      "loss": 0.0019,
      "step": 15760
    },
    {
      "epoch": 0.4345789241622575,
      "grad_norm": 0.01686929352581501,
      "learning_rate": 1.1308972663139331e-05,
      "loss": 0.0672,
      "step": 15770
    },
    {
      "epoch": 0.4348544973544973,
      "grad_norm": 0.019492991268634796,
      "learning_rate": 1.1303461199294534e-05,
      "loss": 0.1195,
      "step": 15780
    },
    {
      "epoch": 0.4351300705467372,
      "grad_norm": 0.0287180095911026,
      "learning_rate": 1.1297949735449736e-05,
      "loss": 0.0311,
      "step": 15790
    },
    {
      "epoch": 0.43540564373897706,
      "grad_norm": 0.025538725778460503,
      "learning_rate": 1.129243827160494e-05,
      "loss": 0.0715,
      "step": 15800
    },
    {
      "epoch": 0.4356812169312169,
      "grad_norm": 0.13256756961345673,
      "learning_rate": 1.128692680776014e-05,
      "loss": 0.1459,
      "step": 15810
    },
    {
      "epoch": 0.4359567901234568,
      "grad_norm": 0.10346189141273499,
      "learning_rate": 1.1281415343915346e-05,
      "loss": 0.0547,
      "step": 15820
    },
    {
      "epoch": 0.43623236331569665,
      "grad_norm": 0.19022969901561737,
      "learning_rate": 1.1275903880070547e-05,
      "loss": 0.003,
      "step": 15830
    },
    {
      "epoch": 0.4365079365079365,
      "grad_norm": 120.53125762939453,
      "learning_rate": 1.127039241622575e-05,
      "loss": 0.0532,
      "step": 15840
    },
    {
      "epoch": 0.4367835097001764,
      "grad_norm": 0.011349642649292946,
      "learning_rate": 1.1264880952380954e-05,
      "loss": 0.0011,
      "step": 15850
    },
    {
      "epoch": 0.43705908289241624,
      "grad_norm": 8.369837760925293,
      "learning_rate": 1.1259369488536155e-05,
      "loss": 0.2115,
      "step": 15860
    },
    {
      "epoch": 0.4373346560846561,
      "grad_norm": 5.8569817543029785,
      "learning_rate": 1.125385802469136e-05,
      "loss": 0.0737,
      "step": 15870
    },
    {
      "epoch": 0.43761022927689597,
      "grad_norm": 0.014684798195958138,
      "learning_rate": 1.1248346560846562e-05,
      "loss": 0.0846,
      "step": 15880
    },
    {
      "epoch": 0.4378858024691358,
      "grad_norm": 0.6456583738327026,
      "learning_rate": 1.1242835097001765e-05,
      "loss": 0.0673,
      "step": 15890
    },
    {
      "epoch": 0.43816137566137564,
      "grad_norm": 0.011805311776697636,
      "learning_rate": 1.1237323633156967e-05,
      "loss": 0.0107,
      "step": 15900
    },
    {
      "epoch": 0.4384369488536155,
      "grad_norm": 0.014591832645237446,
      "learning_rate": 1.123181216931217e-05,
      "loss": 0.0247,
      "step": 15910
    },
    {
      "epoch": 0.43871252204585537,
      "grad_norm": 153.8942413330078,
      "learning_rate": 1.1226300705467372e-05,
      "loss": 0.1365,
      "step": 15920
    },
    {
      "epoch": 0.43898809523809523,
      "grad_norm": 0.018529726192355156,
      "learning_rate": 1.1220789241622576e-05,
      "loss": 0.1542,
      "step": 15930
    },
    {
      "epoch": 0.4392636684303351,
      "grad_norm": 27.48740005493164,
      "learning_rate": 1.1215277777777778e-05,
      "loss": 0.076,
      "step": 15940
    },
    {
      "epoch": 0.43953924162257496,
      "grad_norm": 120.38876342773438,
      "learning_rate": 1.1209766313932981e-05,
      "loss": 0.3543,
      "step": 15950
    },
    {
      "epoch": 0.4398148148148148,
      "grad_norm": 0.03052491694688797,
      "learning_rate": 1.1204254850088185e-05,
      "loss": 0.0994,
      "step": 15960
    },
    {
      "epoch": 0.4400903880070547,
      "grad_norm": 128.64926147460938,
      "learning_rate": 1.1198743386243388e-05,
      "loss": 0.1469,
      "step": 15970
    },
    {
      "epoch": 0.44036596119929455,
      "grad_norm": 0.01941962167620659,
      "learning_rate": 1.1193231922398591e-05,
      "loss": 0.0012,
      "step": 15980
    },
    {
      "epoch": 0.4406415343915344,
      "grad_norm": 0.4527224004268646,
      "learning_rate": 1.1187720458553793e-05,
      "loss": 0.2023,
      "step": 15990
    },
    {
      "epoch": 0.4409171075837742,
      "grad_norm": 0.029080739244818687,
      "learning_rate": 1.1182208994708996e-05,
      "loss": 0.0584,
      "step": 16000
    },
    {
      "epoch": 0.4411926807760141,
      "grad_norm": 2.8944432735443115,
      "learning_rate": 1.1176697530864197e-05,
      "loss": 0.0924,
      "step": 16010
    },
    {
      "epoch": 0.44146825396825395,
      "grad_norm": 0.03472980111837387,
      "learning_rate": 1.1171186067019402e-05,
      "loss": 0.0012,
      "step": 16020
    },
    {
      "epoch": 0.4417438271604938,
      "grad_norm": 0.31569644808769226,
      "learning_rate": 1.1165674603174604e-05,
      "loss": 0.0216,
      "step": 16030
    },
    {
      "epoch": 0.4420194003527337,
      "grad_norm": 0.015201427973806858,
      "learning_rate": 1.1160163139329807e-05,
      "loss": 0.4558,
      "step": 16040
    },
    {
      "epoch": 0.44229497354497355,
      "grad_norm": 5.881524085998535,
      "learning_rate": 1.1154651675485009e-05,
      "loss": 0.0509,
      "step": 16050
    },
    {
      "epoch": 0.4425705467372134,
      "grad_norm": 109.96356201171875,
      "learning_rate": 1.1149140211640212e-05,
      "loss": 0.1513,
      "step": 16060
    },
    {
      "epoch": 0.4428461199294533,
      "grad_norm": 0.013770571909844875,
      "learning_rate": 1.1143628747795417e-05,
      "loss": 0.0015,
      "step": 16070
    },
    {
      "epoch": 0.44312169312169314,
      "grad_norm": 0.013708357699215412,
      "learning_rate": 1.1138117283950619e-05,
      "loss": 0.1707,
      "step": 16080
    },
    {
      "epoch": 0.443397266313933,
      "grad_norm": 153.30723571777344,
      "learning_rate": 1.1132605820105822e-05,
      "loss": 0.1188,
      "step": 16090
    },
    {
      "epoch": 0.44367283950617287,
      "grad_norm": 0.09980294853448868,
      "learning_rate": 1.1127094356261023e-05,
      "loss": 0.0017,
      "step": 16100
    },
    {
      "epoch": 0.4439484126984127,
      "grad_norm": 0.014170761220157146,
      "learning_rate": 1.1121582892416227e-05,
      "loss": 0.0673,
      "step": 16110
    },
    {
      "epoch": 0.44422398589065254,
      "grad_norm": 120.68692016601562,
      "learning_rate": 1.1116071428571428e-05,
      "loss": 0.0544,
      "step": 16120
    },
    {
      "epoch": 0.4444995590828924,
      "grad_norm": 0.33639925718307495,
      "learning_rate": 1.1110559964726633e-05,
      "loss": 0.2601,
      "step": 16130
    },
    {
      "epoch": 0.44477513227513227,
      "grad_norm": 152.5661163330078,
      "learning_rate": 1.1105048500881835e-05,
      "loss": 0.2666,
      "step": 16140
    },
    {
      "epoch": 0.44505070546737213,
      "grad_norm": 0.016291135922074318,
      "learning_rate": 1.1099537037037038e-05,
      "loss": 0.0921,
      "step": 16150
    },
    {
      "epoch": 0.445326278659612,
      "grad_norm": 0.032059118151664734,
      "learning_rate": 1.109402557319224e-05,
      "loss": 0.2566,
      "step": 16160
    },
    {
      "epoch": 0.44560185185185186,
      "grad_norm": 0.029766663908958435,
      "learning_rate": 1.1088514109347443e-05,
      "loss": 0.1333,
      "step": 16170
    },
    {
      "epoch": 0.4458774250440917,
      "grad_norm": 0.02802915684878826,
      "learning_rate": 1.1083002645502648e-05,
      "loss": 0.0149,
      "step": 16180
    },
    {
      "epoch": 0.4461529982363316,
      "grad_norm": 138.1876983642578,
      "learning_rate": 1.107749118165785e-05,
      "loss": 0.1289,
      "step": 16190
    },
    {
      "epoch": 0.44642857142857145,
      "grad_norm": 0.017066750675439835,
      "learning_rate": 1.1071979717813053e-05,
      "loss": 0.1344,
      "step": 16200
    },
    {
      "epoch": 0.44670414462081126,
      "grad_norm": 76.36561584472656,
      "learning_rate": 1.1066468253968254e-05,
      "loss": 0.2348,
      "step": 16210
    },
    {
      "epoch": 0.4469797178130511,
      "grad_norm": 0.8960390090942383,
      "learning_rate": 1.1060956790123457e-05,
      "loss": 0.0047,
      "step": 16220
    },
    {
      "epoch": 0.447255291005291,
      "grad_norm": 0.015998786315321922,
      "learning_rate": 1.105544532627866e-05,
      "loss": 0.0868,
      "step": 16230
    },
    {
      "epoch": 0.44753086419753085,
      "grad_norm": 0.03838218376040459,
      "learning_rate": 1.1049933862433864e-05,
      "loss": 0.0945,
      "step": 16240
    },
    {
      "epoch": 0.4478064373897707,
      "grad_norm": 69.60267639160156,
      "learning_rate": 1.1044422398589065e-05,
      "loss": 0.0966,
      "step": 16250
    },
    {
      "epoch": 0.4480820105820106,
      "grad_norm": 0.02851136401295662,
      "learning_rate": 1.1038910934744269e-05,
      "loss": 0.0092,
      "step": 16260
    },
    {
      "epoch": 0.44835758377425045,
      "grad_norm": 0.845758318901062,
      "learning_rate": 1.103339947089947e-05,
      "loss": 0.0016,
      "step": 16270
    },
    {
      "epoch": 0.4486331569664903,
      "grad_norm": 218.17611694335938,
      "learning_rate": 1.1027888007054675e-05,
      "loss": 0.1178,
      "step": 16280
    },
    {
      "epoch": 0.4489087301587302,
      "grad_norm": 0.01721647009253502,
      "learning_rate": 1.1022376543209879e-05,
      "loss": 0.1544,
      "step": 16290
    },
    {
      "epoch": 0.44918430335097004,
      "grad_norm": 0.018228251487016678,
      "learning_rate": 1.101686507936508e-05,
      "loss": 0.0608,
      "step": 16300
    },
    {
      "epoch": 0.4494598765432099,
      "grad_norm": 0.01665487512946129,
      "learning_rate": 1.1011353615520283e-05,
      "loss": 0.0301,
      "step": 16310
    },
    {
      "epoch": 0.4497354497354497,
      "grad_norm": 0.10794761776924133,
      "learning_rate": 1.1005842151675485e-05,
      "loss": 0.2137,
      "step": 16320
    },
    {
      "epoch": 0.4500110229276896,
      "grad_norm": 0.0644320696592331,
      "learning_rate": 1.100033068783069e-05,
      "loss": 0.1788,
      "step": 16330
    },
    {
      "epoch": 0.45028659611992944,
      "grad_norm": 62.4148063659668,
      "learning_rate": 1.0994819223985891e-05,
      "loss": 0.1152,
      "step": 16340
    },
    {
      "epoch": 0.4505621693121693,
      "grad_norm": 0.09533747285604477,
      "learning_rate": 1.0989307760141095e-05,
      "loss": 0.237,
      "step": 16350
    },
    {
      "epoch": 0.45083774250440917,
      "grad_norm": 47.67021560668945,
      "learning_rate": 1.0983796296296296e-05,
      "loss": 0.1976,
      "step": 16360
    },
    {
      "epoch": 0.45111331569664903,
      "grad_norm": 0.013356019742786884,
      "learning_rate": 1.09782848324515e-05,
      "loss": 0.0061,
      "step": 16370
    },
    {
      "epoch": 0.4513888888888889,
      "grad_norm": 0.019057558849453926,
      "learning_rate": 1.0972773368606704e-05,
      "loss": 0.0858,
      "step": 16380
    },
    {
      "epoch": 0.45166446208112876,
      "grad_norm": 0.02472498267889023,
      "learning_rate": 1.0967261904761906e-05,
      "loss": 0.1002,
      "step": 16390
    },
    {
      "epoch": 0.4519400352733686,
      "grad_norm": 0.054168395698070526,
      "learning_rate": 1.096175044091711e-05,
      "loss": 0.039,
      "step": 16400
    },
    {
      "epoch": 0.4522156084656085,
      "grad_norm": 0.027907613664865494,
      "learning_rate": 1.095623897707231e-05,
      "loss": 0.0669,
      "step": 16410
    },
    {
      "epoch": 0.45249118165784835,
      "grad_norm": 0.5388233661651611,
      "learning_rate": 1.0950727513227514e-05,
      "loss": 0.0012,
      "step": 16420
    },
    {
      "epoch": 0.45276675485008816,
      "grad_norm": 8.068694114685059,
      "learning_rate": 1.0945216049382716e-05,
      "loss": 0.068,
      "step": 16430
    },
    {
      "epoch": 0.453042328042328,
      "grad_norm": 0.017061645165085793,
      "learning_rate": 1.093970458553792e-05,
      "loss": 0.0675,
      "step": 16440
    },
    {
      "epoch": 0.4533179012345679,
      "grad_norm": 0.02717842534184456,
      "learning_rate": 1.0934193121693122e-05,
      "loss": 0.0882,
      "step": 16450
    },
    {
      "epoch": 0.45359347442680775,
      "grad_norm": 0.014304902404546738,
      "learning_rate": 1.0928681657848325e-05,
      "loss": 0.259,
      "step": 16460
    },
    {
      "epoch": 0.4538690476190476,
      "grad_norm": 83.38167572021484,
      "learning_rate": 1.0923170194003527e-05,
      "loss": 0.0838,
      "step": 16470
    },
    {
      "epoch": 0.4541446208112875,
      "grad_norm": 0.02939099632203579,
      "learning_rate": 1.0917658730158732e-05,
      "loss": 0.07,
      "step": 16480
    },
    {
      "epoch": 0.45442019400352734,
      "grad_norm": 0.03398129343986511,
      "learning_rate": 1.0912147266313935e-05,
      "loss": 0.0894,
      "step": 16490
    },
    {
      "epoch": 0.4546957671957672,
      "grad_norm": 0.173622727394104,
      "learning_rate": 1.0906635802469137e-05,
      "loss": 0.0015,
      "step": 16500
    },
    {
      "epoch": 0.4549713403880071,
      "grad_norm": 0.02384325861930847,
      "learning_rate": 1.090112433862434e-05,
      "loss": 0.0292,
      "step": 16510
    },
    {
      "epoch": 0.45524691358024694,
      "grad_norm": 17.07392692565918,
      "learning_rate": 1.0895612874779542e-05,
      "loss": 0.1783,
      "step": 16520
    },
    {
      "epoch": 0.45552248677248675,
      "grad_norm": 162.22366333007812,
      "learning_rate": 1.0890101410934747e-05,
      "loss": 0.1385,
      "step": 16530
    },
    {
      "epoch": 0.4557980599647266,
      "grad_norm": 0.0122249536216259,
      "learning_rate": 1.0884589947089948e-05,
      "loss": 0.2178,
      "step": 16540
    },
    {
      "epoch": 0.4560736331569665,
      "grad_norm": 0.020972250029444695,
      "learning_rate": 1.0879078483245151e-05,
      "loss": 0.2288,
      "step": 16550
    },
    {
      "epoch": 0.45634920634920634,
      "grad_norm": 0.18257082998752594,
      "learning_rate": 1.0873567019400353e-05,
      "loss": 0.1143,
      "step": 16560
    },
    {
      "epoch": 0.4566247795414462,
      "grad_norm": 0.014021500013768673,
      "learning_rate": 1.0868055555555556e-05,
      "loss": 0.09,
      "step": 16570
    },
    {
      "epoch": 0.45690035273368607,
      "grad_norm": 2.8367998600006104,
      "learning_rate": 1.0862544091710758e-05,
      "loss": 0.0894,
      "step": 16580
    },
    {
      "epoch": 0.45717592592592593,
      "grad_norm": 1.1585791110992432,
      "learning_rate": 1.0857032627865963e-05,
      "loss": 0.0715,
      "step": 16590
    },
    {
      "epoch": 0.4574514991181658,
      "grad_norm": 33.10268020629883,
      "learning_rate": 1.0851521164021166e-05,
      "loss": 0.1974,
      "step": 16600
    },
    {
      "epoch": 0.45772707231040566,
      "grad_norm": 0.07375448942184448,
      "learning_rate": 1.0846009700176368e-05,
      "loss": 0.0689,
      "step": 16610
    },
    {
      "epoch": 0.4580026455026455,
      "grad_norm": 119.4370346069336,
      "learning_rate": 1.084049823633157e-05,
      "loss": 0.1601,
      "step": 16620
    },
    {
      "epoch": 0.4582782186948854,
      "grad_norm": 0.015021768398582935,
      "learning_rate": 1.0834986772486772e-05,
      "loss": 0.052,
      "step": 16630
    },
    {
      "epoch": 0.4585537918871252,
      "grad_norm": 41.45602035522461,
      "learning_rate": 1.0829475308641977e-05,
      "loss": 0.3016,
      "step": 16640
    },
    {
      "epoch": 0.45882936507936506,
      "grad_norm": 138.26150512695312,
      "learning_rate": 1.0823963844797179e-05,
      "loss": 0.1041,
      "step": 16650
    },
    {
      "epoch": 0.4591049382716049,
      "grad_norm": 0.09433083236217499,
      "learning_rate": 1.0818452380952382e-05,
      "loss": 0.1313,
      "step": 16660
    },
    {
      "epoch": 0.4593805114638448,
      "grad_norm": 0.025243503972887993,
      "learning_rate": 1.0812940917107584e-05,
      "loss": 0.0055,
      "step": 16670
    },
    {
      "epoch": 0.45965608465608465,
      "grad_norm": 0.0186590738594532,
      "learning_rate": 1.0807429453262787e-05,
      "loss": 0.0747,
      "step": 16680
    },
    {
      "epoch": 0.4599316578483245,
      "grad_norm": 0.02290048822760582,
      "learning_rate": 1.0801917989417989e-05,
      "loss": 0.0465,
      "step": 16690
    },
    {
      "epoch": 0.4602072310405644,
      "grad_norm": 0.0334305614233017,
      "learning_rate": 1.0796406525573193e-05,
      "loss": 0.0022,
      "step": 16700
    },
    {
      "epoch": 0.46048280423280424,
      "grad_norm": 0.07997331023216248,
      "learning_rate": 1.0790895061728397e-05,
      "loss": 0.135,
      "step": 16710
    },
    {
      "epoch": 0.4607583774250441,
      "grad_norm": 0.014863671734929085,
      "learning_rate": 1.0785383597883598e-05,
      "loss": 0.0734,
      "step": 16720
    },
    {
      "epoch": 0.46103395061728397,
      "grad_norm": 0.023181112483143806,
      "learning_rate": 1.0779872134038802e-05,
      "loss": 0.016,
      "step": 16730
    },
    {
      "epoch": 0.46130952380952384,
      "grad_norm": 99.14202117919922,
      "learning_rate": 1.0774360670194005e-05,
      "loss": 0.0733,
      "step": 16740
    },
    {
      "epoch": 0.46158509700176364,
      "grad_norm": 0.01675647869706154,
      "learning_rate": 1.0768849206349208e-05,
      "loss": 0.0041,
      "step": 16750
    },
    {
      "epoch": 0.4618606701940035,
      "grad_norm": 0.04338746890425682,
      "learning_rate": 1.076333774250441e-05,
      "loss": 0.1825,
      "step": 16760
    },
    {
      "epoch": 0.4621362433862434,
      "grad_norm": 0.015579608269035816,
      "learning_rate": 1.0757826278659613e-05,
      "loss": 0.1292,
      "step": 16770
    },
    {
      "epoch": 0.46241181657848324,
      "grad_norm": 0.9167034029960632,
      "learning_rate": 1.0752314814814814e-05,
      "loss": 0.0263,
      "step": 16780
    },
    {
      "epoch": 0.4626873897707231,
      "grad_norm": 45.609798431396484,
      "learning_rate": 1.074680335097002e-05,
      "loss": 0.0757,
      "step": 16790
    },
    {
      "epoch": 0.46296296296296297,
      "grad_norm": 0.6223145723342896,
      "learning_rate": 1.0741291887125221e-05,
      "loss": 0.0884,
      "step": 16800
    },
    {
      "epoch": 0.46323853615520283,
      "grad_norm": 1.0358110666275024,
      "learning_rate": 1.0735780423280424e-05,
      "loss": 0.0213,
      "step": 16810
    },
    {
      "epoch": 0.4635141093474427,
      "grad_norm": 0.023191481828689575,
      "learning_rate": 1.0730268959435628e-05,
      "loss": 0.0047,
      "step": 16820
    },
    {
      "epoch": 0.46378968253968256,
      "grad_norm": 0.7408540844917297,
      "learning_rate": 1.0724757495590829e-05,
      "loss": 0.0311,
      "step": 16830
    },
    {
      "epoch": 0.4640652557319224,
      "grad_norm": 0.044307298958301544,
      "learning_rate": 1.0719246031746034e-05,
      "loss": 0.0012,
      "step": 16840
    },
    {
      "epoch": 0.46434082892416223,
      "grad_norm": 167.2202606201172,
      "learning_rate": 1.0713734567901236e-05,
      "loss": 0.1741,
      "step": 16850
    },
    {
      "epoch": 0.4646164021164021,
      "grad_norm": 0.8105502724647522,
      "learning_rate": 1.0708223104056439e-05,
      "loss": 0.3375,
      "step": 16860
    },
    {
      "epoch": 0.46489197530864196,
      "grad_norm": 7.523132801055908,
      "learning_rate": 1.070271164021164e-05,
      "loss": 0.1423,
      "step": 16870
    },
    {
      "epoch": 0.4651675485008818,
      "grad_norm": 0.02301115356385708,
      "learning_rate": 1.0697200176366844e-05,
      "loss": 0.042,
      "step": 16880
    },
    {
      "epoch": 0.4654431216931217,
      "grad_norm": 17.12550163269043,
      "learning_rate": 1.0691688712522045e-05,
      "loss": 0.0968,
      "step": 16890
    },
    {
      "epoch": 0.46571869488536155,
      "grad_norm": 0.022789960727095604,
      "learning_rate": 1.068617724867725e-05,
      "loss": 0.0734,
      "step": 16900
    },
    {
      "epoch": 0.4659942680776014,
      "grad_norm": 0.013490431942045689,
      "learning_rate": 1.0680665784832452e-05,
      "loss": 0.0796,
      "step": 16910
    },
    {
      "epoch": 0.4662698412698413,
      "grad_norm": 0.0548701286315918,
      "learning_rate": 1.0675154320987655e-05,
      "loss": 0.1167,
      "step": 16920
    },
    {
      "epoch": 0.46654541446208114,
      "grad_norm": 0.05715750530362129,
      "learning_rate": 1.0669642857142858e-05,
      "loss": 0.0011,
      "step": 16930
    },
    {
      "epoch": 0.466820987654321,
      "grad_norm": 0.029425179585814476,
      "learning_rate": 1.066413139329806e-05,
      "loss": 0.1615,
      "step": 16940
    },
    {
      "epoch": 0.46709656084656087,
      "grad_norm": 0.5012626647949219,
      "learning_rate": 1.0658619929453265e-05,
      "loss": 0.0762,
      "step": 16950
    },
    {
      "epoch": 0.4673721340388007,
      "grad_norm": 0.01939983293414116,
      "learning_rate": 1.0653108465608466e-05,
      "loss": 0.0658,
      "step": 16960
    },
    {
      "epoch": 0.46764770723104054,
      "grad_norm": 4.9578070640563965,
      "learning_rate": 1.064759700176367e-05,
      "loss": 0.0972,
      "step": 16970
    },
    {
      "epoch": 0.4679232804232804,
      "grad_norm": 0.014954442158341408,
      "learning_rate": 1.0642085537918871e-05,
      "loss": 0.0075,
      "step": 16980
    },
    {
      "epoch": 0.46819885361552027,
      "grad_norm": 0.7786701321601868,
      "learning_rate": 1.0636574074074074e-05,
      "loss": 0.0682,
      "step": 16990
    },
    {
      "epoch": 0.46847442680776014,
      "grad_norm": 0.6914312243461609,
      "learning_rate": 1.0631062610229278e-05,
      "loss": 0.1035,
      "step": 17000
    },
    {
      "epoch": 0.46875,
      "grad_norm": 27.12845230102539,
      "learning_rate": 1.0625551146384481e-05,
      "loss": 0.1396,
      "step": 17010
    },
    {
      "epoch": 0.46902557319223986,
      "grad_norm": 14.371621131896973,
      "learning_rate": 1.0620039682539683e-05,
      "loss": 0.1171,
      "step": 17020
    },
    {
      "epoch": 0.46930114638447973,
      "grad_norm": 0.16761313378810883,
      "learning_rate": 1.0614528218694886e-05,
      "loss": 0.0798,
      "step": 17030
    },
    {
      "epoch": 0.4695767195767196,
      "grad_norm": 0.01650162786245346,
      "learning_rate": 1.060901675485009e-05,
      "loss": 0.136,
      "step": 17040
    },
    {
      "epoch": 0.46985229276895946,
      "grad_norm": 0.018392294645309448,
      "learning_rate": 1.0603505291005292e-05,
      "loss": 0.0011,
      "step": 17050
    },
    {
      "epoch": 0.4701278659611993,
      "grad_norm": 0.021373983472585678,
      "learning_rate": 1.0597993827160496e-05,
      "loss": 0.2471,
      "step": 17060
    },
    {
      "epoch": 0.47040343915343913,
      "grad_norm": 1.8183879852294922,
      "learning_rate": 1.0592482363315697e-05,
      "loss": 0.0498,
      "step": 17070
    },
    {
      "epoch": 0.470679012345679,
      "grad_norm": 20.719898223876953,
      "learning_rate": 1.05869708994709e-05,
      "loss": 0.2639,
      "step": 17080
    },
    {
      "epoch": 0.47095458553791886,
      "grad_norm": 74.22734069824219,
      "learning_rate": 1.0581459435626102e-05,
      "loss": 0.0463,
      "step": 17090
    },
    {
      "epoch": 0.4712301587301587,
      "grad_norm": 2.5604372024536133,
      "learning_rate": 1.0575947971781307e-05,
      "loss": 0.041,
      "step": 17100
    },
    {
      "epoch": 0.4715057319223986,
      "grad_norm": 0.07035841047763824,
      "learning_rate": 1.0570436507936508e-05,
      "loss": 0.066,
      "step": 17110
    },
    {
      "epoch": 0.47178130511463845,
      "grad_norm": 0.02058270201086998,
      "learning_rate": 1.0564925044091712e-05,
      "loss": 0.1392,
      "step": 17120
    },
    {
      "epoch": 0.4720568783068783,
      "grad_norm": 0.02312762662768364,
      "learning_rate": 1.0559413580246915e-05,
      "loss": 0.1587,
      "step": 17130
    },
    {
      "epoch": 0.4723324514991182,
      "grad_norm": 0.02488882467150688,
      "learning_rate": 1.0553902116402117e-05,
      "loss": 0.13,
      "step": 17140
    },
    {
      "epoch": 0.47260802469135804,
      "grad_norm": 138.43426513671875,
      "learning_rate": 1.0548390652557321e-05,
      "loss": 0.1518,
      "step": 17150
    },
    {
      "epoch": 0.4728835978835979,
      "grad_norm": 0.03082081489264965,
      "learning_rate": 1.0542879188712523e-05,
      "loss": 0.0745,
      "step": 17160
    },
    {
      "epoch": 0.47315917107583777,
      "grad_norm": 0.026994632557034492,
      "learning_rate": 1.0537367724867726e-05,
      "loss": 0.2185,
      "step": 17170
    },
    {
      "epoch": 0.4734347442680776,
      "grad_norm": 79.47576141357422,
      "learning_rate": 1.0531856261022928e-05,
      "loss": 0.2145,
      "step": 17180
    },
    {
      "epoch": 0.47371031746031744,
      "grad_norm": 41.30725860595703,
      "learning_rate": 1.0526344797178131e-05,
      "loss": 0.0988,
      "step": 17190
    },
    {
      "epoch": 0.4739858906525573,
      "grad_norm": 5.52752161026001,
      "learning_rate": 1.0520833333333333e-05,
      "loss": 0.0018,
      "step": 17200
    },
    {
      "epoch": 0.47426146384479717,
      "grad_norm": 0.015405938029289246,
      "learning_rate": 1.0515321869488538e-05,
      "loss": 0.0024,
      "step": 17210
    },
    {
      "epoch": 0.47453703703703703,
      "grad_norm": 0.014180165715515614,
      "learning_rate": 1.050981040564374e-05,
      "loss": 0.0894,
      "step": 17220
    },
    {
      "epoch": 0.4748126102292769,
      "grad_norm": 0.013708394952118397,
      "learning_rate": 1.0504298941798942e-05,
      "loss": 0.0474,
      "step": 17230
    },
    {
      "epoch": 0.47508818342151676,
      "grad_norm": 56.632808685302734,
      "learning_rate": 1.0498787477954146e-05,
      "loss": 0.1351,
      "step": 17240
    },
    {
      "epoch": 0.4753637566137566,
      "grad_norm": 142.14077758789062,
      "learning_rate": 1.0493276014109349e-05,
      "loss": 0.0295,
      "step": 17250
    },
    {
      "epoch": 0.4756393298059965,
      "grad_norm": 0.04747185856103897,
      "learning_rate": 1.0487764550264552e-05,
      "loss": 0.0641,
      "step": 17260
    },
    {
      "epoch": 0.47591490299823636,
      "grad_norm": 0.01448100246489048,
      "learning_rate": 1.0482253086419754e-05,
      "loss": 0.0063,
      "step": 17270
    },
    {
      "epoch": 0.47619047619047616,
      "grad_norm": 0.0226108618080616,
      "learning_rate": 1.0476741622574957e-05,
      "loss": 0.0327,
      "step": 17280
    },
    {
      "epoch": 0.47646604938271603,
      "grad_norm": 0.019110986962914467,
      "learning_rate": 1.0471230158730159e-05,
      "loss": 0.1892,
      "step": 17290
    },
    {
      "epoch": 0.4767416225749559,
      "grad_norm": 14.559168815612793,
      "learning_rate": 1.0465718694885364e-05,
      "loss": 0.1556,
      "step": 17300
    },
    {
      "epoch": 0.47701719576719576,
      "grad_norm": 0.02540847659111023,
      "learning_rate": 1.0460207231040565e-05,
      "loss": 0.0011,
      "step": 17310
    },
    {
      "epoch": 0.4772927689594356,
      "grad_norm": 0.1122056096792221,
      "learning_rate": 1.0454695767195768e-05,
      "loss": 0.1688,
      "step": 17320
    },
    {
      "epoch": 0.4775683421516755,
      "grad_norm": 0.015715407207608223,
      "learning_rate": 1.044918430335097e-05,
      "loss": 0.0872,
      "step": 17330
    },
    {
      "epoch": 0.47784391534391535,
      "grad_norm": 0.26270121335983276,
      "learning_rate": 1.0443672839506173e-05,
      "loss": 0.1073,
      "step": 17340
    },
    {
      "epoch": 0.4781194885361552,
      "grad_norm": 0.02093961276113987,
      "learning_rate": 1.0438161375661378e-05,
      "loss": 0.0835,
      "step": 17350
    },
    {
      "epoch": 0.4783950617283951,
      "grad_norm": 0.015280797146260738,
      "learning_rate": 1.043264991181658e-05,
      "loss": 0.0274,
      "step": 17360
    },
    {
      "epoch": 0.47867063492063494,
      "grad_norm": 1.08529531955719,
      "learning_rate": 1.0427138447971783e-05,
      "loss": 0.136,
      "step": 17370
    },
    {
      "epoch": 0.4789462081128748,
      "grad_norm": 0.3617822825908661,
      "learning_rate": 1.0421626984126985e-05,
      "loss": 0.1418,
      "step": 17380
    },
    {
      "epoch": 0.4792217813051146,
      "grad_norm": 0.026936659589409828,
      "learning_rate": 1.0416115520282188e-05,
      "loss": 0.0839,
      "step": 17390
    },
    {
      "epoch": 0.4794973544973545,
      "grad_norm": 0.014979472383856773,
      "learning_rate": 1.041060405643739e-05,
      "loss": 0.1746,
      "step": 17400
    },
    {
      "epoch": 0.47977292768959434,
      "grad_norm": 0.04236694797873497,
      "learning_rate": 1.0405092592592594e-05,
      "loss": 0.001,
      "step": 17410
    },
    {
      "epoch": 0.4800485008818342,
      "grad_norm": 8.745633125305176,
      "learning_rate": 1.0399581128747796e-05,
      "loss": 0.0555,
      "step": 17420
    },
    {
      "epoch": 0.48032407407407407,
      "grad_norm": 0.025920934975147247,
      "learning_rate": 1.0394069664903e-05,
      "loss": 0.1441,
      "step": 17430
    },
    {
      "epoch": 0.48059964726631393,
      "grad_norm": 0.013461178168654442,
      "learning_rate": 1.03885582010582e-05,
      "loss": 0.035,
      "step": 17440
    },
    {
      "epoch": 0.4808752204585538,
      "grad_norm": 0.030168820172548294,
      "learning_rate": 1.0383046737213404e-05,
      "loss": 0.119,
      "step": 17450
    },
    {
      "epoch": 0.48115079365079366,
      "grad_norm": 0.8583923578262329,
      "learning_rate": 1.0377535273368609e-05,
      "loss": 0.0866,
      "step": 17460
    },
    {
      "epoch": 0.4814263668430335,
      "grad_norm": 0.0720134824514389,
      "learning_rate": 1.037202380952381e-05,
      "loss": 0.1022,
      "step": 17470
    },
    {
      "epoch": 0.4817019400352734,
      "grad_norm": 0.01847773976624012,
      "learning_rate": 1.0366512345679014e-05,
      "loss": 0.0597,
      "step": 17480
    },
    {
      "epoch": 0.48197751322751325,
      "grad_norm": 0.0875352993607521,
      "learning_rate": 1.0361000881834215e-05,
      "loss": 0.1056,
      "step": 17490
    },
    {
      "epoch": 0.48225308641975306,
      "grad_norm": 0.07776355743408203,
      "learning_rate": 1.0355489417989419e-05,
      "loss": 0.0586,
      "step": 17500
    },
    {
      "epoch": 0.4825286596119929,
      "grad_norm": 13.329693794250488,
      "learning_rate": 1.0349977954144622e-05,
      "loss": 0.2462,
      "step": 17510
    },
    {
      "epoch": 0.4828042328042328,
      "grad_norm": 0.034389428794384,
      "learning_rate": 1.0344466490299825e-05,
      "loss": 0.001,
      "step": 17520
    },
    {
      "epoch": 0.48307980599647266,
      "grad_norm": 0.4093307852745056,
      "learning_rate": 1.0338955026455027e-05,
      "loss": 0.0603,
      "step": 17530
    },
    {
      "epoch": 0.4833553791887125,
      "grad_norm": 0.01875975728034973,
      "learning_rate": 1.033344356261023e-05,
      "loss": 0.1327,
      "step": 17540
    },
    {
      "epoch": 0.4836309523809524,
      "grad_norm": 0.1422203630208969,
      "learning_rate": 1.0327932098765431e-05,
      "loss": 0.0023,
      "step": 17550
    },
    {
      "epoch": 0.48390652557319225,
      "grad_norm": 128.4788055419922,
      "learning_rate": 1.0322420634920636e-05,
      "loss": 0.1441,
      "step": 17560
    },
    {
      "epoch": 0.4841820987654321,
      "grad_norm": 0.01898246444761753,
      "learning_rate": 1.031690917107584e-05,
      "loss": 0.0711,
      "step": 17570
    },
    {
      "epoch": 0.484457671957672,
      "grad_norm": 0.040167856961488724,
      "learning_rate": 1.0311397707231041e-05,
      "loss": 0.0536,
      "step": 17580
    },
    {
      "epoch": 0.48473324514991184,
      "grad_norm": 0.012880167923867702,
      "learning_rate": 1.0305886243386245e-05,
      "loss": 0.0725,
      "step": 17590
    },
    {
      "epoch": 0.48500881834215165,
      "grad_norm": 19.46021842956543,
      "learning_rate": 1.0300374779541446e-05,
      "loss": 0.3235,
      "step": 17600
    },
    {
      "epoch": 0.4852843915343915,
      "grad_norm": 19.614891052246094,
      "learning_rate": 1.0294863315696651e-05,
      "loss": 0.0907,
      "step": 17610
    },
    {
      "epoch": 0.4855599647266314,
      "grad_norm": 0.01578076183795929,
      "learning_rate": 1.0289351851851853e-05,
      "loss": 0.118,
      "step": 17620
    },
    {
      "epoch": 0.48583553791887124,
      "grad_norm": 0.014290045015513897,
      "learning_rate": 1.0283840388007056e-05,
      "loss": 0.2589,
      "step": 17630
    },
    {
      "epoch": 0.4861111111111111,
      "grad_norm": 0.0256216861307621,
      "learning_rate": 1.0278328924162257e-05,
      "loss": 0.0013,
      "step": 17640
    },
    {
      "epoch": 0.48638668430335097,
      "grad_norm": 2.2708513736724854,
      "learning_rate": 1.027281746031746e-05,
      "loss": 0.0384,
      "step": 17650
    },
    {
      "epoch": 0.48666225749559083,
      "grad_norm": 0.049545768648386,
      "learning_rate": 1.0267305996472662e-05,
      "loss": 0.0611,
      "step": 17660
    },
    {
      "epoch": 0.4869378306878307,
      "grad_norm": 0.045992638915777206,
      "learning_rate": 1.0261794532627867e-05,
      "loss": 0.1892,
      "step": 17670
    },
    {
      "epoch": 0.48721340388007056,
      "grad_norm": 0.5722023248672485,
      "learning_rate": 1.025628306878307e-05,
      "loss": 0.0943,
      "step": 17680
    },
    {
      "epoch": 0.4874889770723104,
      "grad_norm": 0.032268647104501724,
      "learning_rate": 1.0250771604938272e-05,
      "loss": 0.0761,
      "step": 17690
    },
    {
      "epoch": 0.4877645502645503,
      "grad_norm": 0.10161472111940384,
      "learning_rate": 1.0245260141093475e-05,
      "loss": 0.1142,
      "step": 17700
    },
    {
      "epoch": 0.4880401234567901,
      "grad_norm": 0.020921753719449043,
      "learning_rate": 1.0239748677248677e-05,
      "loss": 0.0368,
      "step": 17710
    },
    {
      "epoch": 0.48831569664902996,
      "grad_norm": 0.017246156930923462,
      "learning_rate": 1.0234237213403882e-05,
      "loss": 0.1468,
      "step": 17720
    },
    {
      "epoch": 0.4885912698412698,
      "grad_norm": 0.015334288589656353,
      "learning_rate": 1.0228725749559083e-05,
      "loss": 0.2143,
      "step": 17730
    },
    {
      "epoch": 0.4888668430335097,
      "grad_norm": 0.03263343125581741,
      "learning_rate": 1.0223214285714287e-05,
      "loss": 0.001,
      "step": 17740
    },
    {
      "epoch": 0.48914241622574955,
      "grad_norm": 3.3583316802978516,
      "learning_rate": 1.0217702821869488e-05,
      "loss": 0.0814,
      "step": 17750
    },
    {
      "epoch": 0.4894179894179894,
      "grad_norm": 0.03401365503668785,
      "learning_rate": 1.0212191358024691e-05,
      "loss": 0.0966,
      "step": 17760
    },
    {
      "epoch": 0.4896935626102293,
      "grad_norm": 0.0521988719701767,
      "learning_rate": 1.0206679894179895e-05,
      "loss": 0.1538,
      "step": 17770
    },
    {
      "epoch": 0.48996913580246915,
      "grad_norm": 0.1823524534702301,
      "learning_rate": 1.0201168430335098e-05,
      "loss": 0.1334,
      "step": 17780
    },
    {
      "epoch": 0.490244708994709,
      "grad_norm": 0.030888007953763008,
      "learning_rate": 1.0195656966490301e-05,
      "loss": 0.0009,
      "step": 17790
    },
    {
      "epoch": 0.4905202821869489,
      "grad_norm": 116.3779525756836,
      "learning_rate": 1.0190145502645503e-05,
      "loss": 0.0703,
      "step": 17800
    },
    {
      "epoch": 0.49079585537918874,
      "grad_norm": 115.69345092773438,
      "learning_rate": 1.0184634038800708e-05,
      "loss": 0.045,
      "step": 17810
    },
    {
      "epoch": 0.49107142857142855,
      "grad_norm": 0.012977015227079391,
      "learning_rate": 1.017912257495591e-05,
      "loss": 0.0738,
      "step": 17820
    },
    {
      "epoch": 0.4913470017636684,
      "grad_norm": 0.2025662064552307,
      "learning_rate": 1.0173611111111113e-05,
      "loss": 0.0303,
      "step": 17830
    },
    {
      "epoch": 0.4916225749559083,
      "grad_norm": 0.017532020807266235,
      "learning_rate": 1.0168099647266314e-05,
      "loss": 0.0017,
      "step": 17840
    },
    {
      "epoch": 0.49189814814814814,
      "grad_norm": 0.012381828390061855,
      "learning_rate": 1.0162588183421517e-05,
      "loss": 0.0546,
      "step": 17850
    },
    {
      "epoch": 0.492173721340388,
      "grad_norm": 0.014116336591541767,
      "learning_rate": 1.0157076719576719e-05,
      "loss": 0.2569,
      "step": 17860
    },
    {
      "epoch": 0.49244929453262787,
      "grad_norm": 0.09744452685117722,
      "learning_rate": 1.0151565255731924e-05,
      "loss": 0.0403,
      "step": 17870
    },
    {
      "epoch": 0.49272486772486773,
      "grad_norm": 0.017333263531327248,
      "learning_rate": 1.0146053791887127e-05,
      "loss": 0.2299,
      "step": 17880
    },
    {
      "epoch": 0.4930004409171076,
      "grad_norm": 0.04096619039773941,
      "learning_rate": 1.0140542328042329e-05,
      "loss": 0.0013,
      "step": 17890
    },
    {
      "epoch": 0.49327601410934746,
      "grad_norm": 0.07038091123104095,
      "learning_rate": 1.0135030864197532e-05,
      "loss": 0.0703,
      "step": 17900
    },
    {
      "epoch": 0.4935515873015873,
      "grad_norm": 0.010803278535604477,
      "learning_rate": 1.0129519400352734e-05,
      "loss": 0.1902,
      "step": 17910
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 3.6217041015625,
      "learning_rate": 1.0124007936507939e-05,
      "loss": 0.0973,
      "step": 17920
    },
    {
      "epoch": 0.494102733686067,
      "grad_norm": 0.01126058865338564,
      "learning_rate": 1.011849647266314e-05,
      "loss": 0.001,
      "step": 17930
    },
    {
      "epoch": 0.49437830687830686,
      "grad_norm": 0.12361124157905579,
      "learning_rate": 1.0112985008818343e-05,
      "loss": 0.0008,
      "step": 17940
    },
    {
      "epoch": 0.4946538800705467,
      "grad_norm": 0.03671301528811455,
      "learning_rate": 1.0107473544973545e-05,
      "loss": 0.0485,
      "step": 17950
    },
    {
      "epoch": 0.4949294532627866,
      "grad_norm": 1.2351042032241821,
      "learning_rate": 1.0101962081128748e-05,
      "loss": 0.3209,
      "step": 17960
    },
    {
      "epoch": 0.49520502645502645,
      "grad_norm": 0.016386620700359344,
      "learning_rate": 1.009645061728395e-05,
      "loss": 0.0062,
      "step": 17970
    },
    {
      "epoch": 0.4954805996472663,
      "grad_norm": 56.75571060180664,
      "learning_rate": 1.0090939153439155e-05,
      "loss": 0.008,
      "step": 17980
    },
    {
      "epoch": 0.4957561728395062,
      "grad_norm": 0.04268776997923851,
      "learning_rate": 1.0085427689594358e-05,
      "loss": 0.0687,
      "step": 17990
    },
    {
      "epoch": 0.49603174603174605,
      "grad_norm": 63.309104919433594,
      "learning_rate": 1.007991622574956e-05,
      "loss": 0.0988,
      "step": 18000
    },
    {
      "epoch": 0.4963073192239859,
      "grad_norm": 0.031172094866633415,
      "learning_rate": 1.0074404761904763e-05,
      "loss": 0.0577,
      "step": 18010
    },
    {
      "epoch": 0.4965828924162258,
      "grad_norm": 0.01017521321773529,
      "learning_rate": 1.0068893298059966e-05,
      "loss": 0.0999,
      "step": 18020
    },
    {
      "epoch": 0.4968584656084656,
      "grad_norm": 0.8064532279968262,
      "learning_rate": 1.006338183421517e-05,
      "loss": 0.1026,
      "step": 18030
    },
    {
      "epoch": 0.49713403880070545,
      "grad_norm": 0.02633262798190117,
      "learning_rate": 1.005787037037037e-05,
      "loss": 0.0181,
      "step": 18040
    },
    {
      "epoch": 0.4974096119929453,
      "grad_norm": 0.06485273689031601,
      "learning_rate": 1.0052358906525574e-05,
      "loss": 0.0051,
      "step": 18050
    },
    {
      "epoch": 0.4976851851851852,
      "grad_norm": 0.05803399533033371,
      "learning_rate": 1.0046847442680776e-05,
      "loss": 0.0023,
      "step": 18060
    },
    {
      "epoch": 0.49796075837742504,
      "grad_norm": 6.344544410705566,
      "learning_rate": 1.004133597883598e-05,
      "loss": 0.2252,
      "step": 18070
    },
    {
      "epoch": 0.4982363315696649,
      "grad_norm": 116.66387939453125,
      "learning_rate": 1.0035824514991182e-05,
      "loss": 0.1369,
      "step": 18080
    },
    {
      "epoch": 0.49851190476190477,
      "grad_norm": 95.7582015991211,
      "learning_rate": 1.0030313051146385e-05,
      "loss": 0.1737,
      "step": 18090
    },
    {
      "epoch": 0.49878747795414463,
      "grad_norm": 0.3720870614051819,
      "learning_rate": 1.0024801587301589e-05,
      "loss": 0.0894,
      "step": 18100
    },
    {
      "epoch": 0.4990630511463845,
      "grad_norm": 0.009805026464164257,
      "learning_rate": 1.001929012345679e-05,
      "loss": 0.0012,
      "step": 18110
    },
    {
      "epoch": 0.49933862433862436,
      "grad_norm": 0.09119710326194763,
      "learning_rate": 1.0013778659611995e-05,
      "loss": 0.2908,
      "step": 18120
    },
    {
      "epoch": 0.4996141975308642,
      "grad_norm": 0.056913744658231735,
      "learning_rate": 1.0008267195767197e-05,
      "loss": 0.044,
      "step": 18130
    },
    {
      "epoch": 0.49988977072310403,
      "grad_norm": 0.06333337724208832,
      "learning_rate": 1.00027557319224e-05,
      "loss": 0.1412,
      "step": 18140
    },
    {
      "epoch": 0.500165343915344,
      "grad_norm": 14.101468086242676,
      "learning_rate": 9.997244268077603e-06,
      "loss": 0.1732,
      "step": 18150
    },
    {
      "epoch": 0.5004409171075838,
      "grad_norm": 104.14920043945312,
      "learning_rate": 9.991732804232805e-06,
      "loss": 0.0524,
      "step": 18160
    },
    {
      "epoch": 0.5007164902998237,
      "grad_norm": 0.026083864271640778,
      "learning_rate": 9.986221340388008e-06,
      "loss": 0.3793,
      "step": 18170
    },
    {
      "epoch": 0.5009920634920635,
      "grad_norm": 0.010699382983148098,
      "learning_rate": 9.980709876543211e-06,
      "loss": 0.1113,
      "step": 18180
    },
    {
      "epoch": 0.5012676366843033,
      "grad_norm": 0.02485722117125988,
      "learning_rate": 9.975198412698413e-06,
      "loss": 0.0872,
      "step": 18190
    },
    {
      "epoch": 0.5015432098765432,
      "grad_norm": 0.011343123391270638,
      "learning_rate": 9.969686948853616e-06,
      "loss": 0.001,
      "step": 18200
    },
    {
      "epoch": 0.501818783068783,
      "grad_norm": 0.012543249875307083,
      "learning_rate": 9.96417548500882e-06,
      "loss": 0.0147,
      "step": 18210
    },
    {
      "epoch": 0.502094356261023,
      "grad_norm": 0.012539493851363659,
      "learning_rate": 9.958664021164021e-06,
      "loss": 0.1122,
      "step": 18220
    },
    {
      "epoch": 0.5023699294532628,
      "grad_norm": 57.60398483276367,
      "learning_rate": 9.953152557319224e-06,
      "loss": 0.0865,
      "step": 18230
    },
    {
      "epoch": 0.5026455026455027,
      "grad_norm": 66.46641540527344,
      "learning_rate": 9.947641093474428e-06,
      "loss": 0.2635,
      "step": 18240
    },
    {
      "epoch": 0.5029210758377425,
      "grad_norm": 38.46506881713867,
      "learning_rate": 9.942129629629629e-06,
      "loss": 0.2644,
      "step": 18250
    },
    {
      "epoch": 0.5031966490299824,
      "grad_norm": 1.4624300003051758,
      "learning_rate": 9.936618165784834e-06,
      "loss": 0.0591,
      "step": 18260
    },
    {
      "epoch": 0.5034722222222222,
      "grad_norm": 0.032123155891895294,
      "learning_rate": 9.931106701940036e-06,
      "loss": 0.1382,
      "step": 18270
    },
    {
      "epoch": 0.5037477954144621,
      "grad_norm": 36.77497482299805,
      "learning_rate": 9.925595238095239e-06,
      "loss": 0.0523,
      "step": 18280
    },
    {
      "epoch": 0.5040233686067019,
      "grad_norm": 0.013731206767261028,
      "learning_rate": 9.920083774250442e-06,
      "loss": 0.1832,
      "step": 18290
    },
    {
      "epoch": 0.5042989417989417,
      "grad_norm": 109.89283752441406,
      "learning_rate": 9.914572310405645e-06,
      "loss": 0.0842,
      "step": 18300
    },
    {
      "epoch": 0.5045745149911817,
      "grad_norm": 60.267311096191406,
      "learning_rate": 9.909060846560847e-06,
      "loss": 0.0356,
      "step": 18310
    },
    {
      "epoch": 0.5048500881834215,
      "grad_norm": 152.12059020996094,
      "learning_rate": 9.90354938271605e-06,
      "loss": 0.0689,
      "step": 18320
    },
    {
      "epoch": 0.5051256613756614,
      "grad_norm": 0.1865474134683609,
      "learning_rate": 9.898037918871253e-06,
      "loss": 0.1208,
      "step": 18330
    },
    {
      "epoch": 0.5054012345679012,
      "grad_norm": 0.6079084873199463,
      "learning_rate": 9.892526455026455e-06,
      "loss": 0.1606,
      "step": 18340
    },
    {
      "epoch": 0.5056768077601411,
      "grad_norm": 41.298824310302734,
      "learning_rate": 9.887014991181658e-06,
      "loss": 0.1247,
      "step": 18350
    },
    {
      "epoch": 0.5059523809523809,
      "grad_norm": 0.011701248586177826,
      "learning_rate": 9.881503527336862e-06,
      "loss": 0.0123,
      "step": 18360
    },
    {
      "epoch": 0.5062279541446209,
      "grad_norm": 17.558441162109375,
      "learning_rate": 9.875992063492065e-06,
      "loss": 0.3757,
      "step": 18370
    },
    {
      "epoch": 0.5065035273368607,
      "grad_norm": 0.014407061971724033,
      "learning_rate": 9.870480599647268e-06,
      "loss": 0.1681,
      "step": 18380
    },
    {
      "epoch": 0.5067791005291006,
      "grad_norm": 3.7628841400146484,
      "learning_rate": 9.86496913580247e-06,
      "loss": 0.12,
      "step": 18390
    },
    {
      "epoch": 0.5070546737213404,
      "grad_norm": 0.30657729506492615,
      "learning_rate": 9.859457671957673e-06,
      "loss": 0.0022,
      "step": 18400
    },
    {
      "epoch": 0.5073302469135802,
      "grad_norm": 22.248384475708008,
      "learning_rate": 9.853946208112876e-06,
      "loss": 0.1902,
      "step": 18410
    },
    {
      "epoch": 0.5076058201058201,
      "grad_norm": 0.026288777589797974,
      "learning_rate": 9.848434744268078e-06,
      "loss": 0.0017,
      "step": 18420
    },
    {
      "epoch": 0.5078813932980599,
      "grad_norm": 0.012685930356383324,
      "learning_rate": 9.842923280423281e-06,
      "loss": 0.1496,
      "step": 18430
    },
    {
      "epoch": 0.5081569664902998,
      "grad_norm": 0.15294235944747925,
      "learning_rate": 9.837411816578484e-06,
      "loss": 0.2026,
      "step": 18440
    },
    {
      "epoch": 0.5084325396825397,
      "grad_norm": 0.011801257729530334,
      "learning_rate": 9.831900352733686e-06,
      "loss": 0.0799,
      "step": 18450
    },
    {
      "epoch": 0.5087081128747796,
      "grad_norm": 0.012989730574190617,
      "learning_rate": 9.826388888888889e-06,
      "loss": 0.0245,
      "step": 18460
    },
    {
      "epoch": 0.5089836860670194,
      "grad_norm": 0.01976085640490055,
      "learning_rate": 9.820877425044092e-06,
      "loss": 0.0823,
      "step": 18470
    },
    {
      "epoch": 0.5092592592592593,
      "grad_norm": 0.017515676096081734,
      "learning_rate": 9.815365961199296e-06,
      "loss": 0.0152,
      "step": 18480
    },
    {
      "epoch": 0.5095348324514991,
      "grad_norm": 0.18364763259887695,
      "learning_rate": 9.809854497354499e-06,
      "loss": 0.1194,
      "step": 18490
    },
    {
      "epoch": 0.509810405643739,
      "grad_norm": 0.0216380525380373,
      "learning_rate": 9.8043430335097e-06,
      "loss": 0.0788,
      "step": 18500
    },
    {
      "epoch": 0.5100859788359788,
      "grad_norm": 127.14634704589844,
      "learning_rate": 9.798831569664904e-06,
      "loss": 0.1379,
      "step": 18510
    },
    {
      "epoch": 0.5103615520282186,
      "grad_norm": 53.13920211791992,
      "learning_rate": 9.793320105820107e-06,
      "loss": 0.1975,
      "step": 18520
    },
    {
      "epoch": 0.5106371252204586,
      "grad_norm": 0.023983793333172798,
      "learning_rate": 9.78780864197531e-06,
      "loss": 0.0903,
      "step": 18530
    },
    {
      "epoch": 0.5109126984126984,
      "grad_norm": 0.13543292880058289,
      "learning_rate": 9.782297178130512e-06,
      "loss": 0.2341,
      "step": 18540
    },
    {
      "epoch": 0.5111882716049383,
      "grad_norm": 38.69557189941406,
      "learning_rate": 9.776785714285715e-06,
      "loss": 0.0813,
      "step": 18550
    },
    {
      "epoch": 0.5114638447971781,
      "grad_norm": 0.011544082313776016,
      "learning_rate": 9.771274250440918e-06,
      "loss": 0.0399,
      "step": 18560
    },
    {
      "epoch": 0.511739417989418,
      "grad_norm": 0.038031864911317825,
      "learning_rate": 9.76576278659612e-06,
      "loss": 0.0098,
      "step": 18570
    },
    {
      "epoch": 0.5120149911816578,
      "grad_norm": 0.01670764572918415,
      "learning_rate": 9.760251322751325e-06,
      "loss": 0.1606,
      "step": 18580
    },
    {
      "epoch": 0.5122905643738977,
      "grad_norm": 0.013152780942618847,
      "learning_rate": 9.754739858906526e-06,
      "loss": 0.1181,
      "step": 18590
    },
    {
      "epoch": 0.5125661375661376,
      "grad_norm": 0.029038194566965103,
      "learning_rate": 9.74922839506173e-06,
      "loss": 0.0009,
      "step": 18600
    },
    {
      "epoch": 0.5128417107583775,
      "grad_norm": 0.013094404712319374,
      "learning_rate": 9.743716931216933e-06,
      "loss": 0.3159,
      "step": 18610
    },
    {
      "epoch": 0.5131172839506173,
      "grad_norm": 63.76976776123047,
      "learning_rate": 9.738205467372134e-06,
      "loss": 0.0911,
      "step": 18620
    },
    {
      "epoch": 0.5133928571428571,
      "grad_norm": 14.359762191772461,
      "learning_rate": 9.732694003527338e-06,
      "loss": 0.0163,
      "step": 18630
    },
    {
      "epoch": 0.513668430335097,
      "grad_norm": 0.7644507884979248,
      "learning_rate": 9.727182539682541e-06,
      "loss": 0.0851,
      "step": 18640
    },
    {
      "epoch": 0.5139440035273368,
      "grad_norm": 0.015744052827358246,
      "learning_rate": 9.721671075837743e-06,
      "loss": 0.0262,
      "step": 18650
    },
    {
      "epoch": 0.5142195767195767,
      "grad_norm": 0.042551543563604355,
      "learning_rate": 9.716159611992946e-06,
      "loss": 0.0642,
      "step": 18660
    },
    {
      "epoch": 0.5144951499118166,
      "grad_norm": 0.012831817381083965,
      "learning_rate": 9.710648148148149e-06,
      "loss": 0.0009,
      "step": 18670
    },
    {
      "epoch": 0.5147707231040565,
      "grad_norm": 0.011872059665620327,
      "learning_rate": 9.705136684303352e-06,
      "loss": 0.0826,
      "step": 18680
    },
    {
      "epoch": 0.5150462962962963,
      "grad_norm": 0.02893780544400215,
      "learning_rate": 9.699625220458556e-06,
      "loss": 0.0385,
      "step": 18690
    },
    {
      "epoch": 0.5153218694885362,
      "grad_norm": 0.029664576053619385,
      "learning_rate": 9.694113756613757e-06,
      "loss": 0.2777,
      "step": 18700
    },
    {
      "epoch": 0.515597442680776,
      "grad_norm": 0.019703835248947144,
      "learning_rate": 9.68860229276896e-06,
      "loss": 0.0221,
      "step": 18710
    },
    {
      "epoch": 0.5158730158730159,
      "grad_norm": 0.022024236619472504,
      "learning_rate": 9.683090828924164e-06,
      "loss": 0.1124,
      "step": 18720
    },
    {
      "epoch": 0.5161485890652557,
      "grad_norm": 0.012524981051683426,
      "learning_rate": 9.677579365079365e-06,
      "loss": 0.2918,
      "step": 18730
    },
    {
      "epoch": 0.5164241622574955,
      "grad_norm": 0.047427110373973846,
      "learning_rate": 9.672067901234568e-06,
      "loss": 0.0275,
      "step": 18740
    },
    {
      "epoch": 0.5166997354497355,
      "grad_norm": 0.012107769027352333,
      "learning_rate": 9.666556437389772e-06,
      "loss": 0.1157,
      "step": 18750
    },
    {
      "epoch": 0.5169753086419753,
      "grad_norm": 0.02364959940314293,
      "learning_rate": 9.661044973544973e-06,
      "loss": 0.0993,
      "step": 18760
    },
    {
      "epoch": 0.5172508818342152,
      "grad_norm": 56.505638122558594,
      "learning_rate": 9.655533509700177e-06,
      "loss": 0.0176,
      "step": 18770
    },
    {
      "epoch": 0.517526455026455,
      "grad_norm": 21.273117065429688,
      "learning_rate": 9.65002204585538e-06,
      "loss": 0.0023,
      "step": 18780
    },
    {
      "epoch": 0.5178020282186949,
      "grad_norm": 0.018889959901571274,
      "learning_rate": 9.644510582010583e-06,
      "loss": 0.1542,
      "step": 18790
    },
    {
      "epoch": 0.5180776014109347,
      "grad_norm": 0.03553272411227226,
      "learning_rate": 9.638999118165786e-06,
      "loss": 0.1953,
      "step": 18800
    },
    {
      "epoch": 0.5183531746031746,
      "grad_norm": 0.04983438923954964,
      "learning_rate": 9.63348765432099e-06,
      "loss": 0.1646,
      "step": 18810
    },
    {
      "epoch": 0.5186287477954145,
      "grad_norm": 0.013688359409570694,
      "learning_rate": 9.627976190476191e-06,
      "loss": 0.0019,
      "step": 18820
    },
    {
      "epoch": 0.5189043209876543,
      "grad_norm": 0.01669643633067608,
      "learning_rate": 9.622464726631394e-06,
      "loss": 0.0913,
      "step": 18830
    },
    {
      "epoch": 0.5191798941798942,
      "grad_norm": 70.68257141113281,
      "learning_rate": 9.616953262786598e-06,
      "loss": 0.1424,
      "step": 18840
    },
    {
      "epoch": 0.519455467372134,
      "grad_norm": 0.010265989229083061,
      "learning_rate": 9.6114417989418e-06,
      "loss": 0.0914,
      "step": 18850
    },
    {
      "epoch": 0.5197310405643739,
      "grad_norm": 0.0642925351858139,
      "learning_rate": 9.605930335097002e-06,
      "loss": 0.0698,
      "step": 18860
    },
    {
      "epoch": 0.5200066137566137,
      "grad_norm": 0.042365215718746185,
      "learning_rate": 9.600418871252206e-06,
      "loss": 0.0035,
      "step": 18870
    },
    {
      "epoch": 0.5202821869488536,
      "grad_norm": 0.07945515960454941,
      "learning_rate": 9.594907407407407e-06,
      "loss": 0.0009,
      "step": 18880
    },
    {
      "epoch": 0.5205577601410935,
      "grad_norm": 174.8668670654297,
      "learning_rate": 9.58939594356261e-06,
      "loss": 0.0205,
      "step": 18890
    },
    {
      "epoch": 0.5208333333333334,
      "grad_norm": 0.014147181063890457,
      "learning_rate": 9.583884479717814e-06,
      "loss": 0.0533,
      "step": 18900
    },
    {
      "epoch": 0.5211089065255732,
      "grad_norm": 0.011555918492376804,
      "learning_rate": 9.578373015873017e-06,
      "loss": 0.0769,
      "step": 18910
    },
    {
      "epoch": 0.5213844797178131,
      "grad_norm": 0.01139090582728386,
      "learning_rate": 9.57286155202822e-06,
      "loss": 0.206,
      "step": 18920
    },
    {
      "epoch": 0.5216600529100529,
      "grad_norm": 0.022370843216776848,
      "learning_rate": 9.567350088183422e-06,
      "loss": 0.1881,
      "step": 18930
    },
    {
      "epoch": 0.5219356261022927,
      "grad_norm": 152.80348205566406,
      "learning_rate": 9.561838624338625e-06,
      "loss": 0.1452,
      "step": 18940
    },
    {
      "epoch": 0.5222111992945326,
      "grad_norm": 153.70948791503906,
      "learning_rate": 9.556327160493828e-06,
      "loss": 0.1136,
      "step": 18950
    },
    {
      "epoch": 0.5224867724867724,
      "grad_norm": 0.01387120969593525,
      "learning_rate": 9.55081569664903e-06,
      "loss": 0.0076,
      "step": 18960
    },
    {
      "epoch": 0.5227623456790124,
      "grad_norm": 12.789715766906738,
      "learning_rate": 9.545304232804233e-06,
      "loss": 0.1396,
      "step": 18970
    },
    {
      "epoch": 0.5230379188712522,
      "grad_norm": 0.01201759371906519,
      "learning_rate": 9.539792768959436e-06,
      "loss": 0.0135,
      "step": 18980
    },
    {
      "epoch": 0.5233134920634921,
      "grad_norm": 0.12304316461086273,
      "learning_rate": 9.534281305114638e-06,
      "loss": 0.0531,
      "step": 18990
    },
    {
      "epoch": 0.5235890652557319,
      "grad_norm": 0.017076991498470306,
      "learning_rate": 9.528769841269841e-06,
      "loss": 0.0423,
      "step": 19000
    },
    {
      "epoch": 0.5238646384479718,
      "grad_norm": 204.20225524902344,
      "learning_rate": 9.523258377425045e-06,
      "loss": 0.1134,
      "step": 19010
    },
    {
      "epoch": 0.5241402116402116,
      "grad_norm": 0.016961265355348587,
      "learning_rate": 9.517746913580248e-06,
      "loss": 0.0389,
      "step": 19020
    },
    {
      "epoch": 0.5244157848324515,
      "grad_norm": 1.910343885421753,
      "learning_rate": 9.512235449735451e-06,
      "loss": 0.0011,
      "step": 19030
    },
    {
      "epoch": 0.5246913580246914,
      "grad_norm": 0.01612863503396511,
      "learning_rate": 9.506723985890653e-06,
      "loss": 0.0644,
      "step": 19040
    },
    {
      "epoch": 0.5249669312169312,
      "grad_norm": 0.6721423864364624,
      "learning_rate": 9.501212522045856e-06,
      "loss": 0.1146,
      "step": 19050
    },
    {
      "epoch": 0.5252425044091711,
      "grad_norm": 0.01504079531878233,
      "learning_rate": 9.49570105820106e-06,
      "loss": 0.0011,
      "step": 19060
    },
    {
      "epoch": 0.5255180776014109,
      "grad_norm": 0.07511967420578003,
      "learning_rate": 9.490189594356262e-06,
      "loss": 0.0988,
      "step": 19070
    },
    {
      "epoch": 0.5257936507936508,
      "grad_norm": 0.02907981351017952,
      "learning_rate": 9.484678130511464e-06,
      "loss": 0.1218,
      "step": 19080
    },
    {
      "epoch": 0.5260692239858906,
      "grad_norm": 0.015196695923805237,
      "learning_rate": 9.479166666666667e-06,
      "loss": 0.0584,
      "step": 19090
    },
    {
      "epoch": 0.5263447971781305,
      "grad_norm": 0.010557588189840317,
      "learning_rate": 9.47365520282187e-06,
      "loss": 0.2047,
      "step": 19100
    },
    {
      "epoch": 0.5266203703703703,
      "grad_norm": 12.99949836730957,
      "learning_rate": 9.468143738977074e-06,
      "loss": 0.1413,
      "step": 19110
    },
    {
      "epoch": 0.5268959435626103,
      "grad_norm": 0.019717330113053322,
      "learning_rate": 9.462632275132277e-06,
      "loss": 0.1778,
      "step": 19120
    },
    {
      "epoch": 0.5271715167548501,
      "grad_norm": 0.055843647569417953,
      "learning_rate": 9.457120811287479e-06,
      "loss": 0.1125,
      "step": 19130
    },
    {
      "epoch": 0.52744708994709,
      "grad_norm": 0.02551831305027008,
      "learning_rate": 9.451609347442682e-06,
      "loss": 0.0213,
      "step": 19140
    },
    {
      "epoch": 0.5277226631393298,
      "grad_norm": 0.012317877262830734,
      "learning_rate": 9.446097883597885e-06,
      "loss": 0.0009,
      "step": 19150
    },
    {
      "epoch": 0.5279982363315696,
      "grad_norm": 0.11799465864896774,
      "learning_rate": 9.440586419753087e-06,
      "loss": 0.0022,
      "step": 19160
    },
    {
      "epoch": 0.5282738095238095,
      "grad_norm": 0.04863578453660011,
      "learning_rate": 9.43507495590829e-06,
      "loss": 0.1139,
      "step": 19170
    },
    {
      "epoch": 0.5285493827160493,
      "grad_norm": 0.01590290665626526,
      "learning_rate": 9.429563492063493e-06,
      "loss": 0.0926,
      "step": 19180
    },
    {
      "epoch": 0.5288249559082893,
      "grad_norm": 0.9531297087669373,
      "learning_rate": 9.424052028218695e-06,
      "loss": 0.0626,
      "step": 19190
    },
    {
      "epoch": 0.5291005291005291,
      "grad_norm": 31.547439575195312,
      "learning_rate": 9.418540564373898e-06,
      "loss": 0.0244,
      "step": 19200
    },
    {
      "epoch": 0.529376102292769,
      "grad_norm": 152.66983032226562,
      "learning_rate": 9.413029100529101e-06,
      "loss": 0.1099,
      "step": 19210
    },
    {
      "epoch": 0.5296516754850088,
      "grad_norm": 0.02259817346930504,
      "learning_rate": 9.407517636684305e-06,
      "loss": 0.0009,
      "step": 19220
    },
    {
      "epoch": 0.5299272486772487,
      "grad_norm": 0.042294230312108994,
      "learning_rate": 9.402006172839508e-06,
      "loss": 0.2045,
      "step": 19230
    },
    {
      "epoch": 0.5302028218694885,
      "grad_norm": 0.010940332897007465,
      "learning_rate": 9.39649470899471e-06,
      "loss": 0.0074,
      "step": 19240
    },
    {
      "epoch": 0.5304783950617284,
      "grad_norm": 0.010162868537008762,
      "learning_rate": 9.390983245149913e-06,
      "loss": 0.0792,
      "step": 19250
    },
    {
      "epoch": 0.5307539682539683,
      "grad_norm": 0.014909133315086365,
      "learning_rate": 9.385471781305116e-06,
      "loss": 0.0009,
      "step": 19260
    },
    {
      "epoch": 0.5310295414462081,
      "grad_norm": 0.01801157183945179,
      "learning_rate": 9.379960317460317e-06,
      "loss": 0.0671,
      "step": 19270
    },
    {
      "epoch": 0.531305114638448,
      "grad_norm": 0.48822182416915894,
      "learning_rate": 9.37444885361552e-06,
      "loss": 0.0731,
      "step": 19280
    },
    {
      "epoch": 0.5315806878306878,
      "grad_norm": 0.025763360783457756,
      "learning_rate": 9.368937389770724e-06,
      "loss": 0.1037,
      "step": 19290
    },
    {
      "epoch": 0.5318562610229277,
      "grad_norm": 89.86502838134766,
      "learning_rate": 9.363425925925927e-06,
      "loss": 0.0804,
      "step": 19300
    },
    {
      "epoch": 0.5321318342151675,
      "grad_norm": 0.010308676399290562,
      "learning_rate": 9.357914462081129e-06,
      "loss": 0.2316,
      "step": 19310
    },
    {
      "epoch": 0.5324074074074074,
      "grad_norm": 162.11158752441406,
      "learning_rate": 9.352402998236332e-06,
      "loss": 0.1634,
      "step": 19320
    },
    {
      "epoch": 0.5326829805996472,
      "grad_norm": 0.01672583445906639,
      "learning_rate": 9.346891534391535e-06,
      "loss": 0.002,
      "step": 19330
    },
    {
      "epoch": 0.5329585537918872,
      "grad_norm": 0.024724742397665977,
      "learning_rate": 9.341380070546739e-06,
      "loss": 0.149,
      "step": 19340
    },
    {
      "epoch": 0.533234126984127,
      "grad_norm": 0.17850910127162933,
      "learning_rate": 9.335868606701942e-06,
      "loss": 0.0087,
      "step": 19350
    },
    {
      "epoch": 0.5335097001763669,
      "grad_norm": 0.03665535897016525,
      "learning_rate": 9.330357142857143e-06,
      "loss": 0.113,
      "step": 19360
    },
    {
      "epoch": 0.5337852733686067,
      "grad_norm": 0.1468227207660675,
      "learning_rate": 9.324845679012347e-06,
      "loss": 0.0585,
      "step": 19370
    },
    {
      "epoch": 0.5340608465608465,
      "grad_norm": 36.5416259765625,
      "learning_rate": 9.31933421516755e-06,
      "loss": 0.1656,
      "step": 19380
    },
    {
      "epoch": 0.5343364197530864,
      "grad_norm": 0.02194368466734886,
      "learning_rate": 9.313822751322751e-06,
      "loss": 0.1387,
      "step": 19390
    },
    {
      "epoch": 0.5346119929453262,
      "grad_norm": 4.039468765258789,
      "learning_rate": 9.308311287477955e-06,
      "loss": 0.0804,
      "step": 19400
    },
    {
      "epoch": 0.5348875661375662,
      "grad_norm": 0.034098368138074875,
      "learning_rate": 9.302799823633158e-06,
      "loss": 0.3856,
      "step": 19410
    },
    {
      "epoch": 0.535163139329806,
      "grad_norm": 0.033479709178209305,
      "learning_rate": 9.29728835978836e-06,
      "loss": 0.1675,
      "step": 19420
    },
    {
      "epoch": 0.5354387125220459,
      "grad_norm": 0.010844340547919273,
      "learning_rate": 9.291776895943564e-06,
      "loss": 0.113,
      "step": 19430
    },
    {
      "epoch": 0.5357142857142857,
      "grad_norm": 0.012367594055831432,
      "learning_rate": 9.286265432098766e-06,
      "loss": 0.1765,
      "step": 19440
    },
    {
      "epoch": 0.5359898589065256,
      "grad_norm": 0.017102420330047607,
      "learning_rate": 9.28075396825397e-06,
      "loss": 0.0788,
      "step": 19450
    },
    {
      "epoch": 0.5362654320987654,
      "grad_norm": 33.394901275634766,
      "learning_rate": 9.275242504409173e-06,
      "loss": 0.1207,
      "step": 19460
    },
    {
      "epoch": 0.5365410052910053,
      "grad_norm": 121.81251525878906,
      "learning_rate": 9.269731040564374e-06,
      "loss": 0.0452,
      "step": 19470
    },
    {
      "epoch": 0.5368165784832452,
      "grad_norm": 0.011115931905806065,
      "learning_rate": 9.264219576719577e-06,
      "loss": 0.0752,
      "step": 19480
    },
    {
      "epoch": 0.537092151675485,
      "grad_norm": 97.36014556884766,
      "learning_rate": 9.25870811287478e-06,
      "loss": 0.155,
      "step": 19490
    },
    {
      "epoch": 0.5373677248677249,
      "grad_norm": 0.36932528018951416,
      "learning_rate": 9.253196649029982e-06,
      "loss": 0.0523,
      "step": 19500
    },
    {
      "epoch": 0.5376432980599647,
      "grad_norm": 0.011683693155646324,
      "learning_rate": 9.247685185185185e-06,
      "loss": 0.001,
      "step": 19510
    },
    {
      "epoch": 0.5379188712522046,
      "grad_norm": 2.7482895851135254,
      "learning_rate": 9.242173721340389e-06,
      "loss": 0.0833,
      "step": 19520
    },
    {
      "epoch": 0.5381944444444444,
      "grad_norm": 0.03269468992948532,
      "learning_rate": 9.23666225749559e-06,
      "loss": 0.067,
      "step": 19530
    },
    {
      "epoch": 0.5384700176366843,
      "grad_norm": 0.05016826465725899,
      "learning_rate": 9.231150793650795e-06,
      "loss": 0.1413,
      "step": 19540
    },
    {
      "epoch": 0.5387455908289241,
      "grad_norm": 0.014317275956273079,
      "learning_rate": 9.225639329805997e-06,
      "loss": 0.238,
      "step": 19550
    },
    {
      "epoch": 0.5390211640211641,
      "grad_norm": 0.41233938932418823,
      "learning_rate": 9.2201278659612e-06,
      "loss": 0.0328,
      "step": 19560
    },
    {
      "epoch": 0.5392967372134039,
      "grad_norm": 0.024514470249414444,
      "learning_rate": 9.214616402116403e-06,
      "loss": 0.256,
      "step": 19570
    },
    {
      "epoch": 0.5395723104056437,
      "grad_norm": 36.141571044921875,
      "learning_rate": 9.209104938271607e-06,
      "loss": 0.0918,
      "step": 19580
    },
    {
      "epoch": 0.5398478835978836,
      "grad_norm": 72.76187896728516,
      "learning_rate": 9.203593474426808e-06,
      "loss": 0.0939,
      "step": 19590
    },
    {
      "epoch": 0.5401234567901234,
      "grad_norm": 0.18551959097385406,
      "learning_rate": 9.198082010582011e-06,
      "loss": 0.0411,
      "step": 19600
    },
    {
      "epoch": 0.5403990299823633,
      "grad_norm": 1.4836128950119019,
      "learning_rate": 9.192570546737215e-06,
      "loss": 0.0984,
      "step": 19610
    },
    {
      "epoch": 0.5406746031746031,
      "grad_norm": 0.03238728642463684,
      "learning_rate": 9.187059082892416e-06,
      "loss": 0.001,
      "step": 19620
    },
    {
      "epoch": 0.5409501763668431,
      "grad_norm": 0.013528983108699322,
      "learning_rate": 9.18154761904762e-06,
      "loss": 0.044,
      "step": 19630
    },
    {
      "epoch": 0.5412257495590829,
      "grad_norm": 0.6707661151885986,
      "learning_rate": 9.176036155202823e-06,
      "loss": 0.1324,
      "step": 19640
    },
    {
      "epoch": 0.5415013227513228,
      "grad_norm": 0.018473267555236816,
      "learning_rate": 9.170524691358026e-06,
      "loss": 0.0709,
      "step": 19650
    },
    {
      "epoch": 0.5417768959435626,
      "grad_norm": 162.15890502929688,
      "learning_rate": 9.16501322751323e-06,
      "loss": 0.1897,
      "step": 19660
    },
    {
      "epoch": 0.5420524691358025,
      "grad_norm": 0.012259883806109428,
      "learning_rate": 9.15950176366843e-06,
      "loss": 0.0021,
      "step": 19670
    },
    {
      "epoch": 0.5423280423280423,
      "grad_norm": 0.02225564979016781,
      "learning_rate": 9.153990299823634e-06,
      "loss": 0.022,
      "step": 19680
    },
    {
      "epoch": 0.5426036155202821,
      "grad_norm": 0.014732646755874157,
      "learning_rate": 9.148478835978837e-06,
      "loss": 0.1171,
      "step": 19690
    },
    {
      "epoch": 0.542879188712522,
      "grad_norm": 0.020659547299146652,
      "learning_rate": 9.142967372134039e-06,
      "loss": 0.0047,
      "step": 19700
    },
    {
      "epoch": 0.5431547619047619,
      "grad_norm": 53.028564453125,
      "learning_rate": 9.137455908289242e-06,
      "loss": 0.1772,
      "step": 19710
    },
    {
      "epoch": 0.5434303350970018,
      "grad_norm": 0.1524023860692978,
      "learning_rate": 9.131944444444445e-06,
      "loss": 0.0939,
      "step": 19720
    },
    {
      "epoch": 0.5437059082892416,
      "grad_norm": 133.52328491210938,
      "learning_rate": 9.126432980599647e-06,
      "loss": 0.1391,
      "step": 19730
    },
    {
      "epoch": 0.5439814814814815,
      "grad_norm": 0.01031127106398344,
      "learning_rate": 9.12092151675485e-06,
      "loss": 0.0009,
      "step": 19740
    },
    {
      "epoch": 0.5442570546737213,
      "grad_norm": 0.009796367026865482,
      "learning_rate": 9.115410052910054e-06,
      "loss": 0.0295,
      "step": 19750
    },
    {
      "epoch": 0.5445326278659612,
      "grad_norm": 0.014294076710939407,
      "learning_rate": 9.109898589065257e-06,
      "loss": 0.1044,
      "step": 19760
    },
    {
      "epoch": 0.544808201058201,
      "grad_norm": 183.73117065429688,
      "learning_rate": 9.10438712522046e-06,
      "loss": 0.1982,
      "step": 19770
    },
    {
      "epoch": 0.545083774250441,
      "grad_norm": 0.023337962105870247,
      "learning_rate": 9.098875661375662e-06,
      "loss": 0.1392,
      "step": 19780
    },
    {
      "epoch": 0.5453593474426808,
      "grad_norm": 0.010350153781473637,
      "learning_rate": 9.093364197530865e-06,
      "loss": 0.0856,
      "step": 19790
    },
    {
      "epoch": 0.5456349206349206,
      "grad_norm": 0.08635164797306061,
      "learning_rate": 9.087852733686068e-06,
      "loss": 0.0122,
      "step": 19800
    },
    {
      "epoch": 0.5459104938271605,
      "grad_norm": 0.9689809083938599,
      "learning_rate": 9.08234126984127e-06,
      "loss": 0.0468,
      "step": 19810
    },
    {
      "epoch": 0.5461860670194003,
      "grad_norm": 0.012831032276153564,
      "learning_rate": 9.076829805996473e-06,
      "loss": 0.0874,
      "step": 19820
    },
    {
      "epoch": 0.5464616402116402,
      "grad_norm": 0.012570139020681381,
      "learning_rate": 9.071318342151676e-06,
      "loss": 0.1294,
      "step": 19830
    },
    {
      "epoch": 0.54673721340388,
      "grad_norm": 0.028377715498209,
      "learning_rate": 9.06580687830688e-06,
      "loss": 0.0527,
      "step": 19840
    },
    {
      "epoch": 0.54701278659612,
      "grad_norm": 0.009991479106247425,
      "learning_rate": 9.060295414462081e-06,
      "loss": 0.0552,
      "step": 19850
    },
    {
      "epoch": 0.5472883597883598,
      "grad_norm": 34.655540466308594,
      "learning_rate": 9.054783950617286e-06,
      "loss": 0.3283,
      "step": 19860
    },
    {
      "epoch": 0.5475639329805997,
      "grad_norm": 0.047437530010938644,
      "learning_rate": 9.049272486772488e-06,
      "loss": 0.0014,
      "step": 19870
    },
    {
      "epoch": 0.5478395061728395,
      "grad_norm": 0.012184365652501583,
      "learning_rate": 9.04376102292769e-06,
      "loss": 0.094,
      "step": 19880
    },
    {
      "epoch": 0.5481150793650794,
      "grad_norm": 0.011902790516614914,
      "learning_rate": 9.038249559082894e-06,
      "loss": 0.0073,
      "step": 19890
    },
    {
      "epoch": 0.5483906525573192,
      "grad_norm": 6.092496871948242,
      "learning_rate": 9.032738095238096e-06,
      "loss": 0.0012,
      "step": 19900
    },
    {
      "epoch": 0.548666225749559,
      "grad_norm": 195.18080139160156,
      "learning_rate": 9.027226631393299e-06,
      "loss": 0.1053,
      "step": 19910
    },
    {
      "epoch": 0.548941798941799,
      "grad_norm": 0.01762690395116806,
      "learning_rate": 9.021715167548502e-06,
      "loss": 0.0757,
      "step": 19920
    },
    {
      "epoch": 0.5492173721340388,
      "grad_norm": 0.6712008714675903,
      "learning_rate": 9.016203703703704e-06,
      "loss": 0.1348,
      "step": 19930
    },
    {
      "epoch": 0.5494929453262787,
      "grad_norm": 0.42045965790748596,
      "learning_rate": 9.010692239858907e-06,
      "loss": 0.1498,
      "step": 19940
    },
    {
      "epoch": 0.5497685185185185,
      "grad_norm": 0.0452093742787838,
      "learning_rate": 9.00518077601411e-06,
      "loss": 0.144,
      "step": 19950
    },
    {
      "epoch": 0.5500440917107584,
      "grad_norm": 87.92577362060547,
      "learning_rate": 8.999669312169312e-06,
      "loss": 0.1934,
      "step": 19960
    },
    {
      "epoch": 0.5503196649029982,
      "grad_norm": 0.021447008475661278,
      "learning_rate": 8.994157848324517e-06,
      "loss": 0.1501,
      "step": 19970
    },
    {
      "epoch": 0.5505952380952381,
      "grad_norm": 13.263294219970703,
      "learning_rate": 8.988646384479718e-06,
      "loss": 0.162,
      "step": 19980
    },
    {
      "epoch": 0.550870811287478,
      "grad_norm": 141.54324340820312,
      "learning_rate": 8.983134920634922e-06,
      "loss": 0.1123,
      "step": 19990
    },
    {
      "epoch": 0.5511463844797179,
      "grad_norm": 0.07097070664167404,
      "learning_rate": 8.977623456790125e-06,
      "loss": 0.0501,
      "step": 20000
    },
    {
      "epoch": 0.5514219576719577,
      "grad_norm": 0.013887351378798485,
      "learning_rate": 8.972111992945326e-06,
      "loss": 0.0025,
      "step": 20010
    },
    {
      "epoch": 0.5516975308641975,
      "grad_norm": 0.03432227298617363,
      "learning_rate": 8.96660052910053e-06,
      "loss": 0.0753,
      "step": 20020
    },
    {
      "epoch": 0.5519731040564374,
      "grad_norm": 0.021074997261166573,
      "learning_rate": 8.961089065255733e-06,
      "loss": 0.1188,
      "step": 20030
    },
    {
      "epoch": 0.5522486772486772,
      "grad_norm": 0.02163068950176239,
      "learning_rate": 8.955577601410934e-06,
      "loss": 0.0685,
      "step": 20040
    },
    {
      "epoch": 0.5525242504409171,
      "grad_norm": 176.98016357421875,
      "learning_rate": 8.950066137566138e-06,
      "loss": 0.0181,
      "step": 20050
    },
    {
      "epoch": 0.5527998236331569,
      "grad_norm": 0.0116268927231431,
      "learning_rate": 8.944554673721341e-06,
      "loss": 0.0956,
      "step": 20060
    },
    {
      "epoch": 0.5530753968253969,
      "grad_norm": 0.025607459247112274,
      "learning_rate": 8.939043209876544e-06,
      "loss": 0.3222,
      "step": 20070
    },
    {
      "epoch": 0.5533509700176367,
      "grad_norm": 0.01750165969133377,
      "learning_rate": 8.933531746031747e-06,
      "loss": 0.2017,
      "step": 20080
    },
    {
      "epoch": 0.5536265432098766,
      "grad_norm": 3.0286331176757812,
      "learning_rate": 8.92802028218695e-06,
      "loss": 0.005,
      "step": 20090
    },
    {
      "epoch": 0.5539021164021164,
      "grad_norm": 0.011715985834598541,
      "learning_rate": 8.922508818342152e-06,
      "loss": 0.0427,
      "step": 20100
    },
    {
      "epoch": 0.5541776895943563,
      "grad_norm": 0.02143857628107071,
      "learning_rate": 8.916997354497356e-06,
      "loss": 0.0688,
      "step": 20110
    },
    {
      "epoch": 0.5544532627865961,
      "grad_norm": 126.96844482421875,
      "learning_rate": 8.911485890652559e-06,
      "loss": 0.2384,
      "step": 20120
    },
    {
      "epoch": 0.5547288359788359,
      "grad_norm": 0.00910030398517847,
      "learning_rate": 8.90597442680776e-06,
      "loss": 0.1274,
      "step": 20130
    },
    {
      "epoch": 0.5550044091710759,
      "grad_norm": 2.486268997192383,
      "learning_rate": 8.900462962962964e-06,
      "loss": 0.0016,
      "step": 20140
    },
    {
      "epoch": 0.5552799823633157,
      "grad_norm": 148.3619384765625,
      "learning_rate": 8.894951499118167e-06,
      "loss": 0.2126,
      "step": 20150
    },
    {
      "epoch": 0.5555555555555556,
      "grad_norm": 0.15700489282608032,
      "learning_rate": 8.889440035273368e-06,
      "loss": 0.0442,
      "step": 20160
    },
    {
      "epoch": 0.5558311287477954,
      "grad_norm": 0.020319076254963875,
      "learning_rate": 8.883928571428572e-06,
      "loss": 0.0007,
      "step": 20170
    },
    {
      "epoch": 0.5561067019400353,
      "grad_norm": 0.6890736222267151,
      "learning_rate": 8.878417107583775e-06,
      "loss": 0.0285,
      "step": 20180
    },
    {
      "epoch": 0.5563822751322751,
      "grad_norm": 0.025320105254650116,
      "learning_rate": 8.872905643738978e-06,
      "loss": 0.2062,
      "step": 20190
    },
    {
      "epoch": 0.556657848324515,
      "grad_norm": 0.016779368743300438,
      "learning_rate": 8.867394179894182e-06,
      "loss": 0.0473,
      "step": 20200
    },
    {
      "epoch": 0.5569334215167548,
      "grad_norm": 0.05094359815120697,
      "learning_rate": 8.861882716049383e-06,
      "loss": 0.1758,
      "step": 20210
    },
    {
      "epoch": 0.5572089947089947,
      "grad_norm": 0.6360114812850952,
      "learning_rate": 8.856371252204586e-06,
      "loss": 0.1066,
      "step": 20220
    },
    {
      "epoch": 0.5574845679012346,
      "grad_norm": 0.03827017545700073,
      "learning_rate": 8.85085978835979e-06,
      "loss": 0.028,
      "step": 20230
    },
    {
      "epoch": 0.5577601410934744,
      "grad_norm": 1.5189449787139893,
      "learning_rate": 8.845348324514991e-06,
      "loss": 0.025,
      "step": 20240
    },
    {
      "epoch": 0.5580357142857143,
      "grad_norm": 139.54652404785156,
      "learning_rate": 8.839836860670194e-06,
      "loss": 0.023,
      "step": 20250
    },
    {
      "epoch": 0.5583112874779541,
      "grad_norm": 0.01873476430773735,
      "learning_rate": 8.834325396825398e-06,
      "loss": 0.1529,
      "step": 20260
    },
    {
      "epoch": 0.558586860670194,
      "grad_norm": 0.013670297339558601,
      "learning_rate": 8.8288139329806e-06,
      "loss": 0.1265,
      "step": 20270
    },
    {
      "epoch": 0.5588624338624338,
      "grad_norm": 0.03909332677721977,
      "learning_rate": 8.823302469135803e-06,
      "loss": 0.041,
      "step": 20280
    },
    {
      "epoch": 0.5591380070546738,
      "grad_norm": 0.009802119806408882,
      "learning_rate": 8.817791005291006e-06,
      "loss": 0.1315,
      "step": 20290
    },
    {
      "epoch": 0.5594135802469136,
      "grad_norm": 0.6827573180198669,
      "learning_rate": 8.812279541446209e-06,
      "loss": 0.0037,
      "step": 20300
    },
    {
      "epoch": 0.5596891534391535,
      "grad_norm": 0.7040966749191284,
      "learning_rate": 8.806768077601412e-06,
      "loss": 0.077,
      "step": 20310
    },
    {
      "epoch": 0.5599647266313933,
      "grad_norm": 0.010371766984462738,
      "learning_rate": 8.801256613756614e-06,
      "loss": 0.0655,
      "step": 20320
    },
    {
      "epoch": 0.5602402998236331,
      "grad_norm": 0.44152867794036865,
      "learning_rate": 8.795745149911817e-06,
      "loss": 0.1632,
      "step": 20330
    },
    {
      "epoch": 0.560515873015873,
      "grad_norm": 0.019602062180638313,
      "learning_rate": 8.79023368606702e-06,
      "loss": 0.0691,
      "step": 20340
    },
    {
      "epoch": 0.5607914462081128,
      "grad_norm": 0.02511991187930107,
      "learning_rate": 8.784722222222224e-06,
      "loss": 0.0168,
      "step": 20350
    },
    {
      "epoch": 0.5610670194003528,
      "grad_norm": 150.08453369140625,
      "learning_rate": 8.779210758377425e-06,
      "loss": 0.2409,
      "step": 20360
    },
    {
      "epoch": 0.5613425925925926,
      "grad_norm": 86.88274383544922,
      "learning_rate": 8.773699294532628e-06,
      "loss": 0.0514,
      "step": 20370
    },
    {
      "epoch": 0.5616181657848325,
      "grad_norm": 0.009289857000112534,
      "learning_rate": 8.768187830687832e-06,
      "loss": 0.0765,
      "step": 20380
    },
    {
      "epoch": 0.5618937389770723,
      "grad_norm": 0.3394201099872589,
      "learning_rate": 8.762676366843033e-06,
      "loss": 0.3025,
      "step": 20390
    },
    {
      "epoch": 0.5621693121693122,
      "grad_norm": 0.4427143633365631,
      "learning_rate": 8.757164902998238e-06,
      "loss": 0.001,
      "step": 20400
    },
    {
      "epoch": 0.562444885361552,
      "grad_norm": 0.25281935930252075,
      "learning_rate": 8.75165343915344e-06,
      "loss": 0.0063,
      "step": 20410
    },
    {
      "epoch": 0.5627204585537919,
      "grad_norm": 0.027992136776447296,
      "learning_rate": 8.746141975308643e-06,
      "loss": 0.0014,
      "step": 20420
    },
    {
      "epoch": 0.5629960317460317,
      "grad_norm": 0.014118154533207417,
      "learning_rate": 8.740630511463846e-06,
      "loss": 0.0727,
      "step": 20430
    },
    {
      "epoch": 0.5632716049382716,
      "grad_norm": 0.0190273467451334,
      "learning_rate": 8.735119047619048e-06,
      "loss": 0.3482,
      "step": 20440
    },
    {
      "epoch": 0.5635471781305115,
      "grad_norm": 0.046992380172014236,
      "learning_rate": 8.729607583774251e-06,
      "loss": 0.0917,
      "step": 20450
    },
    {
      "epoch": 0.5638227513227513,
      "grad_norm": 1.729168176651001,
      "learning_rate": 8.724096119929454e-06,
      "loss": 0.0883,
      "step": 20460
    },
    {
      "epoch": 0.5640983245149912,
      "grad_norm": 0.014809142798185349,
      "learning_rate": 8.718584656084656e-06,
      "loss": 0.0009,
      "step": 20470
    },
    {
      "epoch": 0.564373897707231,
      "grad_norm": 0.09498964250087738,
      "learning_rate": 8.71307319223986e-06,
      "loss": 0.0497,
      "step": 20480
    },
    {
      "epoch": 0.5646494708994709,
      "grad_norm": 0.030930228531360626,
      "learning_rate": 8.707561728395062e-06,
      "loss": 0.001,
      "step": 20490
    },
    {
      "epoch": 0.5649250440917107,
      "grad_norm": 0.009642950259149075,
      "learning_rate": 8.702050264550266e-06,
      "loss": 0.0008,
      "step": 20500
    },
    {
      "epoch": 0.5652006172839507,
      "grad_norm": 1.5400511026382446,
      "learning_rate": 8.696538800705469e-06,
      "loss": 0.1396,
      "step": 20510
    },
    {
      "epoch": 0.5654761904761905,
      "grad_norm": 0.22063641250133514,
      "learning_rate": 8.69102733686067e-06,
      "loss": 0.0875,
      "step": 20520
    },
    {
      "epoch": 0.5657517636684304,
      "grad_norm": 127.04241180419922,
      "learning_rate": 8.685515873015874e-06,
      "loss": 0.0656,
      "step": 20530
    },
    {
      "epoch": 0.5660273368606702,
      "grad_norm": 0.010962818749248981,
      "learning_rate": 8.680004409171077e-06,
      "loss": 0.1882,
      "step": 20540
    },
    {
      "epoch": 0.56630291005291,
      "grad_norm": 0.01128082163631916,
      "learning_rate": 8.674492945326279e-06,
      "loss": 0.2862,
      "step": 20550
    },
    {
      "epoch": 0.5665784832451499,
      "grad_norm": 71.14220428466797,
      "learning_rate": 8.668981481481482e-06,
      "loss": 0.047,
      "step": 20560
    },
    {
      "epoch": 0.5668540564373897,
      "grad_norm": 0.01701001450419426,
      "learning_rate": 8.663470017636685e-06,
      "loss": 0.0596,
      "step": 20570
    },
    {
      "epoch": 0.5671296296296297,
      "grad_norm": 0.039532460272312164,
      "learning_rate": 8.657958553791887e-06,
      "loss": 0.0731,
      "step": 20580
    },
    {
      "epoch": 0.5674052028218695,
      "grad_norm": 0.01704023964703083,
      "learning_rate": 8.65244708994709e-06,
      "loss": 0.0046,
      "step": 20590
    },
    {
      "epoch": 0.5676807760141094,
      "grad_norm": 0.09261646121740341,
      "learning_rate": 8.646935626102293e-06,
      "loss": 0.001,
      "step": 20600
    },
    {
      "epoch": 0.5679563492063492,
      "grad_norm": 0.014044291339814663,
      "learning_rate": 8.641424162257496e-06,
      "loss": 0.0146,
      "step": 20610
    },
    {
      "epoch": 0.5682319223985891,
      "grad_norm": 1.009135365486145,
      "learning_rate": 8.6359126984127e-06,
      "loss": 0.087,
      "step": 20620
    },
    {
      "epoch": 0.5685074955908289,
      "grad_norm": 0.02176576666533947,
      "learning_rate": 8.630401234567903e-06,
      "loss": 0.0975,
      "step": 20630
    },
    {
      "epoch": 0.5687830687830688,
      "grad_norm": 138.1335906982422,
      "learning_rate": 8.624889770723105e-06,
      "loss": 0.2064,
      "step": 20640
    },
    {
      "epoch": 0.5690586419753086,
      "grad_norm": 57.79267120361328,
      "learning_rate": 8.619378306878308e-06,
      "loss": 0.0057,
      "step": 20650
    },
    {
      "epoch": 0.5693342151675485,
      "grad_norm": 7.9660325050354,
      "learning_rate": 8.613866843033511e-06,
      "loss": 0.0364,
      "step": 20660
    },
    {
      "epoch": 0.5696097883597884,
      "grad_norm": 120.11942291259766,
      "learning_rate": 8.608355379188713e-06,
      "loss": 0.1367,
      "step": 20670
    },
    {
      "epoch": 0.5698853615520282,
      "grad_norm": 0.012719163671135902,
      "learning_rate": 8.602843915343916e-06,
      "loss": 0.004,
      "step": 20680
    },
    {
      "epoch": 0.5701609347442681,
      "grad_norm": 0.01463082805275917,
      "learning_rate": 8.59733245149912e-06,
      "loss": 0.041,
      "step": 20690
    },
    {
      "epoch": 0.5704365079365079,
      "grad_norm": 226.79566955566406,
      "learning_rate": 8.59182098765432e-06,
      "loss": 0.2943,
      "step": 20700
    },
    {
      "epoch": 0.5707120811287478,
      "grad_norm": 0.019782831892371178,
      "learning_rate": 8.586309523809524e-06,
      "loss": 0.0441,
      "step": 20710
    },
    {
      "epoch": 0.5709876543209876,
      "grad_norm": 107.64851379394531,
      "learning_rate": 8.580798059964727e-06,
      "loss": 0.1362,
      "step": 20720
    },
    {
      "epoch": 0.5712632275132276,
      "grad_norm": 0.05347915366292,
      "learning_rate": 8.57528659611993e-06,
      "loss": 0.0813,
      "step": 20730
    },
    {
      "epoch": 0.5715388007054674,
      "grad_norm": 23.158849716186523,
      "learning_rate": 8.569775132275134e-06,
      "loss": 0.1578,
      "step": 20740
    },
    {
      "epoch": 0.5718143738977073,
      "grad_norm": 0.04498138651251793,
      "learning_rate": 8.564263668430335e-06,
      "loss": 0.0481,
      "step": 20750
    },
    {
      "epoch": 0.5720899470899471,
      "grad_norm": 0.011000435799360275,
      "learning_rate": 8.558752204585539e-06,
      "loss": 0.1225,
      "step": 20760
    },
    {
      "epoch": 0.5723655202821869,
      "grad_norm": 0.9134964346885681,
      "learning_rate": 8.553240740740742e-06,
      "loss": 0.1633,
      "step": 20770
    },
    {
      "epoch": 0.5726410934744268,
      "grad_norm": 14.269510269165039,
      "learning_rate": 8.547729276895943e-06,
      "loss": 0.0027,
      "step": 20780
    },
    {
      "epoch": 0.5729166666666666,
      "grad_norm": 9.267882347106934,
      "learning_rate": 8.542217813051147e-06,
      "loss": 0.0991,
      "step": 20790
    },
    {
      "epoch": 0.5731922398589065,
      "grad_norm": 0.8976588845252991,
      "learning_rate": 8.53670634920635e-06,
      "loss": 0.1004,
      "step": 20800
    },
    {
      "epoch": 0.5734678130511464,
      "grad_norm": 0.03237764909863472,
      "learning_rate": 8.531194885361551e-06,
      "loss": 0.0119,
      "step": 20810
    },
    {
      "epoch": 0.5737433862433863,
      "grad_norm": 121.45204162597656,
      "learning_rate": 8.525683421516756e-06,
      "loss": 0.2411,
      "step": 20820
    },
    {
      "epoch": 0.5740189594356261,
      "grad_norm": 0.02717507630586624,
      "learning_rate": 8.520171957671958e-06,
      "loss": 0.0758,
      "step": 20830
    },
    {
      "epoch": 0.574294532627866,
      "grad_norm": 5.822948932647705,
      "learning_rate": 8.514660493827161e-06,
      "loss": 0.0195,
      "step": 20840
    },
    {
      "epoch": 0.5745701058201058,
      "grad_norm": 0.012029963545501232,
      "learning_rate": 8.509149029982365e-06,
      "loss": 0.0034,
      "step": 20850
    },
    {
      "epoch": 0.5748456790123457,
      "grad_norm": 0.027661046013236046,
      "learning_rate": 8.503637566137568e-06,
      "loss": 0.0856,
      "step": 20860
    },
    {
      "epoch": 0.5751212522045855,
      "grad_norm": 0.01876436360180378,
      "learning_rate": 8.49812610229277e-06,
      "loss": 0.1317,
      "step": 20870
    },
    {
      "epoch": 0.5753968253968254,
      "grad_norm": 0.019468355923891068,
      "learning_rate": 8.492614638447973e-06,
      "loss": 0.0009,
      "step": 20880
    },
    {
      "epoch": 0.5756723985890653,
      "grad_norm": 72.9722671508789,
      "learning_rate": 8.487103174603176e-06,
      "loss": 0.2494,
      "step": 20890
    },
    {
      "epoch": 0.5759479717813051,
      "grad_norm": 0.03135903924703598,
      "learning_rate": 8.481591710758377e-06,
      "loss": 0.2493,
      "step": 20900
    },
    {
      "epoch": 0.576223544973545,
      "grad_norm": 0.01237170584499836,
      "learning_rate": 8.47608024691358e-06,
      "loss": 0.0506,
      "step": 20910
    },
    {
      "epoch": 0.5764991181657848,
      "grad_norm": 53.507843017578125,
      "learning_rate": 8.470568783068784e-06,
      "loss": 0.0041,
      "step": 20920
    },
    {
      "epoch": 0.5767746913580247,
      "grad_norm": 0.011332573369145393,
      "learning_rate": 8.465057319223987e-06,
      "loss": 0.0048,
      "step": 20930
    },
    {
      "epoch": 0.5770502645502645,
      "grad_norm": 0.015213459730148315,
      "learning_rate": 8.45954585537919e-06,
      "loss": 0.0906,
      "step": 20940
    },
    {
      "epoch": 0.5773258377425045,
      "grad_norm": 0.39431703090667725,
      "learning_rate": 8.454034391534392e-06,
      "loss": 0.1671,
      "step": 20950
    },
    {
      "epoch": 0.5776014109347443,
      "grad_norm": 0.01603691652417183,
      "learning_rate": 8.448522927689595e-06,
      "loss": 0.2449,
      "step": 20960
    },
    {
      "epoch": 0.5778769841269841,
      "grad_norm": 0.032213903963565826,
      "learning_rate": 8.443011463844799e-06,
      "loss": 0.0033,
      "step": 20970
    },
    {
      "epoch": 0.578152557319224,
      "grad_norm": 0.17374663054943085,
      "learning_rate": 8.4375e-06,
      "loss": 0.0008,
      "step": 20980
    },
    {
      "epoch": 0.5784281305114638,
      "grad_norm": 0.025249874219298363,
      "learning_rate": 8.431988536155203e-06,
      "loss": 0.015,
      "step": 20990
    },
    {
      "epoch": 0.5787037037037037,
      "grad_norm": 0.011101910844445229,
      "learning_rate": 8.426477072310407e-06,
      "loss": 0.2419,
      "step": 21000
    },
    {
      "epoch": 0.5789792768959435,
      "grad_norm": 0.01485291775316,
      "learning_rate": 8.420965608465608e-06,
      "loss": 0.0913,
      "step": 21010
    },
    {
      "epoch": 0.5792548500881834,
      "grad_norm": 0.013696704059839249,
      "learning_rate": 8.415454144620811e-06,
      "loss": 0.0828,
      "step": 21020
    },
    {
      "epoch": 0.5795304232804233,
      "grad_norm": 0.014329073019325733,
      "learning_rate": 8.409942680776015e-06,
      "loss": 0.1893,
      "step": 21030
    },
    {
      "epoch": 0.5798059964726632,
      "grad_norm": 0.10649242252111435,
      "learning_rate": 8.404431216931218e-06,
      "loss": 0.0155,
      "step": 21040
    },
    {
      "epoch": 0.580081569664903,
      "grad_norm": 0.0426517128944397,
      "learning_rate": 8.398919753086421e-06,
      "loss": 0.0103,
      "step": 21050
    },
    {
      "epoch": 0.5803571428571429,
      "grad_norm": 0.084163136780262,
      "learning_rate": 8.393408289241623e-06,
      "loss": 0.0014,
      "step": 21060
    },
    {
      "epoch": 0.5806327160493827,
      "grad_norm": 0.03538445383310318,
      "learning_rate": 8.387896825396826e-06,
      "loss": 0.1918,
      "step": 21070
    },
    {
      "epoch": 0.5809082892416225,
      "grad_norm": 0.09460151195526123,
      "learning_rate": 8.38238536155203e-06,
      "loss": 0.0824,
      "step": 21080
    },
    {
      "epoch": 0.5811838624338624,
      "grad_norm": 53.31222152709961,
      "learning_rate": 8.376873897707231e-06,
      "loss": 0.0529,
      "step": 21090
    },
    {
      "epoch": 0.5814594356261023,
      "grad_norm": 0.7316672801971436,
      "learning_rate": 8.371362433862434e-06,
      "loss": 0.0024,
      "step": 21100
    },
    {
      "epoch": 0.5817350088183422,
      "grad_norm": 0.014930843375623226,
      "learning_rate": 8.365850970017637e-06,
      "loss": 0.0897,
      "step": 21110
    },
    {
      "epoch": 0.582010582010582,
      "grad_norm": 40.80036544799805,
      "learning_rate": 8.36033950617284e-06,
      "loss": 0.0257,
      "step": 21120
    },
    {
      "epoch": 0.5822861552028219,
      "grad_norm": 0.15681175887584686,
      "learning_rate": 8.354828042328042e-06,
      "loss": 0.0082,
      "step": 21130
    },
    {
      "epoch": 0.5825617283950617,
      "grad_norm": 102.26219177246094,
      "learning_rate": 8.349316578483245e-06,
      "loss": 0.1296,
      "step": 21140
    },
    {
      "epoch": 0.5828373015873016,
      "grad_norm": 174.15675354003906,
      "learning_rate": 8.343805114638449e-06,
      "loss": 0.2127,
      "step": 21150
    },
    {
      "epoch": 0.5831128747795414,
      "grad_norm": 18.427736282348633,
      "learning_rate": 8.338293650793652e-06,
      "loss": 0.1269,
      "step": 21160
    },
    {
      "epoch": 0.5833884479717814,
      "grad_norm": 306.0111389160156,
      "learning_rate": 8.332782186948855e-06,
      "loss": 0.2015,
      "step": 21170
    },
    {
      "epoch": 0.5836640211640212,
      "grad_norm": 0.011499284766614437,
      "learning_rate": 8.327270723104057e-06,
      "loss": 0.003,
      "step": 21180
    },
    {
      "epoch": 0.583939594356261,
      "grad_norm": 0.7037647366523743,
      "learning_rate": 8.32175925925926e-06,
      "loss": 0.0329,
      "step": 21190
    },
    {
      "epoch": 0.5842151675485009,
      "grad_norm": 0.027733178809285164,
      "learning_rate": 8.316247795414463e-06,
      "loss": 0.0937,
      "step": 21200
    },
    {
      "epoch": 0.5844907407407407,
      "grad_norm": 0.029896095395088196,
      "learning_rate": 8.310736331569665e-06,
      "loss": 0.0012,
      "step": 21210
    },
    {
      "epoch": 0.5847663139329806,
      "grad_norm": 0.009513454511761665,
      "learning_rate": 8.305224867724868e-06,
      "loss": 0.0011,
      "step": 21220
    },
    {
      "epoch": 0.5850418871252204,
      "grad_norm": 0.017911231145262718,
      "learning_rate": 8.299713403880071e-06,
      "loss": 0.0934,
      "step": 21230
    },
    {
      "epoch": 0.5853174603174603,
      "grad_norm": 158.7496795654297,
      "learning_rate": 8.294201940035273e-06,
      "loss": 0.1815,
      "step": 21240
    },
    {
      "epoch": 0.5855930335097002,
      "grad_norm": 149.7504425048828,
      "learning_rate": 8.288690476190478e-06,
      "loss": 0.1019,
      "step": 21250
    },
    {
      "epoch": 0.5858686067019401,
      "grad_norm": 93.7885513305664,
      "learning_rate": 8.28317901234568e-06,
      "loss": 0.0807,
      "step": 21260
    },
    {
      "epoch": 0.5861441798941799,
      "grad_norm": 0.016919247806072235,
      "learning_rate": 8.277667548500883e-06,
      "loss": 0.1611,
      "step": 21270
    },
    {
      "epoch": 0.5864197530864198,
      "grad_norm": 0.1089920923113823,
      "learning_rate": 8.272156084656086e-06,
      "loss": 0.0399,
      "step": 21280
    },
    {
      "epoch": 0.5866953262786596,
      "grad_norm": 0.24371686577796936,
      "learning_rate": 8.266644620811288e-06,
      "loss": 0.027,
      "step": 21290
    },
    {
      "epoch": 0.5869708994708994,
      "grad_norm": 0.013194544240832329,
      "learning_rate": 8.26113315696649e-06,
      "loss": 0.1591,
      "step": 21300
    },
    {
      "epoch": 0.5872464726631393,
      "grad_norm": 53.55250930786133,
      "learning_rate": 8.255621693121694e-06,
      "loss": 0.1076,
      "step": 21310
    },
    {
      "epoch": 0.5875220458553791,
      "grad_norm": 0.02232004515826702,
      "learning_rate": 8.250110229276896e-06,
      "loss": 0.025,
      "step": 21320
    },
    {
      "epoch": 0.5877976190476191,
      "grad_norm": 0.677468478679657,
      "learning_rate": 8.244598765432099e-06,
      "loss": 0.1128,
      "step": 21330
    },
    {
      "epoch": 0.5880731922398589,
      "grad_norm": 0.010646003298461437,
      "learning_rate": 8.239087301587302e-06,
      "loss": 0.0168,
      "step": 21340
    },
    {
      "epoch": 0.5883487654320988,
      "grad_norm": 0.018358949571847916,
      "learning_rate": 8.233575837742504e-06,
      "loss": 0.0093,
      "step": 21350
    },
    {
      "epoch": 0.5886243386243386,
      "grad_norm": 0.01884620636701584,
      "learning_rate": 8.228064373897709e-06,
      "loss": 0.0897,
      "step": 21360
    },
    {
      "epoch": 0.5888999118165785,
      "grad_norm": 0.014290759339928627,
      "learning_rate": 8.222552910052912e-06,
      "loss": 0.0949,
      "step": 21370
    },
    {
      "epoch": 0.5891754850088183,
      "grad_norm": 0.0479215644299984,
      "learning_rate": 8.217041446208114e-06,
      "loss": 0.0014,
      "step": 21380
    },
    {
      "epoch": 0.5894510582010583,
      "grad_norm": 0.05783824250102043,
      "learning_rate": 8.211529982363317e-06,
      "loss": 0.298,
      "step": 21390
    },
    {
      "epoch": 0.5897266313932981,
      "grad_norm": 4.0181474685668945,
      "learning_rate": 8.20601851851852e-06,
      "loss": 0.0035,
      "step": 21400
    },
    {
      "epoch": 0.5900022045855379,
      "grad_norm": 0.028327399864792824,
      "learning_rate": 8.200507054673722e-06,
      "loss": 0.0888,
      "step": 21410
    },
    {
      "epoch": 0.5902777777777778,
      "grad_norm": 0.7891846895217896,
      "learning_rate": 8.194995590828925e-06,
      "loss": 0.3295,
      "step": 21420
    },
    {
      "epoch": 0.5905533509700176,
      "grad_norm": 0.05438992381095886,
      "learning_rate": 8.189484126984128e-06,
      "loss": 0.0189,
      "step": 21430
    },
    {
      "epoch": 0.5908289241622575,
      "grad_norm": 0.01436446700245142,
      "learning_rate": 8.18397266313933e-06,
      "loss": 0.1522,
      "step": 21440
    },
    {
      "epoch": 0.5911044973544973,
      "grad_norm": 33.41630172729492,
      "learning_rate": 8.178461199294533e-06,
      "loss": 0.0896,
      "step": 21450
    },
    {
      "epoch": 0.5913800705467372,
      "grad_norm": 0.015113987028598785,
      "learning_rate": 8.172949735449736e-06,
      "loss": 0.1686,
      "step": 21460
    },
    {
      "epoch": 0.591655643738977,
      "grad_norm": 0.09954884648323059,
      "learning_rate": 8.16743827160494e-06,
      "loss": 0.0841,
      "step": 21470
    },
    {
      "epoch": 0.591931216931217,
      "grad_norm": 0.01128325890749693,
      "learning_rate": 8.161926807760143e-06,
      "loss": 0.063,
      "step": 21480
    },
    {
      "epoch": 0.5922067901234568,
      "grad_norm": 0.2872229516506195,
      "learning_rate": 8.156415343915344e-06,
      "loss": 0.0657,
      "step": 21490
    },
    {
      "epoch": 0.5924823633156967,
      "grad_norm": 0.03313710168004036,
      "learning_rate": 8.150903880070548e-06,
      "loss": 0.0018,
      "step": 21500
    },
    {
      "epoch": 0.5927579365079365,
      "grad_norm": 0.010391615331172943,
      "learning_rate": 8.14539241622575e-06,
      "loss": 0.0069,
      "step": 21510
    },
    {
      "epoch": 0.5930335097001763,
      "grad_norm": 83.89572143554688,
      "learning_rate": 8.139880952380952e-06,
      "loss": 0.1189,
      "step": 21520
    },
    {
      "epoch": 0.5933090828924162,
      "grad_norm": 0.01707334630191326,
      "learning_rate": 8.134369488536156e-06,
      "loss": 0.0746,
      "step": 21530
    },
    {
      "epoch": 0.593584656084656,
      "grad_norm": 0.06597363203763962,
      "learning_rate": 8.128858024691359e-06,
      "loss": 0.2017,
      "step": 21540
    },
    {
      "epoch": 0.593860229276896,
      "grad_norm": 0.09969576448202133,
      "learning_rate": 8.12334656084656e-06,
      "loss": 0.0879,
      "step": 21550
    },
    {
      "epoch": 0.5941358024691358,
      "grad_norm": 0.013523278757929802,
      "learning_rate": 8.117835097001764e-06,
      "loss": 0.0017,
      "step": 21560
    },
    {
      "epoch": 0.5944113756613757,
      "grad_norm": 0.07962630689144135,
      "learning_rate": 8.112323633156967e-06,
      "loss": 0.0088,
      "step": 21570
    },
    {
      "epoch": 0.5946869488536155,
      "grad_norm": 0.013667362742125988,
      "learning_rate": 8.10681216931217e-06,
      "loss": 0.0372,
      "step": 21580
    },
    {
      "epoch": 0.5949625220458554,
      "grad_norm": 0.008316987194120884,
      "learning_rate": 8.101300705467373e-06,
      "loss": 0.1807,
      "step": 21590
    },
    {
      "epoch": 0.5952380952380952,
      "grad_norm": 21.63526153564453,
      "learning_rate": 8.095789241622575e-06,
      "loss": 0.1275,
      "step": 21600
    },
    {
      "epoch": 0.5955136684303352,
      "grad_norm": 0.04485144838690758,
      "learning_rate": 8.090277777777778e-06,
      "loss": 0.1369,
      "step": 21610
    },
    {
      "epoch": 0.595789241622575,
      "grad_norm": 0.012739734724164009,
      "learning_rate": 8.084766313932982e-06,
      "loss": 0.0266,
      "step": 21620
    },
    {
      "epoch": 0.5960648148148148,
      "grad_norm": 0.018104616552591324,
      "learning_rate": 8.079254850088185e-06,
      "loss": 0.0439,
      "step": 21630
    },
    {
      "epoch": 0.5963403880070547,
      "grad_norm": 0.09501006454229355,
      "learning_rate": 8.073743386243386e-06,
      "loss": 0.1,
      "step": 21640
    },
    {
      "epoch": 0.5966159611992945,
      "grad_norm": 0.6129931807518005,
      "learning_rate": 8.06823192239859e-06,
      "loss": 0.1112,
      "step": 21650
    },
    {
      "epoch": 0.5968915343915344,
      "grad_norm": 0.1773536503314972,
      "learning_rate": 8.062720458553793e-06,
      "loss": 0.0887,
      "step": 21660
    },
    {
      "epoch": 0.5971671075837742,
      "grad_norm": 0.01010932493954897,
      "learning_rate": 8.057208994708994e-06,
      "loss": 0.1572,
      "step": 21670
    },
    {
      "epoch": 0.5974426807760141,
      "grad_norm": 0.015173806808888912,
      "learning_rate": 8.0516975308642e-06,
      "loss": 0.0432,
      "step": 21680
    },
    {
      "epoch": 0.597718253968254,
      "grad_norm": 0.0362703837454319,
      "learning_rate": 8.046186067019401e-06,
      "loss": 0.0482,
      "step": 21690
    },
    {
      "epoch": 0.5979938271604939,
      "grad_norm": 0.014097769744694233,
      "learning_rate": 8.040674603174604e-06,
      "loss": 0.0592,
      "step": 21700
    },
    {
      "epoch": 0.5982694003527337,
      "grad_norm": 193.5970458984375,
      "learning_rate": 8.035163139329807e-06,
      "loss": 0.098,
      "step": 21710
    },
    {
      "epoch": 0.5985449735449735,
      "grad_norm": 0.010494948364794254,
      "learning_rate": 8.029651675485009e-06,
      "loss": 0.0008,
      "step": 21720
    },
    {
      "epoch": 0.5988205467372134,
      "grad_norm": 126.73045349121094,
      "learning_rate": 8.024140211640212e-06,
      "loss": 0.2136,
      "step": 21730
    },
    {
      "epoch": 0.5990961199294532,
      "grad_norm": 3.8287014961242676,
      "learning_rate": 8.018628747795416e-06,
      "loss": 0.1321,
      "step": 21740
    },
    {
      "epoch": 0.5993716931216931,
      "grad_norm": 0.0342533178627491,
      "learning_rate": 8.013117283950617e-06,
      "loss": 0.3079,
      "step": 21750
    },
    {
      "epoch": 0.599647266313933,
      "grad_norm": 0.01548533421009779,
      "learning_rate": 8.00760582010582e-06,
      "loss": 0.0221,
      "step": 21760
    },
    {
      "epoch": 0.5999228395061729,
      "grad_norm": 0.04113014042377472,
      "learning_rate": 8.002094356261024e-06,
      "loss": 0.077,
      "step": 21770
    },
    {
      "epoch": 0.6001984126984127,
      "grad_norm": 0.7131957411766052,
      "learning_rate": 7.996582892416225e-06,
      "loss": 0.2133,
      "step": 21780
    },
    {
      "epoch": 0.6004739858906526,
      "grad_norm": 0.013042375445365906,
      "learning_rate": 7.99107142857143e-06,
      "loss": 0.0015,
      "step": 21790
    },
    {
      "epoch": 0.6007495590828924,
      "grad_norm": 0.2919464409351349,
      "learning_rate": 7.985559964726632e-06,
      "loss": 0.1384,
      "step": 21800
    },
    {
      "epoch": 0.6010251322751323,
      "grad_norm": 0.19309177994728088,
      "learning_rate": 7.980048500881835e-06,
      "loss": 0.0009,
      "step": 21810
    },
    {
      "epoch": 0.6013007054673721,
      "grad_norm": 0.10974821448326111,
      "learning_rate": 7.974537037037038e-06,
      "loss": 0.0016,
      "step": 21820
    },
    {
      "epoch": 0.6015762786596119,
      "grad_norm": 0.38725972175598145,
      "learning_rate": 7.96902557319224e-06,
      "loss": 0.1109,
      "step": 21830
    },
    {
      "epoch": 0.6018518518518519,
      "grad_norm": 0.011473702266812325,
      "learning_rate": 7.963514109347443e-06,
      "loss": 0.0025,
      "step": 21840
    },
    {
      "epoch": 0.6021274250440917,
      "grad_norm": 0.010708112269639969,
      "learning_rate": 7.958002645502646e-06,
      "loss": 0.0428,
      "step": 21850
    },
    {
      "epoch": 0.6024029982363316,
      "grad_norm": 0.025570658966898918,
      "learning_rate": 7.952491181657848e-06,
      "loss": 0.0463,
      "step": 21860
    },
    {
      "epoch": 0.6026785714285714,
      "grad_norm": 0.030855193734169006,
      "learning_rate": 7.946979717813051e-06,
      "loss": 0.1085,
      "step": 21870
    },
    {
      "epoch": 0.6029541446208113,
      "grad_norm": 0.056523073464632034,
      "learning_rate": 7.941468253968254e-06,
      "loss": 0.0012,
      "step": 21880
    },
    {
      "epoch": 0.6032297178130511,
      "grad_norm": 0.032575324177742004,
      "learning_rate": 7.935956790123458e-06,
      "loss": 0.0007,
      "step": 21890
    },
    {
      "epoch": 0.603505291005291,
      "grad_norm": 0.026215998455882072,
      "learning_rate": 7.930445326278661e-06,
      "loss": 0.0153,
      "step": 21900
    },
    {
      "epoch": 0.6037808641975309,
      "grad_norm": 0.009826251305639744,
      "learning_rate": 7.924933862433864e-06,
      "loss": 0.0007,
      "step": 21910
    },
    {
      "epoch": 0.6040564373897708,
      "grad_norm": 0.010532493703067303,
      "learning_rate": 7.919422398589066e-06,
      "loss": 0.1333,
      "step": 21920
    },
    {
      "epoch": 0.6043320105820106,
      "grad_norm": 0.011115903034806252,
      "learning_rate": 7.913910934744269e-06,
      "loss": 0.0016,
      "step": 21930
    },
    {
      "epoch": 0.6046075837742504,
      "grad_norm": 0.07571906596422195,
      "learning_rate": 7.908399470899472e-06,
      "loss": 0.0558,
      "step": 21940
    },
    {
      "epoch": 0.6048831569664903,
      "grad_norm": 0.21483775973320007,
      "learning_rate": 7.902888007054674e-06,
      "loss": 0.2037,
      "step": 21950
    },
    {
      "epoch": 0.6051587301587301,
      "grad_norm": 0.009220609441399574,
      "learning_rate": 7.897376543209877e-06,
      "loss": 0.2364,
      "step": 21960
    },
    {
      "epoch": 0.60543430335097,
      "grad_norm": 151.575439453125,
      "learning_rate": 7.89186507936508e-06,
      "loss": 0.2854,
      "step": 21970
    },
    {
      "epoch": 0.6057098765432098,
      "grad_norm": 0.013288257643580437,
      "learning_rate": 7.886353615520282e-06,
      "loss": 0.1677,
      "step": 21980
    },
    {
      "epoch": 0.6059854497354498,
      "grad_norm": 0.023234503343701363,
      "learning_rate": 7.880842151675485e-06,
      "loss": 0.1188,
      "step": 21990
    },
    {
      "epoch": 0.6062610229276896,
      "grad_norm": 0.09441373497247696,
      "learning_rate": 7.875330687830688e-06,
      "loss": 0.0693,
      "step": 22000
    },
    {
      "epoch": 0.6065365961199295,
      "grad_norm": 153.33531188964844,
      "learning_rate": 7.869819223985892e-06,
      "loss": 0.0324,
      "step": 22010
    },
    {
      "epoch": 0.6068121693121693,
      "grad_norm": 0.03718012198805809,
      "learning_rate": 7.864307760141095e-06,
      "loss": 0.1034,
      "step": 22020
    },
    {
      "epoch": 0.6070877425044092,
      "grad_norm": 0.13291768729686737,
      "learning_rate": 7.858796296296297e-06,
      "loss": 0.0197,
      "step": 22030
    },
    {
      "epoch": 0.607363315696649,
      "grad_norm": 1.2606168985366821,
      "learning_rate": 7.8532848324515e-06,
      "loss": 0.0872,
      "step": 22040
    },
    {
      "epoch": 0.6076388888888888,
      "grad_norm": 0.016868311911821365,
      "learning_rate": 7.847773368606703e-06,
      "loss": 0.0875,
      "step": 22050
    },
    {
      "epoch": 0.6079144620811288,
      "grad_norm": 0.01737118698656559,
      "learning_rate": 7.842261904761905e-06,
      "loss": 0.0007,
      "step": 22060
    },
    {
      "epoch": 0.6081900352733686,
      "grad_norm": 11.153525352478027,
      "learning_rate": 7.836750440917108e-06,
      "loss": 0.1468,
      "step": 22070
    },
    {
      "epoch": 0.6084656084656085,
      "grad_norm": 0.014509208500385284,
      "learning_rate": 7.831238977072311e-06,
      "loss": 0.0095,
      "step": 22080
    },
    {
      "epoch": 0.6087411816578483,
      "grad_norm": 0.01215850468724966,
      "learning_rate": 7.825727513227513e-06,
      "loss": 0.0453,
      "step": 22090
    },
    {
      "epoch": 0.6090167548500882,
      "grad_norm": 0.03521444648504257,
      "learning_rate": 7.820216049382716e-06,
      "loss": 0.024,
      "step": 22100
    },
    {
      "epoch": 0.609292328042328,
      "grad_norm": 31.100048065185547,
      "learning_rate": 7.81470458553792e-06,
      "loss": 0.1698,
      "step": 22110
    },
    {
      "epoch": 0.6095679012345679,
      "grad_norm": 26.407711029052734,
      "learning_rate": 7.809193121693122e-06,
      "loss": 0.1587,
      "step": 22120
    },
    {
      "epoch": 0.6098434744268078,
      "grad_norm": 48.187965393066406,
      "learning_rate": 7.803681657848326e-06,
      "loss": 0.1842,
      "step": 22130
    },
    {
      "epoch": 0.6101190476190477,
      "grad_norm": 0.01812145859003067,
      "learning_rate": 7.798170194003529e-06,
      "loss": 0.0012,
      "step": 22140
    },
    {
      "epoch": 0.6103946208112875,
      "grad_norm": 0.03153872862458229,
      "learning_rate": 7.79265873015873e-06,
      "loss": 0.0846,
      "step": 22150
    },
    {
      "epoch": 0.6106701940035273,
      "grad_norm": 1.0886480808258057,
      "learning_rate": 7.787147266313934e-06,
      "loss": 0.0863,
      "step": 22160
    },
    {
      "epoch": 0.6109457671957672,
      "grad_norm": 8.450095176696777,
      "learning_rate": 7.781635802469137e-06,
      "loss": 0.1732,
      "step": 22170
    },
    {
      "epoch": 0.611221340388007,
      "grad_norm": 0.010585235431790352,
      "learning_rate": 7.776124338624339e-06,
      "loss": 0.1939,
      "step": 22180
    },
    {
      "epoch": 0.6114969135802469,
      "grad_norm": 0.03466664254665375,
      "learning_rate": 7.770612874779542e-06,
      "loss": 0.0614,
      "step": 22190
    },
    {
      "epoch": 0.6117724867724867,
      "grad_norm": 0.1682768613100052,
      "learning_rate": 7.765101410934745e-06,
      "loss": 0.0016,
      "step": 22200
    },
    {
      "epoch": 0.6120480599647267,
      "grad_norm": 0.018465692177414894,
      "learning_rate": 7.759589947089947e-06,
      "loss": 0.1569,
      "step": 22210
    },
    {
      "epoch": 0.6123236331569665,
      "grad_norm": 0.13147960603237152,
      "learning_rate": 7.754078483245152e-06,
      "loss": 0.0277,
      "step": 22220
    },
    {
      "epoch": 0.6125992063492064,
      "grad_norm": 0.06390421092510223,
      "learning_rate": 7.748567019400353e-06,
      "loss": 0.0134,
      "step": 22230
    },
    {
      "epoch": 0.6128747795414462,
      "grad_norm": 1.1089832782745361,
      "learning_rate": 7.743055555555556e-06,
      "loss": 0.0551,
      "step": 22240
    },
    {
      "epoch": 0.6131503527336861,
      "grad_norm": 0.4590568542480469,
      "learning_rate": 7.73754409171076e-06,
      "loss": 0.1339,
      "step": 22250
    },
    {
      "epoch": 0.6134259259259259,
      "grad_norm": 0.36820709705352783,
      "learning_rate": 7.732032627865961e-06,
      "loss": 0.1163,
      "step": 22260
    },
    {
      "epoch": 0.6137014991181657,
      "grad_norm": 3.794435977935791,
      "learning_rate": 7.726521164021165e-06,
      "loss": 0.0024,
      "step": 22270
    },
    {
      "epoch": 0.6139770723104057,
      "grad_norm": 0.01028482150286436,
      "learning_rate": 7.721009700176368e-06,
      "loss": 0.0007,
      "step": 22280
    },
    {
      "epoch": 0.6142526455026455,
      "grad_norm": 0.008988182060420513,
      "learning_rate": 7.71549823633157e-06,
      "loss": 0.1209,
      "step": 22290
    },
    {
      "epoch": 0.6145282186948854,
      "grad_norm": 4.592826843261719,
      "learning_rate": 7.709986772486773e-06,
      "loss": 0.3722,
      "step": 22300
    },
    {
      "epoch": 0.6148037918871252,
      "grad_norm": 0.010295003652572632,
      "learning_rate": 7.704475308641976e-06,
      "loss": 0.0192,
      "step": 22310
    },
    {
      "epoch": 0.6150793650793651,
      "grad_norm": 2.842283010482788,
      "learning_rate": 7.69896384479718e-06,
      "loss": 0.0915,
      "step": 22320
    },
    {
      "epoch": 0.6153549382716049,
      "grad_norm": 9.098910331726074,
      "learning_rate": 7.693452380952382e-06,
      "loss": 0.0888,
      "step": 22330
    },
    {
      "epoch": 0.6156305114638448,
      "grad_norm": 0.30920472741127014,
      "learning_rate": 7.687940917107584e-06,
      "loss": 0.0945,
      "step": 22340
    },
    {
      "epoch": 0.6159060846560847,
      "grad_norm": 0.010505486279726028,
      "learning_rate": 7.682429453262787e-06,
      "loss": 0.0029,
      "step": 22350
    },
    {
      "epoch": 0.6161816578483245,
      "grad_norm": 0.010705733671784401,
      "learning_rate": 7.67691798941799e-06,
      "loss": 0.157,
      "step": 22360
    },
    {
      "epoch": 0.6164572310405644,
      "grad_norm": 0.01523325964808464,
      "learning_rate": 7.671406525573192e-06,
      "loss": 0.0013,
      "step": 22370
    },
    {
      "epoch": 0.6167328042328042,
      "grad_norm": 0.10388008505105972,
      "learning_rate": 7.665895061728395e-06,
      "loss": 0.0925,
      "step": 22380
    },
    {
      "epoch": 0.6170083774250441,
      "grad_norm": 0.044928375631570816,
      "learning_rate": 7.660383597883599e-06,
      "loss": 0.0008,
      "step": 22390
    },
    {
      "epoch": 0.6172839506172839,
      "grad_norm": 0.01730930060148239,
      "learning_rate": 7.654872134038802e-06,
      "loss": 0.2745,
      "step": 22400
    },
    {
      "epoch": 0.6175595238095238,
      "grad_norm": 0.018926365301012993,
      "learning_rate": 7.649360670194003e-06,
      "loss": 0.0511,
      "step": 22410
    },
    {
      "epoch": 0.6178350970017636,
      "grad_norm": 9.803942680358887,
      "learning_rate": 7.643849206349207e-06,
      "loss": 0.3152,
      "step": 22420
    },
    {
      "epoch": 0.6181106701940036,
      "grad_norm": 0.01047383714467287,
      "learning_rate": 7.63833774250441e-06,
      "loss": 0.133,
      "step": 22430
    },
    {
      "epoch": 0.6183862433862434,
      "grad_norm": 0.18669907748699188,
      "learning_rate": 7.632826278659613e-06,
      "loss": 0.1128,
      "step": 22440
    },
    {
      "epoch": 0.6186618165784833,
      "grad_norm": 0.0730275809764862,
      "learning_rate": 7.627314814814816e-06,
      "loss": 0.1381,
      "step": 22450
    },
    {
      "epoch": 0.6189373897707231,
      "grad_norm": 0.018679335713386536,
      "learning_rate": 7.621803350970019e-06,
      "loss": 0.0017,
      "step": 22460
    },
    {
      "epoch": 0.6192129629629629,
      "grad_norm": 0.011525249108672142,
      "learning_rate": 7.616291887125221e-06,
      "loss": 0.0014,
      "step": 22470
    },
    {
      "epoch": 0.6194885361552028,
      "grad_norm": 51.450775146484375,
      "learning_rate": 7.610780423280424e-06,
      "loss": 0.187,
      "step": 22480
    },
    {
      "epoch": 0.6197641093474426,
      "grad_norm": 20.16840934753418,
      "learning_rate": 7.605268959435627e-06,
      "loss": 0.129,
      "step": 22490
    },
    {
      "epoch": 0.6200396825396826,
      "grad_norm": 0.010896079242229462,
      "learning_rate": 7.599757495590829e-06,
      "loss": 0.165,
      "step": 22500
    },
    {
      "epoch": 0.6203152557319224,
      "grad_norm": 78.11693572998047,
      "learning_rate": 7.594246031746032e-06,
      "loss": 0.0169,
      "step": 22510
    },
    {
      "epoch": 0.6205908289241623,
      "grad_norm": 0.049131128937006,
      "learning_rate": 7.588734567901235e-06,
      "loss": 0.1779,
      "step": 22520
    },
    {
      "epoch": 0.6208664021164021,
      "grad_norm": 0.013204055838286877,
      "learning_rate": 7.583223104056437e-06,
      "loss": 0.2524,
      "step": 22530
    },
    {
      "epoch": 0.621141975308642,
      "grad_norm": 0.019220566377043724,
      "learning_rate": 7.5777116402116415e-06,
      "loss": 0.0979,
      "step": 22540
    },
    {
      "epoch": 0.6214175485008818,
      "grad_norm": 115.2440414428711,
      "learning_rate": 7.572200176366844e-06,
      "loss": 0.303,
      "step": 22550
    },
    {
      "epoch": 0.6216931216931217,
      "grad_norm": 0.011209218762814999,
      "learning_rate": 7.566688712522046e-06,
      "loss": 0.1601,
      "step": 22560
    },
    {
      "epoch": 0.6219686948853616,
      "grad_norm": 6.130213260650635,
      "learning_rate": 7.56117724867725e-06,
      "loss": 0.0032,
      "step": 22570
    },
    {
      "epoch": 0.6222442680776014,
      "grad_norm": 0.014145949855446815,
      "learning_rate": 7.555665784832452e-06,
      "loss": 0.1396,
      "step": 22580
    },
    {
      "epoch": 0.6225198412698413,
      "grad_norm": 0.06770367175340652,
      "learning_rate": 7.550154320987655e-06,
      "loss": 0.0011,
      "step": 22590
    },
    {
      "epoch": 0.6227954144620811,
      "grad_norm": 125.85679626464844,
      "learning_rate": 7.544642857142858e-06,
      "loss": 0.1014,
      "step": 22600
    },
    {
      "epoch": 0.623070987654321,
      "grad_norm": 0.016835343092679977,
      "learning_rate": 7.53913139329806e-06,
      "loss": 0.1028,
      "step": 22610
    },
    {
      "epoch": 0.6233465608465608,
      "grad_norm": 0.028276514261960983,
      "learning_rate": 7.533619929453263e-06,
      "loss": 0.1017,
      "step": 22620
    },
    {
      "epoch": 0.6236221340388007,
      "grad_norm": 10.060705184936523,
      "learning_rate": 7.528108465608466e-06,
      "loss": 0.0035,
      "step": 22630
    },
    {
      "epoch": 0.6238977072310405,
      "grad_norm": 0.10190299898386002,
      "learning_rate": 7.52259700176367e-06,
      "loss": 0.0568,
      "step": 22640
    },
    {
      "epoch": 0.6241732804232805,
      "grad_norm": 116.0263900756836,
      "learning_rate": 7.517085537918872e-06,
      "loss": 0.1362,
      "step": 22650
    },
    {
      "epoch": 0.6244488536155203,
      "grad_norm": 108.28160858154297,
      "learning_rate": 7.511574074074075e-06,
      "loss": 0.1935,
      "step": 22660
    },
    {
      "epoch": 0.6247244268077602,
      "grad_norm": 0.012845595367252827,
      "learning_rate": 7.506062610229278e-06,
      "loss": 0.0883,
      "step": 22670
    },
    {
      "epoch": 0.625,
      "grad_norm": 178.69680786132812,
      "learning_rate": 7.50055114638448e-06,
      "loss": 0.0292,
      "step": 22680
    },
    {
      "epoch": 0.6252755731922398,
      "grad_norm": 3.9323527812957764,
      "learning_rate": 7.495039682539683e-06,
      "loss": 0.1409,
      "step": 22690
    },
    {
      "epoch": 0.6255511463844797,
      "grad_norm": 0.01100166141986847,
      "learning_rate": 7.489528218694886e-06,
      "loss": 0.0203,
      "step": 22700
    },
    {
      "epoch": 0.6258267195767195,
      "grad_norm": 0.0854022204875946,
      "learning_rate": 7.4840167548500885e-06,
      "loss": 0.205,
      "step": 22710
    },
    {
      "epoch": 0.6261022927689595,
      "grad_norm": 113.89414978027344,
      "learning_rate": 7.478505291005292e-06,
      "loss": 0.2049,
      "step": 22720
    },
    {
      "epoch": 0.6263778659611993,
      "grad_norm": 1.3079231977462769,
      "learning_rate": 7.472993827160494e-06,
      "loss": 0.1208,
      "step": 22730
    },
    {
      "epoch": 0.6266534391534392,
      "grad_norm": 23.582948684692383,
      "learning_rate": 7.4674823633156965e-06,
      "loss": 0.0895,
      "step": 22740
    },
    {
      "epoch": 0.626929012345679,
      "grad_norm": 0.024734435603022575,
      "learning_rate": 7.461970899470901e-06,
      "loss": 0.0049,
      "step": 22750
    },
    {
      "epoch": 0.6272045855379189,
      "grad_norm": 35.890411376953125,
      "learning_rate": 7.456459435626103e-06,
      "loss": 0.2483,
      "step": 22760
    },
    {
      "epoch": 0.6274801587301587,
      "grad_norm": 0.019547924399375916,
      "learning_rate": 7.450947971781306e-06,
      "loss": 0.049,
      "step": 22770
    },
    {
      "epoch": 0.6277557319223986,
      "grad_norm": 0.010693172924220562,
      "learning_rate": 7.445436507936509e-06,
      "loss": 0.0853,
      "step": 22780
    },
    {
      "epoch": 0.6280313051146384,
      "grad_norm": 0.014102167449891567,
      "learning_rate": 7.439925044091711e-06,
      "loss": 0.0444,
      "step": 22790
    },
    {
      "epoch": 0.6283068783068783,
      "grad_norm": 0.011581889353692532,
      "learning_rate": 7.434413580246914e-06,
      "loss": 0.0887,
      "step": 22800
    },
    {
      "epoch": 0.6285824514991182,
      "grad_norm": 0.01855594478547573,
      "learning_rate": 7.428902116402117e-06,
      "loss": 0.0808,
      "step": 22810
    },
    {
      "epoch": 0.628858024691358,
      "grad_norm": 0.013289262540638447,
      "learning_rate": 7.423390652557319e-06,
      "loss": 0.1323,
      "step": 22820
    },
    {
      "epoch": 0.6291335978835979,
      "grad_norm": 142.6385955810547,
      "learning_rate": 7.4178791887125225e-06,
      "loss": 0.1125,
      "step": 22830
    },
    {
      "epoch": 0.6294091710758377,
      "grad_norm": 0.011800428852438927,
      "learning_rate": 7.412367724867725e-06,
      "loss": 0.0008,
      "step": 22840
    },
    {
      "epoch": 0.6296847442680776,
      "grad_norm": 0.945389986038208,
      "learning_rate": 7.406856261022928e-06,
      "loss": 0.0044,
      "step": 22850
    },
    {
      "epoch": 0.6299603174603174,
      "grad_norm": 0.011173990555107594,
      "learning_rate": 7.401344797178131e-06,
      "loss": 0.1466,
      "step": 22860
    },
    {
      "epoch": 0.6302358906525574,
      "grad_norm": 0.026690317317843437,
      "learning_rate": 7.395833333333335e-06,
      "loss": 0.1353,
      "step": 22870
    },
    {
      "epoch": 0.6305114638447972,
      "grad_norm": 0.013554524630308151,
      "learning_rate": 7.390321869488537e-06,
      "loss": 0.2761,
      "step": 22880
    },
    {
      "epoch": 0.6307870370370371,
      "grad_norm": 0.07172088325023651,
      "learning_rate": 7.3848104056437395e-06,
      "loss": 0.0024,
      "step": 22890
    },
    {
      "epoch": 0.6310626102292769,
      "grad_norm": 0.9077906012535095,
      "learning_rate": 7.379298941798943e-06,
      "loss": 0.1472,
      "step": 22900
    },
    {
      "epoch": 0.6313381834215167,
      "grad_norm": 0.04228553920984268,
      "learning_rate": 7.373787477954145e-06,
      "loss": 0.0581,
      "step": 22910
    },
    {
      "epoch": 0.6316137566137566,
      "grad_norm": 5.883992671966553,
      "learning_rate": 7.3682760141093476e-06,
      "loss": 0.0333,
      "step": 22920
    },
    {
      "epoch": 0.6318893298059964,
      "grad_norm": 1.3034240007400513,
      "learning_rate": 7.362764550264551e-06,
      "loss": 0.0508,
      "step": 22930
    },
    {
      "epoch": 0.6321649029982364,
      "grad_norm": 1.8957152366638184,
      "learning_rate": 7.357253086419753e-06,
      "loss": 0.182,
      "step": 22940
    },
    {
      "epoch": 0.6324404761904762,
      "grad_norm": 0.17723283171653748,
      "learning_rate": 7.351741622574956e-06,
      "loss": 0.1055,
      "step": 22950
    },
    {
      "epoch": 0.6327160493827161,
      "grad_norm": 0.24945315718650818,
      "learning_rate": 7.34623015873016e-06,
      "loss": 0.1692,
      "step": 22960
    },
    {
      "epoch": 0.6329916225749559,
      "grad_norm": 0.07594780623912811,
      "learning_rate": 7.340718694885363e-06,
      "loss": 0.1448,
      "step": 22970
    },
    {
      "epoch": 0.6332671957671958,
      "grad_norm": 89.51438903808594,
      "learning_rate": 7.3352072310405654e-06,
      "loss": 0.0191,
      "step": 22980
    },
    {
      "epoch": 0.6335427689594356,
      "grad_norm": 8.663254737854004,
      "learning_rate": 7.329695767195768e-06,
      "loss": 0.0525,
      "step": 22990
    },
    {
      "epoch": 0.6338183421516755,
      "grad_norm": 0.011898205615580082,
      "learning_rate": 7.324184303350971e-06,
      "loss": 0.0182,
      "step": 23000
    },
    {
      "epoch": 0.6340939153439153,
      "grad_norm": 20.44161033630371,
      "learning_rate": 7.3186728395061735e-06,
      "loss": 0.0028,
      "step": 23010
    },
    {
      "epoch": 0.6343694885361552,
      "grad_norm": 0.015641754493117332,
      "learning_rate": 7.313161375661376e-06,
      "loss": 0.019,
      "step": 23020
    },
    {
      "epoch": 0.6346450617283951,
      "grad_norm": 0.030110087245702744,
      "learning_rate": 7.307649911816579e-06,
      "loss": 0.0327,
      "step": 23030
    },
    {
      "epoch": 0.6349206349206349,
      "grad_norm": 46.7413215637207,
      "learning_rate": 7.302138447971782e-06,
      "loss": 0.1596,
      "step": 23040
    },
    {
      "epoch": 0.6351962081128748,
      "grad_norm": 0.012710999697446823,
      "learning_rate": 7.296626984126984e-06,
      "loss": 0.184,
      "step": 23050
    },
    {
      "epoch": 0.6354717813051146,
      "grad_norm": 0.017975937575101852,
      "learning_rate": 7.291115520282187e-06,
      "loss": 0.1433,
      "step": 23060
    },
    {
      "epoch": 0.6357473544973545,
      "grad_norm": 0.977665364742279,
      "learning_rate": 7.2856040564373905e-06,
      "loss": 0.0833,
      "step": 23070
    },
    {
      "epoch": 0.6360229276895943,
      "grad_norm": 0.012189076282083988,
      "learning_rate": 7.280092592592594e-06,
      "loss": 0.0868,
      "step": 23080
    },
    {
      "epoch": 0.6362985008818343,
      "grad_norm": 0.06959249079227448,
      "learning_rate": 7.274581128747796e-06,
      "loss": 0.0018,
      "step": 23090
    },
    {
      "epoch": 0.6365740740740741,
      "grad_norm": 0.02617449313402176,
      "learning_rate": 7.2690696649029994e-06,
      "loss": 0.0949,
      "step": 23100
    },
    {
      "epoch": 0.6368496472663139,
      "grad_norm": 0.01211657002568245,
      "learning_rate": 7.263558201058202e-06,
      "loss": 0.0046,
      "step": 23110
    },
    {
      "epoch": 0.6371252204585538,
      "grad_norm": 0.010911853052675724,
      "learning_rate": 7.258046737213404e-06,
      "loss": 0.1296,
      "step": 23120
    },
    {
      "epoch": 0.6374007936507936,
      "grad_norm": 4.97084379196167,
      "learning_rate": 7.2525352733686075e-06,
      "loss": 0.0859,
      "step": 23130
    },
    {
      "epoch": 0.6376763668430335,
      "grad_norm": 7.5717902183532715,
      "learning_rate": 7.24702380952381e-06,
      "loss": 0.0041,
      "step": 23140
    },
    {
      "epoch": 0.6379519400352733,
      "grad_norm": 0.08555818349123001,
      "learning_rate": 7.241512345679012e-06,
      "loss": 0.1969,
      "step": 23150
    },
    {
      "epoch": 0.6382275132275133,
      "grad_norm": 0.012538069859147072,
      "learning_rate": 7.236000881834216e-06,
      "loss": 0.001,
      "step": 23160
    },
    {
      "epoch": 0.6385030864197531,
      "grad_norm": 44.31001663208008,
      "learning_rate": 7.230489417989418e-06,
      "loss": 0.0787,
      "step": 23170
    },
    {
      "epoch": 0.638778659611993,
      "grad_norm": 0.026138747110962868,
      "learning_rate": 7.224977954144622e-06,
      "loss": 0.0082,
      "step": 23180
    },
    {
      "epoch": 0.6390542328042328,
      "grad_norm": 0.01913870871067047,
      "learning_rate": 7.2194664902998245e-06,
      "loss": 0.1722,
      "step": 23190
    },
    {
      "epoch": 0.6393298059964727,
      "grad_norm": 95.71944427490234,
      "learning_rate": 7.213955026455027e-06,
      "loss": 0.0698,
      "step": 23200
    },
    {
      "epoch": 0.6396053791887125,
      "grad_norm": 0.08938883990049362,
      "learning_rate": 7.20844356261023e-06,
      "loss": 0.1261,
      "step": 23210
    },
    {
      "epoch": 0.6398809523809523,
      "grad_norm": 0.01949847862124443,
      "learning_rate": 7.202932098765433e-06,
      "loss": 0.0012,
      "step": 23220
    },
    {
      "epoch": 0.6401565255731922,
      "grad_norm": 0.008905196562409401,
      "learning_rate": 7.197420634920636e-06,
      "loss": 0.0811,
      "step": 23230
    },
    {
      "epoch": 0.6404320987654321,
      "grad_norm": 0.05787014216184616,
      "learning_rate": 7.191909171075838e-06,
      "loss": 0.1408,
      "step": 23240
    },
    {
      "epoch": 0.640707671957672,
      "grad_norm": 0.014306606724858284,
      "learning_rate": 7.186397707231041e-06,
      "loss": 0.0201,
      "step": 23250
    },
    {
      "epoch": 0.6409832451499118,
      "grad_norm": 0.039282359182834625,
      "learning_rate": 7.180886243386244e-06,
      "loss": 0.0251,
      "step": 23260
    },
    {
      "epoch": 0.6412588183421517,
      "grad_norm": 0.017434433102607727,
      "learning_rate": 7.175374779541446e-06,
      "loss": 0.0102,
      "step": 23270
    },
    {
      "epoch": 0.6415343915343915,
      "grad_norm": 0.01941341906785965,
      "learning_rate": 7.169863315696649e-06,
      "loss": 0.2,
      "step": 23280
    },
    {
      "epoch": 0.6418099647266314,
      "grad_norm": 11.95515251159668,
      "learning_rate": 7.164351851851853e-06,
      "loss": 0.1303,
      "step": 23290
    },
    {
      "epoch": 0.6420855379188712,
      "grad_norm": 0.0521342009305954,
      "learning_rate": 7.158840388007055e-06,
      "loss": 0.2001,
      "step": 23300
    },
    {
      "epoch": 0.6423611111111112,
      "grad_norm": 0.13578003644943237,
      "learning_rate": 7.1533289241622586e-06,
      "loss": 0.006,
      "step": 23310
    },
    {
      "epoch": 0.642636684303351,
      "grad_norm": 130.42860412597656,
      "learning_rate": 7.147817460317461e-06,
      "loss": 0.158,
      "step": 23320
    },
    {
      "epoch": 0.6429122574955908,
      "grad_norm": 31.37523651123047,
      "learning_rate": 7.142305996472663e-06,
      "loss": 0.3718,
      "step": 23330
    },
    {
      "epoch": 0.6431878306878307,
      "grad_norm": 0.009291925467550755,
      "learning_rate": 7.136794532627867e-06,
      "loss": 0.1908,
      "step": 23340
    },
    {
      "epoch": 0.6434634038800705,
      "grad_norm": 0.015877725556492805,
      "learning_rate": 7.131283068783069e-06,
      "loss": 0.017,
      "step": 23350
    },
    {
      "epoch": 0.6437389770723104,
      "grad_norm": 0.011422783136367798,
      "learning_rate": 7.125771604938272e-06,
      "loss": 0.048,
      "step": 23360
    },
    {
      "epoch": 0.6440145502645502,
      "grad_norm": 119.50555419921875,
      "learning_rate": 7.120260141093475e-06,
      "loss": 0.0534,
      "step": 23370
    },
    {
      "epoch": 0.6442901234567902,
      "grad_norm": 0.01468232274055481,
      "learning_rate": 7.114748677248677e-06,
      "loss": 0.2003,
      "step": 23380
    },
    {
      "epoch": 0.64456569664903,
      "grad_norm": 123.80670166015625,
      "learning_rate": 7.109237213403881e-06,
      "loss": 0.1256,
      "step": 23390
    },
    {
      "epoch": 0.6448412698412699,
      "grad_norm": 169.2831573486328,
      "learning_rate": 7.103725749559084e-06,
      "loss": 0.2097,
      "step": 23400
    },
    {
      "epoch": 0.6451168430335097,
      "grad_norm": 72.5346908569336,
      "learning_rate": 7.098214285714287e-06,
      "loss": 0.0373,
      "step": 23410
    },
    {
      "epoch": 0.6453924162257496,
      "grad_norm": 165.63365173339844,
      "learning_rate": 7.092702821869489e-06,
      "loss": 0.1032,
      "step": 23420
    },
    {
      "epoch": 0.6456679894179894,
      "grad_norm": 0.1636466681957245,
      "learning_rate": 7.087191358024692e-06,
      "loss": 0.1499,
      "step": 23430
    },
    {
      "epoch": 0.6459435626102292,
      "grad_norm": 5.777892589569092,
      "learning_rate": 7.081679894179895e-06,
      "loss": 0.0812,
      "step": 23440
    },
    {
      "epoch": 0.6462191358024691,
      "grad_norm": 38.95811080932617,
      "learning_rate": 7.076168430335097e-06,
      "loss": 0.04,
      "step": 23450
    },
    {
      "epoch": 0.646494708994709,
      "grad_norm": 0.036024898290634155,
      "learning_rate": 7.0706569664903e-06,
      "loss": 0.0896,
      "step": 23460
    },
    {
      "epoch": 0.6467702821869489,
      "grad_norm": 9.627097129821777,
      "learning_rate": 7.065145502645503e-06,
      "loss": 0.419,
      "step": 23470
    },
    {
      "epoch": 0.6470458553791887,
      "grad_norm": 100.04772186279297,
      "learning_rate": 7.0596340388007055e-06,
      "loss": 0.1027,
      "step": 23480
    },
    {
      "epoch": 0.6473214285714286,
      "grad_norm": 1.055176854133606,
      "learning_rate": 7.054122574955909e-06,
      "loss": 0.038,
      "step": 23490
    },
    {
      "epoch": 0.6475970017636684,
      "grad_norm": 0.12128161638975143,
      "learning_rate": 7.048611111111112e-06,
      "loss": 0.1212,
      "step": 23500
    },
    {
      "epoch": 0.6478725749559083,
      "grad_norm": 0.010859970934689045,
      "learning_rate": 7.043099647266315e-06,
      "loss": 0.1653,
      "step": 23510
    },
    {
      "epoch": 0.6481481481481481,
      "grad_norm": 0.07514191418886185,
      "learning_rate": 7.037588183421518e-06,
      "loss": 0.0723,
      "step": 23520
    },
    {
      "epoch": 0.6484237213403881,
      "grad_norm": 34.296024322509766,
      "learning_rate": 7.03207671957672e-06,
      "loss": 0.1175,
      "step": 23530
    },
    {
      "epoch": 0.6486992945326279,
      "grad_norm": 0.0523805133998394,
      "learning_rate": 7.026565255731923e-06,
      "loss": 0.1577,
      "step": 23540
    },
    {
      "epoch": 0.6489748677248677,
      "grad_norm": 6.044422626495361,
      "learning_rate": 7.021053791887126e-06,
      "loss": 0.1012,
      "step": 23550
    },
    {
      "epoch": 0.6492504409171076,
      "grad_norm": 0.06346192955970764,
      "learning_rate": 7.015542328042328e-06,
      "loss": 0.0768,
      "step": 23560
    },
    {
      "epoch": 0.6495260141093474,
      "grad_norm": 0.14177045226097107,
      "learning_rate": 7.010030864197531e-06,
      "loss": 0.0015,
      "step": 23570
    },
    {
      "epoch": 0.6498015873015873,
      "grad_norm": 0.10868054628372192,
      "learning_rate": 7.004519400352734e-06,
      "loss": 0.0398,
      "step": 23580
    },
    {
      "epoch": 0.6500771604938271,
      "grad_norm": 10.25877857208252,
      "learning_rate": 6.999007936507936e-06,
      "loss": 0.0451,
      "step": 23590
    },
    {
      "epoch": 0.650352733686067,
      "grad_norm": 0.12363018840551376,
      "learning_rate": 6.9934964726631395e-06,
      "loss": 0.2423,
      "step": 23600
    },
    {
      "epoch": 0.6506283068783069,
      "grad_norm": 0.1327616274356842,
      "learning_rate": 6.987985008818344e-06,
      "loss": 0.0053,
      "step": 23610
    },
    {
      "epoch": 0.6509038800705468,
      "grad_norm": 0.16068552434444427,
      "learning_rate": 6.982473544973546e-06,
      "loss": 0.0009,
      "step": 23620
    },
    {
      "epoch": 0.6511794532627866,
      "grad_norm": 0.6640244126319885,
      "learning_rate": 6.9769620811287484e-06,
      "loss": 0.0878,
      "step": 23630
    },
    {
      "epoch": 0.6514550264550265,
      "grad_norm": 0.7709914445877075,
      "learning_rate": 6.971450617283952e-06,
      "loss": 0.0764,
      "step": 23640
    },
    {
      "epoch": 0.6517305996472663,
      "grad_norm": 0.01411794126033783,
      "learning_rate": 6.965939153439154e-06,
      "loss": 0.0011,
      "step": 23650
    },
    {
      "epoch": 0.6520061728395061,
      "grad_norm": 0.03877674788236618,
      "learning_rate": 6.9604276895943565e-06,
      "loss": 0.0861,
      "step": 23660
    },
    {
      "epoch": 0.652281746031746,
      "grad_norm": 0.011926657520234585,
      "learning_rate": 6.95491622574956e-06,
      "loss": 0.0138,
      "step": 23670
    },
    {
      "epoch": 0.6525573192239859,
      "grad_norm": 0.12604045867919922,
      "learning_rate": 6.949404761904762e-06,
      "loss": 0.0008,
      "step": 23680
    },
    {
      "epoch": 0.6528328924162258,
      "grad_norm": 0.04067158326506615,
      "learning_rate": 6.943893298059965e-06,
      "loss": 0.1217,
      "step": 23690
    },
    {
      "epoch": 0.6531084656084656,
      "grad_norm": 0.3366636037826538,
      "learning_rate": 6.938381834215168e-06,
      "loss": 0.0255,
      "step": 23700
    },
    {
      "epoch": 0.6533840388007055,
      "grad_norm": 107.35440063476562,
      "learning_rate": 6.932870370370371e-06,
      "loss": 0.082,
      "step": 23710
    },
    {
      "epoch": 0.6536596119929453,
      "grad_norm": 0.007715865503996611,
      "learning_rate": 6.927358906525574e-06,
      "loss": 0.0007,
      "step": 23720
    },
    {
      "epoch": 0.6539351851851852,
      "grad_norm": 0.04011900722980499,
      "learning_rate": 6.921847442680777e-06,
      "loss": 0.0575,
      "step": 23730
    },
    {
      "epoch": 0.654210758377425,
      "grad_norm": 64.72256469726562,
      "learning_rate": 6.91633597883598e-06,
      "loss": 0.1422,
      "step": 23740
    },
    {
      "epoch": 0.6544863315696648,
      "grad_norm": 0.013930167071521282,
      "learning_rate": 6.9108245149911825e-06,
      "loss": 0.0788,
      "step": 23750
    },
    {
      "epoch": 0.6547619047619048,
      "grad_norm": 155.73980712890625,
      "learning_rate": 6.905313051146385e-06,
      "loss": 0.1096,
      "step": 23760
    },
    {
      "epoch": 0.6550374779541446,
      "grad_norm": 0.14579038321971893,
      "learning_rate": 6.899801587301588e-06,
      "loss": 0.104,
      "step": 23770
    },
    {
      "epoch": 0.6553130511463845,
      "grad_norm": 0.028710011392831802,
      "learning_rate": 6.8942901234567905e-06,
      "loss": 0.1413,
      "step": 23780
    },
    {
      "epoch": 0.6555886243386243,
      "grad_norm": 87.8798599243164,
      "learning_rate": 6.888778659611993e-06,
      "loss": 0.1361,
      "step": 23790
    },
    {
      "epoch": 0.6558641975308642,
      "grad_norm": 0.012633801437914371,
      "learning_rate": 6.883267195767196e-06,
      "loss": 0.1092,
      "step": 23800
    },
    {
      "epoch": 0.656139770723104,
      "grad_norm": 0.009727271273732185,
      "learning_rate": 6.877755731922399e-06,
      "loss": 0.092,
      "step": 23810
    },
    {
      "epoch": 0.656415343915344,
      "grad_norm": 0.018721189349889755,
      "learning_rate": 6.872244268077603e-06,
      "loss": 0.0572,
      "step": 23820
    },
    {
      "epoch": 0.6566909171075838,
      "grad_norm": 0.17432840168476105,
      "learning_rate": 6.866732804232805e-06,
      "loss": 0.0709,
      "step": 23830
    },
    {
      "epoch": 0.6569664902998237,
      "grad_norm": 7.424630641937256,
      "learning_rate": 6.8612213403880075e-06,
      "loss": 0.0902,
      "step": 23840
    },
    {
      "epoch": 0.6572420634920635,
      "grad_norm": 0.06746756285429001,
      "learning_rate": 6.855709876543211e-06,
      "loss": 0.0012,
      "step": 23850
    },
    {
      "epoch": 0.6575176366843033,
      "grad_norm": 0.18324153125286102,
      "learning_rate": 6.850198412698413e-06,
      "loss": 0.0016,
      "step": 23860
    },
    {
      "epoch": 0.6577932098765432,
      "grad_norm": 4.08427619934082,
      "learning_rate": 6.8446869488536165e-06,
      "loss": 0.0036,
      "step": 23870
    },
    {
      "epoch": 0.658068783068783,
      "grad_norm": 0.011920499615371227,
      "learning_rate": 6.839175485008819e-06,
      "loss": 0.015,
      "step": 23880
    },
    {
      "epoch": 0.658344356261023,
      "grad_norm": 0.02532813511788845,
      "learning_rate": 6.833664021164021e-06,
      "loss": 0.0446,
      "step": 23890
    },
    {
      "epoch": 0.6586199294532628,
      "grad_norm": 0.01221472304314375,
      "learning_rate": 6.8281525573192246e-06,
      "loss": 0.0545,
      "step": 23900
    },
    {
      "epoch": 0.6588955026455027,
      "grad_norm": 0.009249188005924225,
      "learning_rate": 6.822641093474427e-06,
      "loss": 0.0778,
      "step": 23910
    },
    {
      "epoch": 0.6591710758377425,
      "grad_norm": 0.654866635799408,
      "learning_rate": 6.817129629629629e-06,
      "loss": 0.0139,
      "step": 23920
    },
    {
      "epoch": 0.6594466490299824,
      "grad_norm": 52.89380645751953,
      "learning_rate": 6.8116181657848335e-06,
      "loss": 0.0591,
      "step": 23930
    },
    {
      "epoch": 0.6597222222222222,
      "grad_norm": 0.49969300627708435,
      "learning_rate": 6.806106701940036e-06,
      "loss": 0.0944,
      "step": 23940
    },
    {
      "epoch": 0.6599977954144621,
      "grad_norm": 0.05228378251194954,
      "learning_rate": 6.800595238095239e-06,
      "loss": 0.012,
      "step": 23950
    },
    {
      "epoch": 0.6602733686067019,
      "grad_norm": 121.02371215820312,
      "learning_rate": 6.7950837742504416e-06,
      "loss": 0.2469,
      "step": 23960
    },
    {
      "epoch": 0.6605489417989417,
      "grad_norm": 0.010442090220749378,
      "learning_rate": 6.789572310405644e-06,
      "loss": 0.1826,
      "step": 23970
    },
    {
      "epoch": 0.6608245149911817,
      "grad_norm": 166.40643310546875,
      "learning_rate": 6.784060846560847e-06,
      "loss": 0.1163,
      "step": 23980
    },
    {
      "epoch": 0.6611000881834215,
      "grad_norm": 14.042616844177246,
      "learning_rate": 6.77854938271605e-06,
      "loss": 0.2146,
      "step": 23990
    },
    {
      "epoch": 0.6613756613756614,
      "grad_norm": 221.47689819335938,
      "learning_rate": 6.773037918871253e-06,
      "loss": 0.0327,
      "step": 24000
    },
    {
      "epoch": 0.6616512345679012,
      "grad_norm": 0.012542802840471268,
      "learning_rate": 6.767526455026455e-06,
      "loss": 0.1213,
      "step": 24010
    },
    {
      "epoch": 0.6619268077601411,
      "grad_norm": 0.06068851426243782,
      "learning_rate": 6.762014991181658e-06,
      "loss": 0.0297,
      "step": 24020
    },
    {
      "epoch": 0.6622023809523809,
      "grad_norm": 0.14683659374713898,
      "learning_rate": 6.756503527336862e-06,
      "loss": 0.0946,
      "step": 24030
    },
    {
      "epoch": 0.6624779541446209,
      "grad_norm": 0.03257236257195473,
      "learning_rate": 6.750992063492064e-06,
      "loss": 0.0009,
      "step": 24040
    },
    {
      "epoch": 0.6627535273368607,
      "grad_norm": 0.011230397038161755,
      "learning_rate": 6.7454805996472675e-06,
      "loss": 0.1633,
      "step": 24050
    },
    {
      "epoch": 0.6630291005291006,
      "grad_norm": 0.08538134396076202,
      "learning_rate": 6.73996913580247e-06,
      "loss": 0.183,
      "step": 24060
    },
    {
      "epoch": 0.6633046737213404,
      "grad_norm": 4.865405559539795,
      "learning_rate": 6.734457671957672e-06,
      "loss": 0.1931,
      "step": 24070
    },
    {
      "epoch": 0.6635802469135802,
      "grad_norm": 0.02978602796792984,
      "learning_rate": 6.728946208112876e-06,
      "loss": 0.0008,
      "step": 24080
    },
    {
      "epoch": 0.6638558201058201,
      "grad_norm": 139.12367248535156,
      "learning_rate": 6.723434744268078e-06,
      "loss": 0.0919,
      "step": 24090
    },
    {
      "epoch": 0.6641313932980599,
      "grad_norm": 0.007513099350035191,
      "learning_rate": 6.71792328042328e-06,
      "loss": 0.2318,
      "step": 24100
    },
    {
      "epoch": 0.6644069664902998,
      "grad_norm": 0.007541590370237827,
      "learning_rate": 6.712411816578484e-06,
      "loss": 0.0059,
      "step": 24110
    },
    {
      "epoch": 0.6646825396825397,
      "grad_norm": 0.041036102920770645,
      "learning_rate": 6.706900352733686e-06,
      "loss": 0.0261,
      "step": 24120
    },
    {
      "epoch": 0.6649581128747796,
      "grad_norm": 0.00928924698382616,
      "learning_rate": 6.701388888888889e-06,
      "loss": 0.1506,
      "step": 24130
    },
    {
      "epoch": 0.6652336860670194,
      "grad_norm": 0.009498605504631996,
      "learning_rate": 6.695877425044093e-06,
      "loss": 0.0457,
      "step": 24140
    },
    {
      "epoch": 0.6655092592592593,
      "grad_norm": 3.154782772064209,
      "learning_rate": 6.690365961199296e-06,
      "loss": 0.1275,
      "step": 24150
    },
    {
      "epoch": 0.6657848324514991,
      "grad_norm": 0.009529918432235718,
      "learning_rate": 6.684854497354498e-06,
      "loss": 0.0009,
      "step": 24160
    },
    {
      "epoch": 0.666060405643739,
      "grad_norm": 0.10525266081094742,
      "learning_rate": 6.679343033509701e-06,
      "loss": 0.0008,
      "step": 24170
    },
    {
      "epoch": 0.6663359788359788,
      "grad_norm": 53.13045883178711,
      "learning_rate": 6.673831569664904e-06,
      "loss": 0.1347,
      "step": 24180
    },
    {
      "epoch": 0.6666115520282186,
      "grad_norm": 0.013538963161408901,
      "learning_rate": 6.668320105820106e-06,
      "loss": 0.0009,
      "step": 24190
    },
    {
      "epoch": 0.6668871252204586,
      "grad_norm": 1.5511231422424316,
      "learning_rate": 6.662808641975309e-06,
      "loss": 0.084,
      "step": 24200
    },
    {
      "epoch": 0.6671626984126984,
      "grad_norm": 0.10918103903532028,
      "learning_rate": 6.657297178130512e-06,
      "loss": 0.0007,
      "step": 24210
    },
    {
      "epoch": 0.6674382716049383,
      "grad_norm": 0.025604719296097755,
      "learning_rate": 6.6517857142857144e-06,
      "loss": 0.0209,
      "step": 24220
    },
    {
      "epoch": 0.6677138447971781,
      "grad_norm": 10.351861000061035,
      "learning_rate": 6.646274250440917e-06,
      "loss": 0.1582,
      "step": 24230
    },
    {
      "epoch": 0.667989417989418,
      "grad_norm": 0.019945655018091202,
      "learning_rate": 6.64076278659612e-06,
      "loss": 0.0595,
      "step": 24240
    },
    {
      "epoch": 0.6682649911816578,
      "grad_norm": 0.012801891192793846,
      "learning_rate": 6.635251322751323e-06,
      "loss": 0.1644,
      "step": 24250
    },
    {
      "epoch": 0.6685405643738977,
      "grad_norm": 69.7354507446289,
      "learning_rate": 6.629739858906527e-06,
      "loss": 0.0932,
      "step": 24260
    },
    {
      "epoch": 0.6688161375661376,
      "grad_norm": 0.011564480140805244,
      "learning_rate": 6.624228395061729e-06,
      "loss": 0.0157,
      "step": 24270
    },
    {
      "epoch": 0.6690917107583775,
      "grad_norm": 0.010830678977072239,
      "learning_rate": 6.618716931216932e-06,
      "loss": 0.0637,
      "step": 24280
    },
    {
      "epoch": 0.6693672839506173,
      "grad_norm": 0.03307017311453819,
      "learning_rate": 6.613205467372135e-06,
      "loss": 0.0192,
      "step": 24290
    },
    {
      "epoch": 0.6696428571428571,
      "grad_norm": 0.01182532124221325,
      "learning_rate": 6.607694003527337e-06,
      "loss": 0.2382,
      "step": 24300
    },
    {
      "epoch": 0.669918430335097,
      "grad_norm": 0.027858220040798187,
      "learning_rate": 6.60218253968254e-06,
      "loss": 0.0007,
      "step": 24310
    },
    {
      "epoch": 0.6701940035273368,
      "grad_norm": 29.88505744934082,
      "learning_rate": 6.596671075837743e-06,
      "loss": 0.0035,
      "step": 24320
    },
    {
      "epoch": 0.6704695767195767,
      "grad_norm": 0.8101661801338196,
      "learning_rate": 6.591159611992945e-06,
      "loss": 0.0038,
      "step": 24330
    },
    {
      "epoch": 0.6707451499118166,
      "grad_norm": 0.019980538636446,
      "learning_rate": 6.5856481481481484e-06,
      "loss": 0.0056,
      "step": 24340
    },
    {
      "epoch": 0.6710207231040565,
      "grad_norm": 0.0323013961315155,
      "learning_rate": 6.580136684303352e-06,
      "loss": 0.001,
      "step": 24350
    },
    {
      "epoch": 0.6712962962962963,
      "grad_norm": 0.018909605219960213,
      "learning_rate": 6.574625220458555e-06,
      "loss": 0.1326,
      "step": 24360
    },
    {
      "epoch": 0.6715718694885362,
      "grad_norm": 0.015133201144635677,
      "learning_rate": 6.569113756613757e-06,
      "loss": 0.0006,
      "step": 24370
    },
    {
      "epoch": 0.671847442680776,
      "grad_norm": 1.0063375234603882,
      "learning_rate": 6.563602292768961e-06,
      "loss": 0.0204,
      "step": 24380
    },
    {
      "epoch": 0.6721230158730159,
      "grad_norm": 142.85162353515625,
      "learning_rate": 6.558090828924163e-06,
      "loss": 0.1422,
      "step": 24390
    },
    {
      "epoch": 0.6723985890652557,
      "grad_norm": 0.022376855835318565,
      "learning_rate": 6.5525793650793655e-06,
      "loss": 0.258,
      "step": 24400
    },
    {
      "epoch": 0.6726741622574955,
      "grad_norm": 0.007937546819448471,
      "learning_rate": 6.547067901234569e-06,
      "loss": 0.0527,
      "step": 24410
    },
    {
      "epoch": 0.6729497354497355,
      "grad_norm": 6.920124053955078,
      "learning_rate": 6.541556437389771e-06,
      "loss": 0.1904,
      "step": 24420
    },
    {
      "epoch": 0.6732253086419753,
      "grad_norm": 149.89443969726562,
      "learning_rate": 6.5360449735449735e-06,
      "loss": 0.0774,
      "step": 24430
    },
    {
      "epoch": 0.6735008818342152,
      "grad_norm": 170.5767822265625,
      "learning_rate": 6.530533509700177e-06,
      "loss": 0.0451,
      "step": 24440
    },
    {
      "epoch": 0.673776455026455,
      "grad_norm": 101.46142578125,
      "learning_rate": 6.525022045855379e-06,
      "loss": 0.0545,
      "step": 24450
    },
    {
      "epoch": 0.6740520282186949,
      "grad_norm": 0.012592139653861523,
      "learning_rate": 6.519510582010583e-06,
      "loss": 0.0022,
      "step": 24460
    },
    {
      "epoch": 0.6743276014109347,
      "grad_norm": 0.03665079176425934,
      "learning_rate": 6.513999118165786e-06,
      "loss": 0.0621,
      "step": 24470
    },
    {
      "epoch": 0.6746031746031746,
      "grad_norm": 0.011498204432427883,
      "learning_rate": 6.508487654320988e-06,
      "loss": 0.0142,
      "step": 24480
    },
    {
      "epoch": 0.6748787477954145,
      "grad_norm": 0.07752593606710434,
      "learning_rate": 6.502976190476191e-06,
      "loss": 0.0008,
      "step": 24490
    },
    {
      "epoch": 0.6751543209876543,
      "grad_norm": 0.17629770934581757,
      "learning_rate": 6.497464726631394e-06,
      "loss": 0.0128,
      "step": 24500
    },
    {
      "epoch": 0.6754298941798942,
      "grad_norm": 0.007777998689562082,
      "learning_rate": 6.491953262786597e-06,
      "loss": 0.0857,
      "step": 24510
    },
    {
      "epoch": 0.675705467372134,
      "grad_norm": 159.8789825439453,
      "learning_rate": 6.4864417989417995e-06,
      "loss": 0.1889,
      "step": 24520
    },
    {
      "epoch": 0.6759810405643739,
      "grad_norm": 0.040993984788656235,
      "learning_rate": 6.480930335097002e-06,
      "loss": 0.1242,
      "step": 24530
    },
    {
      "epoch": 0.6762566137566137,
      "grad_norm": 0.07483740895986557,
      "learning_rate": 6.475418871252205e-06,
      "loss": 0.0036,
      "step": 24540
    },
    {
      "epoch": 0.6765321869488536,
      "grad_norm": 265.47454833984375,
      "learning_rate": 6.4699074074074076e-06,
      "loss": 0.2088,
      "step": 24550
    },
    {
      "epoch": 0.6768077601410935,
      "grad_norm": 1.814884066581726,
      "learning_rate": 6.46439594356261e-06,
      "loss": 0.0079,
      "step": 24560
    },
    {
      "epoch": 0.6770833333333334,
      "grad_norm": 0.19102713465690613,
      "learning_rate": 6.458884479717814e-06,
      "loss": 0.1274,
      "step": 24570
    },
    {
      "epoch": 0.6773589065255732,
      "grad_norm": 20.405454635620117,
      "learning_rate": 6.4533730158730165e-06,
      "loss": 0.2404,
      "step": 24580
    },
    {
      "epoch": 0.6776344797178131,
      "grad_norm": 0.011324466206133366,
      "learning_rate": 6.44786155202822e-06,
      "loss": 0.2107,
      "step": 24590
    },
    {
      "epoch": 0.6779100529100529,
      "grad_norm": 0.2857612371444702,
      "learning_rate": 6.442350088183422e-06,
      "loss": 0.0331,
      "step": 24600
    },
    {
      "epoch": 0.6781856261022927,
      "grad_norm": 0.01720747910439968,
      "learning_rate": 6.4368386243386246e-06,
      "loss": 0.0966,
      "step": 24610
    },
    {
      "epoch": 0.6784611992945326,
      "grad_norm": 0.028103606775403023,
      "learning_rate": 6.431327160493828e-06,
      "loss": 0.1859,
      "step": 24620
    },
    {
      "epoch": 0.6787367724867724,
      "grad_norm": 0.009813729673624039,
      "learning_rate": 6.42581569664903e-06,
      "loss": 0.021,
      "step": 24630
    },
    {
      "epoch": 0.6790123456790124,
      "grad_norm": 0.012604969553649426,
      "learning_rate": 6.4203042328042335e-06,
      "loss": 0.0006,
      "step": 24640
    },
    {
      "epoch": 0.6792879188712522,
      "grad_norm": 15.848542213439941,
      "learning_rate": 6.414792768959436e-06,
      "loss": 0.0861,
      "step": 24650
    },
    {
      "epoch": 0.6795634920634921,
      "grad_norm": 0.018404660746455193,
      "learning_rate": 6.409281305114638e-06,
      "loss": 0.0518,
      "step": 24660
    },
    {
      "epoch": 0.6798390652557319,
      "grad_norm": 0.010081515647470951,
      "learning_rate": 6.403769841269842e-06,
      "loss": 0.0689,
      "step": 24670
    },
    {
      "epoch": 0.6801146384479718,
      "grad_norm": 0.011715006083250046,
      "learning_rate": 6.398258377425045e-06,
      "loss": 0.0018,
      "step": 24680
    },
    {
      "epoch": 0.6803902116402116,
      "grad_norm": 0.022088561207056046,
      "learning_rate": 6.392746913580248e-06,
      "loss": 0.0044,
      "step": 24690
    },
    {
      "epoch": 0.6806657848324515,
      "grad_norm": 61.062339782714844,
      "learning_rate": 6.3872354497354505e-06,
      "loss": 0.1365,
      "step": 24700
    },
    {
      "epoch": 0.6809413580246914,
      "grad_norm": 133.4130401611328,
      "learning_rate": 6.381723985890653e-06,
      "loss": 0.0121,
      "step": 24710
    },
    {
      "epoch": 0.6812169312169312,
      "grad_norm": 0.022317934781312943,
      "learning_rate": 6.376212522045856e-06,
      "loss": 0.086,
      "step": 24720
    },
    {
      "epoch": 0.6814925044091711,
      "grad_norm": 0.10597401857376099,
      "learning_rate": 6.370701058201059e-06,
      "loss": 0.1507,
      "step": 24730
    },
    {
      "epoch": 0.6817680776014109,
      "grad_norm": 0.008996421471238136,
      "learning_rate": 6.365189594356261e-06,
      "loss": 0.0854,
      "step": 24740
    },
    {
      "epoch": 0.6820436507936508,
      "grad_norm": 0.3582477867603302,
      "learning_rate": 6.359678130511464e-06,
      "loss": 0.1243,
      "step": 24750
    },
    {
      "epoch": 0.6823192239858906,
      "grad_norm": 0.0156976617872715,
      "learning_rate": 6.354166666666667e-06,
      "loss": 0.0095,
      "step": 24760
    },
    {
      "epoch": 0.6825947971781305,
      "grad_norm": 0.0159398652613163,
      "learning_rate": 6.34865520282187e-06,
      "loss": 0.0873,
      "step": 24770
    },
    {
      "epoch": 0.6828703703703703,
      "grad_norm": 27.69070816040039,
      "learning_rate": 6.343143738977073e-06,
      "loss": 0.2086,
      "step": 24780
    },
    {
      "epoch": 0.6831459435626103,
      "grad_norm": 0.17266902327537537,
      "learning_rate": 6.3376322751322765e-06,
      "loss": 0.0007,
      "step": 24790
    },
    {
      "epoch": 0.6834215167548501,
      "grad_norm": 0.03610696643590927,
      "learning_rate": 6.332120811287479e-06,
      "loss": 0.0881,
      "step": 24800
    },
    {
      "epoch": 0.68369708994709,
      "grad_norm": 0.14802119135856628,
      "learning_rate": 6.326609347442681e-06,
      "loss": 0.0755,
      "step": 24810
    },
    {
      "epoch": 0.6839726631393298,
      "grad_norm": 0.04168128967285156,
      "learning_rate": 6.3210978835978845e-06,
      "loss": 0.1913,
      "step": 24820
    },
    {
      "epoch": 0.6842482363315696,
      "grad_norm": 0.18564307689666748,
      "learning_rate": 6.315586419753087e-06,
      "loss": 0.1634,
      "step": 24830
    },
    {
      "epoch": 0.6845238095238095,
      "grad_norm": 0.012954499572515488,
      "learning_rate": 6.310074955908289e-06,
      "loss": 0.0074,
      "step": 24840
    },
    {
      "epoch": 0.6847993827160493,
      "grad_norm": 0.040249649435281754,
      "learning_rate": 6.304563492063493e-06,
      "loss": 0.0518,
      "step": 24850
    },
    {
      "epoch": 0.6850749559082893,
      "grad_norm": 0.009201891720294952,
      "learning_rate": 6.299052028218695e-06,
      "loss": 0.0006,
      "step": 24860
    },
    {
      "epoch": 0.6853505291005291,
      "grad_norm": 0.010304154828190804,
      "learning_rate": 6.2935405643738974e-06,
      "loss": 0.0006,
      "step": 24870
    },
    {
      "epoch": 0.685626102292769,
      "grad_norm": 0.33140915632247925,
      "learning_rate": 6.288029100529101e-06,
      "loss": 0.0362,
      "step": 24880
    },
    {
      "epoch": 0.6859016754850088,
      "grad_norm": 0.012351798824965954,
      "learning_rate": 6.282517636684304e-06,
      "loss": 0.1072,
      "step": 24890
    },
    {
      "epoch": 0.6861772486772487,
      "grad_norm": 0.013696454465389252,
      "learning_rate": 6.277006172839507e-06,
      "loss": 0.0006,
      "step": 24900
    },
    {
      "epoch": 0.6864528218694885,
      "grad_norm": 0.09966763854026794,
      "learning_rate": 6.27149470899471e-06,
      "loss": 0.0879,
      "step": 24910
    },
    {
      "epoch": 0.6867283950617284,
      "grad_norm": 48.80614471435547,
      "learning_rate": 6.265983245149913e-06,
      "loss": 0.0118,
      "step": 24920
    },
    {
      "epoch": 0.6870039682539683,
      "grad_norm": 0.18194232881069183,
      "learning_rate": 6.260471781305115e-06,
      "loss": 0.2037,
      "step": 24930
    },
    {
      "epoch": 0.6872795414462081,
      "grad_norm": 0.016850711777806282,
      "learning_rate": 6.254960317460318e-06,
      "loss": 0.0445,
      "step": 24940
    },
    {
      "epoch": 0.687555114638448,
      "grad_norm": 61.480316162109375,
      "learning_rate": 6.249448853615521e-06,
      "loss": 0.0346,
      "step": 24950
    },
    {
      "epoch": 0.6878306878306878,
      "grad_norm": 81.57781219482422,
      "learning_rate": 6.243937389770723e-06,
      "loss": 0.0827,
      "step": 24960
    },
    {
      "epoch": 0.6881062610229277,
      "grad_norm": 8.947866439819336,
      "learning_rate": 6.238425925925926e-06,
      "loss": 0.1802,
      "step": 24970
    },
    {
      "epoch": 0.6883818342151675,
      "grad_norm": 0.011851530522108078,
      "learning_rate": 6.232914462081129e-06,
      "loss": 0.0058,
      "step": 24980
    },
    {
      "epoch": 0.6886574074074074,
      "grad_norm": 0.04884486645460129,
      "learning_rate": 6.2274029982363315e-06,
      "loss": 0.0017,
      "step": 24990
    },
    {
      "epoch": 0.6889329805996472,
      "grad_norm": 0.009758776053786278,
      "learning_rate": 6.2218915343915356e-06,
      "loss": 0.0009,
      "step": 25000
    },
    {
      "epoch": 0.6892085537918872,
      "grad_norm": 30.550947189331055,
      "learning_rate": 6.216380070546738e-06,
      "loss": 0.0778,
      "step": 25010
    },
    {
      "epoch": 0.689484126984127,
      "grad_norm": 0.01072928961366415,
      "learning_rate": 6.21086860670194e-06,
      "loss": 0.0026,
      "step": 25020
    },
    {
      "epoch": 0.6897597001763669,
      "grad_norm": 0.12244275212287903,
      "learning_rate": 6.205357142857144e-06,
      "loss": 0.0481,
      "step": 25030
    },
    {
      "epoch": 0.6900352733686067,
      "grad_norm": 0.033088792115449905,
      "learning_rate": 6.199845679012346e-06,
      "loss": 0.1236,
      "step": 25040
    },
    {
      "epoch": 0.6903108465608465,
      "grad_norm": 177.77288818359375,
      "learning_rate": 6.194334215167549e-06,
      "loss": 0.0238,
      "step": 25050
    },
    {
      "epoch": 0.6905864197530864,
      "grad_norm": 0.018674440681934357,
      "learning_rate": 6.188822751322752e-06,
      "loss": 0.0006,
      "step": 25060
    },
    {
      "epoch": 0.6908619929453262,
      "grad_norm": 0.02405591309070587,
      "learning_rate": 6.183311287477954e-06,
      "loss": 0.0956,
      "step": 25070
    },
    {
      "epoch": 0.6911375661375662,
      "grad_norm": 84.65486145019531,
      "learning_rate": 6.177799823633157e-06,
      "loss": 0.0331,
      "step": 25080
    },
    {
      "epoch": 0.691413139329806,
      "grad_norm": 0.026843735948204994,
      "learning_rate": 6.17228835978836e-06,
      "loss": 0.1987,
      "step": 25090
    },
    {
      "epoch": 0.6916887125220459,
      "grad_norm": 0.012248670682311058,
      "learning_rate": 6.166776895943564e-06,
      "loss": 0.1006,
      "step": 25100
    },
    {
      "epoch": 0.6919642857142857,
      "grad_norm": 0.5915761590003967,
      "learning_rate": 6.161265432098766e-06,
      "loss": 0.0837,
      "step": 25110
    },
    {
      "epoch": 0.6922398589065256,
      "grad_norm": 0.03718109801411629,
      "learning_rate": 6.155753968253969e-06,
      "loss": 0.0772,
      "step": 25120
    },
    {
      "epoch": 0.6925154320987654,
      "grad_norm": 0.29646554589271545,
      "learning_rate": 6.150242504409172e-06,
      "loss": 0.0096,
      "step": 25130
    },
    {
      "epoch": 0.6927910052910053,
      "grad_norm": 102.32132720947266,
      "learning_rate": 6.144731040564374e-06,
      "loss": 0.1057,
      "step": 25140
    },
    {
      "epoch": 0.6930665784832452,
      "grad_norm": 0.16333961486816406,
      "learning_rate": 6.139219576719578e-06,
      "loss": 0.0053,
      "step": 25150
    },
    {
      "epoch": 0.693342151675485,
      "grad_norm": 0.21599499881267548,
      "learning_rate": 6.13370811287478e-06,
      "loss": 0.0479,
      "step": 25160
    },
    {
      "epoch": 0.6936177248677249,
      "grad_norm": 0.5140564441680908,
      "learning_rate": 6.1281966490299825e-06,
      "loss": 0.1768,
      "step": 25170
    },
    {
      "epoch": 0.6938932980599647,
      "grad_norm": 0.013803158886730671,
      "learning_rate": 6.122685185185186e-06,
      "loss": 0.0012,
      "step": 25180
    },
    {
      "epoch": 0.6941688712522046,
      "grad_norm": 0.007382919546216726,
      "learning_rate": 6.117173721340388e-06,
      "loss": 0.0628,
      "step": 25190
    },
    {
      "epoch": 0.6944444444444444,
      "grad_norm": 0.012258731760084629,
      "learning_rate": 6.1116622574955906e-06,
      "loss": 0.0272,
      "step": 25200
    },
    {
      "epoch": 0.6947200176366843,
      "grad_norm": 0.009640938602387905,
      "learning_rate": 6.106150793650795e-06,
      "loss": 0.0726,
      "step": 25210
    },
    {
      "epoch": 0.6949955908289241,
      "grad_norm": 0.06556383520364761,
      "learning_rate": 6.100639329805997e-06,
      "loss": 0.0861,
      "step": 25220
    },
    {
      "epoch": 0.6952711640211641,
      "grad_norm": 0.019330834969878197,
      "learning_rate": 6.0951278659612e-06,
      "loss": 0.0012,
      "step": 25230
    },
    {
      "epoch": 0.6955467372134039,
      "grad_norm": 73.13968658447266,
      "learning_rate": 6.089616402116403e-06,
      "loss": 0.0945,
      "step": 25240
    },
    {
      "epoch": 0.6958223104056437,
      "grad_norm": 0.7739659547805786,
      "learning_rate": 6.084104938271605e-06,
      "loss": 0.0442,
      "step": 25250
    },
    {
      "epoch": 0.6960978835978836,
      "grad_norm": 0.03794286400079727,
      "learning_rate": 6.0785934744268084e-06,
      "loss": 0.3624,
      "step": 25260
    },
    {
      "epoch": 0.6963734567901234,
      "grad_norm": 0.00806825328618288,
      "learning_rate": 6.073082010582011e-06,
      "loss": 0.0166,
      "step": 25270
    },
    {
      "epoch": 0.6966490299823633,
      "grad_norm": 0.19704951345920563,
      "learning_rate": 6.067570546737214e-06,
      "loss": 0.0609,
      "step": 25280
    },
    {
      "epoch": 0.6969246031746031,
      "grad_norm": 0.009176263585686684,
      "learning_rate": 6.0620590828924165e-06,
      "loss": 0.001,
      "step": 25290
    },
    {
      "epoch": 0.6972001763668431,
      "grad_norm": 0.010012025944888592,
      "learning_rate": 6.056547619047619e-06,
      "loss": 0.0007,
      "step": 25300
    },
    {
      "epoch": 0.6974757495590829,
      "grad_norm": 0.07807811349630356,
      "learning_rate": 6.051036155202822e-06,
      "loss": 0.0015,
      "step": 25310
    },
    {
      "epoch": 0.6977513227513228,
      "grad_norm": 144.37266540527344,
      "learning_rate": 6.0455246913580254e-06,
      "loss": 0.0722,
      "step": 25320
    },
    {
      "epoch": 0.6980268959435626,
      "grad_norm": 0.0112240519374609,
      "learning_rate": 6.040013227513229e-06,
      "loss": 0.0869,
      "step": 25330
    },
    {
      "epoch": 0.6983024691358025,
      "grad_norm": 0.04508187994360924,
      "learning_rate": 6.034501763668431e-06,
      "loss": 0.0279,
      "step": 25340
    },
    {
      "epoch": 0.6985780423280423,
      "grad_norm": 0.009800611063838005,
      "learning_rate": 6.0289902998236335e-06,
      "loss": 0.0021,
      "step": 25350
    },
    {
      "epoch": 0.6988536155202821,
      "grad_norm": 0.007609484251588583,
      "learning_rate": 6.023478835978837e-06,
      "loss": 0.0007,
      "step": 25360
    },
    {
      "epoch": 0.699129188712522,
      "grad_norm": 201.15792846679688,
      "learning_rate": 6.017967372134039e-06,
      "loss": 0.2058,
      "step": 25370
    },
    {
      "epoch": 0.6994047619047619,
      "grad_norm": 0.02882266230881214,
      "learning_rate": 6.012455908289242e-06,
      "loss": 0.1024,
      "step": 25380
    },
    {
      "epoch": 0.6996803350970018,
      "grad_norm": 0.01600629836320877,
      "learning_rate": 6.006944444444445e-06,
      "loss": 0.0782,
      "step": 25390
    },
    {
      "epoch": 0.6999559082892416,
      "grad_norm": 0.34087473154067993,
      "learning_rate": 6.001432980599647e-06,
      "loss": 0.0395,
      "step": 25400
    },
    {
      "epoch": 0.7002314814814815,
      "grad_norm": 0.019127510488033295,
      "learning_rate": 5.9959215167548505e-06,
      "loss": 0.1204,
      "step": 25410
    },
    {
      "epoch": 0.7005070546737213,
      "grad_norm": 0.03171200305223465,
      "learning_rate": 5.990410052910054e-06,
      "loss": 0.0011,
      "step": 25420
    },
    {
      "epoch": 0.7007826278659612,
      "grad_norm": 0.06145163252949715,
      "learning_rate": 5.984898589065257e-06,
      "loss": 0.0007,
      "step": 25430
    },
    {
      "epoch": 0.701058201058201,
      "grad_norm": 185.2317352294922,
      "learning_rate": 5.9793871252204595e-06,
      "loss": 0.114,
      "step": 25440
    },
    {
      "epoch": 0.701333774250441,
      "grad_norm": 0.2510848343372345,
      "learning_rate": 5.973875661375662e-06,
      "loss": 0.0007,
      "step": 25450
    },
    {
      "epoch": 0.7016093474426808,
      "grad_norm": 1.238529086112976,
      "learning_rate": 5.968364197530865e-06,
      "loss": 0.1087,
      "step": 25460
    },
    {
      "epoch": 0.7018849206349206,
      "grad_norm": 0.12915758788585663,
      "learning_rate": 5.9628527336860675e-06,
      "loss": 0.1165,
      "step": 25470
    },
    {
      "epoch": 0.7021604938271605,
      "grad_norm": 15.913758277893066,
      "learning_rate": 5.95734126984127e-06,
      "loss": 0.1155,
      "step": 25480
    },
    {
      "epoch": 0.7024360670194003,
      "grad_norm": 0.01714136265218258,
      "learning_rate": 5.951829805996473e-06,
      "loss": 0.0171,
      "step": 25490
    },
    {
      "epoch": 0.7027116402116402,
      "grad_norm": 0.006930089555680752,
      "learning_rate": 5.946318342151676e-06,
      "loss": 0.0006,
      "step": 25500
    },
    {
      "epoch": 0.70298721340388,
      "grad_norm": 0.02435026504099369,
      "learning_rate": 5.940806878306878e-06,
      "loss": 0.1776,
      "step": 25510
    },
    {
      "epoch": 0.70326278659612,
      "grad_norm": 0.013065029866993427,
      "learning_rate": 5.935295414462081e-06,
      "loss": 0.0036,
      "step": 25520
    },
    {
      "epoch": 0.7035383597883598,
      "grad_norm": 0.007637438364326954,
      "learning_rate": 5.9297839506172846e-06,
      "loss": 0.1025,
      "step": 25530
    },
    {
      "epoch": 0.7038139329805997,
      "grad_norm": 0.012327693402767181,
      "learning_rate": 5.924272486772488e-06,
      "loss": 0.0439,
      "step": 25540
    },
    {
      "epoch": 0.7040895061728395,
      "grad_norm": 0.1157875657081604,
      "learning_rate": 5.91876102292769e-06,
      "loss": 0.037,
      "step": 25550
    },
    {
      "epoch": 0.7043650793650794,
      "grad_norm": 0.213652104139328,
      "learning_rate": 5.9132495590828935e-06,
      "loss": 0.0835,
      "step": 25560
    },
    {
      "epoch": 0.7046406525573192,
      "grad_norm": 0.013503101654350758,
      "learning_rate": 5.907738095238096e-06,
      "loss": 0.084,
      "step": 25570
    },
    {
      "epoch": 0.704916225749559,
      "grad_norm": 0.025751950219273567,
      "learning_rate": 5.902226631393298e-06,
      "loss": 0.2371,
      "step": 25580
    },
    {
      "epoch": 0.705191798941799,
      "grad_norm": 0.013726428151130676,
      "learning_rate": 5.8967151675485016e-06,
      "loss": 0.0725,
      "step": 25590
    },
    {
      "epoch": 0.7054673721340388,
      "grad_norm": 0.009066488593816757,
      "learning_rate": 5.891203703703704e-06,
      "loss": 0.1383,
      "step": 25600
    },
    {
      "epoch": 0.7057429453262787,
      "grad_norm": 4.782369613647461,
      "learning_rate": 5.885692239858906e-06,
      "loss": 0.001,
      "step": 25610
    },
    {
      "epoch": 0.7060185185185185,
      "grad_norm": 0.10707423835992813,
      "learning_rate": 5.88018077601411e-06,
      "loss": 0.104,
      "step": 25620
    },
    {
      "epoch": 0.7062940917107584,
      "grad_norm": 0.021161045879125595,
      "learning_rate": 5.874669312169312e-06,
      "loss": 0.1303,
      "step": 25630
    },
    {
      "epoch": 0.7065696649029982,
      "grad_norm": 108.58472442626953,
      "learning_rate": 5.869157848324516e-06,
      "loss": 0.0949,
      "step": 25640
    },
    {
      "epoch": 0.7068452380952381,
      "grad_norm": 0.014587693847715855,
      "learning_rate": 5.8636463844797186e-06,
      "loss": 0.083,
      "step": 25650
    },
    {
      "epoch": 0.707120811287478,
      "grad_norm": 0.05545389652252197,
      "learning_rate": 5.858134920634921e-06,
      "loss": 0.1276,
      "step": 25660
    },
    {
      "epoch": 0.7073963844797179,
      "grad_norm": 0.021344469860196114,
      "learning_rate": 5.852623456790124e-06,
      "loss": 0.0659,
      "step": 25670
    },
    {
      "epoch": 0.7076719576719577,
      "grad_norm": 147.748779296875,
      "learning_rate": 5.847111992945327e-06,
      "loss": 0.0164,
      "step": 25680
    },
    {
      "epoch": 0.7079475308641975,
      "grad_norm": 0.008608820848166943,
      "learning_rate": 5.84160052910053e-06,
      "loss": 0.1232,
      "step": 25690
    },
    {
      "epoch": 0.7082231040564374,
      "grad_norm": 0.008020882494747639,
      "learning_rate": 5.836089065255732e-06,
      "loss": 0.0006,
      "step": 25700
    },
    {
      "epoch": 0.7084986772486772,
      "grad_norm": 4.686032772064209,
      "learning_rate": 5.830577601410935e-06,
      "loss": 0.0949,
      "step": 25710
    },
    {
      "epoch": 0.7087742504409171,
      "grad_norm": 162.79931640625,
      "learning_rate": 5.825066137566138e-06,
      "loss": 0.1082,
      "step": 25720
    },
    {
      "epoch": 0.7090498236331569,
      "grad_norm": 114.4571533203125,
      "learning_rate": 5.81955467372134e-06,
      "loss": 0.145,
      "step": 25730
    },
    {
      "epoch": 0.7093253968253969,
      "grad_norm": 21.423864364624023,
      "learning_rate": 5.814043209876543e-06,
      "loss": 0.0938,
      "step": 25740
    },
    {
      "epoch": 0.7096009700176367,
      "grad_norm": 0.2432849407196045,
      "learning_rate": 5.808531746031747e-06,
      "loss": 0.0006,
      "step": 25750
    },
    {
      "epoch": 0.7098765432098766,
      "grad_norm": 0.03018483705818653,
      "learning_rate": 5.803020282186949e-06,
      "loss": 0.0085,
      "step": 25760
    },
    {
      "epoch": 0.7101521164021164,
      "grad_norm": 0.026811083778738976,
      "learning_rate": 5.797508818342153e-06,
      "loss": 0.0213,
      "step": 25770
    },
    {
      "epoch": 0.7104276895943563,
      "grad_norm": 0.02369583398103714,
      "learning_rate": 5.791997354497355e-06,
      "loss": 0.105,
      "step": 25780
    },
    {
      "epoch": 0.7107032627865961,
      "grad_norm": 0.009799684397876263,
      "learning_rate": 5.786485890652557e-06,
      "loss": 0.0705,
      "step": 25790
    },
    {
      "epoch": 0.7109788359788359,
      "grad_norm": 0.008424254134297371,
      "learning_rate": 5.780974426807761e-06,
      "loss": 0.0115,
      "step": 25800
    },
    {
      "epoch": 0.7112544091710759,
      "grad_norm": 262.06036376953125,
      "learning_rate": 5.775462962962963e-06,
      "loss": 0.1282,
      "step": 25810
    },
    {
      "epoch": 0.7115299823633157,
      "grad_norm": 0.05065421015024185,
      "learning_rate": 5.769951499118166e-06,
      "loss": 0.017,
      "step": 25820
    },
    {
      "epoch": 0.7118055555555556,
      "grad_norm": 0.0171552374958992,
      "learning_rate": 5.764440035273369e-06,
      "loss": 0.0298,
      "step": 25830
    },
    {
      "epoch": 0.7120811287477954,
      "grad_norm": 0.016411248594522476,
      "learning_rate": 5.758928571428571e-06,
      "loss": 0.2217,
      "step": 25840
    },
    {
      "epoch": 0.7123567019400353,
      "grad_norm": 0.0341302752494812,
      "learning_rate": 5.753417107583775e-06,
      "loss": 0.0557,
      "step": 25850
    },
    {
      "epoch": 0.7126322751322751,
      "grad_norm": 0.016540654003620148,
      "learning_rate": 5.747905643738978e-06,
      "loss": 0.0484,
      "step": 25860
    },
    {
      "epoch": 0.712907848324515,
      "grad_norm": 0.007543828804045916,
      "learning_rate": 5.742394179894181e-06,
      "loss": 0.2539,
      "step": 25870
    },
    {
      "epoch": 0.7131834215167548,
      "grad_norm": 20.673011779785156,
      "learning_rate": 5.736882716049383e-06,
      "loss": 0.1009,
      "step": 25880
    },
    {
      "epoch": 0.7134589947089947,
      "grad_norm": 0.006970008369535208,
      "learning_rate": 5.731371252204586e-06,
      "loss": 0.016,
      "step": 25890
    },
    {
      "epoch": 0.7137345679012346,
      "grad_norm": 0.007010703906416893,
      "learning_rate": 5.725859788359789e-06,
      "loss": 0.0322,
      "step": 25900
    },
    {
      "epoch": 0.7140101410934744,
      "grad_norm": 0.09346353262662888,
      "learning_rate": 5.7203483245149914e-06,
      "loss": 0.2551,
      "step": 25910
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 0.16175490617752075,
      "learning_rate": 5.714836860670195e-06,
      "loss": 0.068,
      "step": 25920
    },
    {
      "epoch": 0.7145612874779541,
      "grad_norm": 0.007500146981328726,
      "learning_rate": 5.709325396825397e-06,
      "loss": 0.0925,
      "step": 25930
    },
    {
      "epoch": 0.714836860670194,
      "grad_norm": 0.09647504985332489,
      "learning_rate": 5.7038139329805995e-06,
      "loss": 0.0896,
      "step": 25940
    },
    {
      "epoch": 0.7151124338624338,
      "grad_norm": 32.76077651977539,
      "learning_rate": 5.698302469135803e-06,
      "loss": 0.0766,
      "step": 25950
    },
    {
      "epoch": 0.7153880070546738,
      "grad_norm": 0.016343755647540092,
      "learning_rate": 5.692791005291006e-06,
      "loss": 0.0501,
      "step": 25960
    },
    {
      "epoch": 0.7156635802469136,
      "grad_norm": 0.01138172298669815,
      "learning_rate": 5.687279541446209e-06,
      "loss": 0.0028,
      "step": 25970
    },
    {
      "epoch": 0.7159391534391535,
      "grad_norm": 0.06029549241065979,
      "learning_rate": 5.681768077601412e-06,
      "loss": 0.0066,
      "step": 25980
    },
    {
      "epoch": 0.7162147266313933,
      "grad_norm": 0.012572776526212692,
      "learning_rate": 5.676256613756614e-06,
      "loss": 0.0006,
      "step": 25990
    },
    {
      "epoch": 0.7164902998236331,
      "grad_norm": 0.054085105657577515,
      "learning_rate": 5.670745149911817e-06,
      "loss": 0.0081,
      "step": 26000
    },
    {
      "epoch": 0.716765873015873,
      "grad_norm": 208.692626953125,
      "learning_rate": 5.66523368606702e-06,
      "loss": 0.0849,
      "step": 26010
    },
    {
      "epoch": 0.7170414462081128,
      "grad_norm": 1.842857837677002,
      "learning_rate": 5.659722222222222e-06,
      "loss": 0.1459,
      "step": 26020
    },
    {
      "epoch": 0.7173170194003528,
      "grad_norm": 0.15554426610469818,
      "learning_rate": 5.6542107583774255e-06,
      "loss": 0.0006,
      "step": 26030
    },
    {
      "epoch": 0.7175925925925926,
      "grad_norm": 0.01071738451719284,
      "learning_rate": 5.648699294532628e-06,
      "loss": 0.1953,
      "step": 26040
    },
    {
      "epoch": 0.7178681657848325,
      "grad_norm": 0.008058133535087109,
      "learning_rate": 5.643187830687831e-06,
      "loss": 0.0212,
      "step": 26050
    },
    {
      "epoch": 0.7181437389770723,
      "grad_norm": 0.009397877380251884,
      "learning_rate": 5.6376763668430335e-06,
      "loss": 0.1829,
      "step": 26060
    },
    {
      "epoch": 0.7184193121693122,
      "grad_norm": 0.026600884273648262,
      "learning_rate": 5.632164902998238e-06,
      "loss": 0.0006,
      "step": 26070
    },
    {
      "epoch": 0.718694885361552,
      "grad_norm": 0.010174153372645378,
      "learning_rate": 5.62665343915344e-06,
      "loss": 0.1146,
      "step": 26080
    },
    {
      "epoch": 0.7189704585537919,
      "grad_norm": 0.07680114358663559,
      "learning_rate": 5.6211419753086425e-06,
      "loss": 0.1169,
      "step": 26090
    },
    {
      "epoch": 0.7192460317460317,
      "grad_norm": 0.013236772269010544,
      "learning_rate": 5.615630511463846e-06,
      "loss": 0.059,
      "step": 26100
    },
    {
      "epoch": 0.7195216049382716,
      "grad_norm": 0.007347441744059324,
      "learning_rate": 5.610119047619048e-06,
      "loss": 0.0398,
      "step": 26110
    },
    {
      "epoch": 0.7197971781305115,
      "grad_norm": 0.008500207215547562,
      "learning_rate": 5.6046075837742505e-06,
      "loss": 0.0007,
      "step": 26120
    },
    {
      "epoch": 0.7200727513227513,
      "grad_norm": 114.95506286621094,
      "learning_rate": 5.599096119929454e-06,
      "loss": 0.2113,
      "step": 26130
    },
    {
      "epoch": 0.7203483245149912,
      "grad_norm": 0.44372472167015076,
      "learning_rate": 5.593584656084656e-06,
      "loss": 0.1188,
      "step": 26140
    },
    {
      "epoch": 0.720623897707231,
      "grad_norm": 0.5423082709312439,
      "learning_rate": 5.588073192239859e-06,
      "loss": 0.0878,
      "step": 26150
    },
    {
      "epoch": 0.7208994708994709,
      "grad_norm": 0.0075822509825229645,
      "learning_rate": 5.582561728395062e-06,
      "loss": 0.0824,
      "step": 26160
    },
    {
      "epoch": 0.7211750440917107,
      "grad_norm": 0.042106784880161285,
      "learning_rate": 5.577050264550265e-06,
      "loss": 0.1041,
      "step": 26170
    },
    {
      "epoch": 0.7214506172839507,
      "grad_norm": 102.21296691894531,
      "learning_rate": 5.571538800705468e-06,
      "loss": 0.2243,
      "step": 26180
    },
    {
      "epoch": 0.7217261904761905,
      "grad_norm": 12.790874481201172,
      "learning_rate": 5.566027336860671e-06,
      "loss": 0.0914,
      "step": 26190
    },
    {
      "epoch": 0.7220017636684304,
      "grad_norm": 29.454057693481445,
      "learning_rate": 5.560515873015874e-06,
      "loss": 0.1726,
      "step": 26200
    },
    {
      "epoch": 0.7222773368606702,
      "grad_norm": 32.25735855102539,
      "learning_rate": 5.5550044091710765e-06,
      "loss": 0.0671,
      "step": 26210
    },
    {
      "epoch": 0.72255291005291,
      "grad_norm": 0.012815074063837528,
      "learning_rate": 5.549492945326279e-06,
      "loss": 0.1336,
      "step": 26220
    },
    {
      "epoch": 0.7228284832451499,
      "grad_norm": 0.45811671018600464,
      "learning_rate": 5.543981481481482e-06,
      "loss": 0.0853,
      "step": 26230
    },
    {
      "epoch": 0.7231040564373897,
      "grad_norm": 81.38363647460938,
      "learning_rate": 5.5384700176366846e-06,
      "loss": 0.1517,
      "step": 26240
    },
    {
      "epoch": 0.7233796296296297,
      "grad_norm": 0.014223371632397175,
      "learning_rate": 5.532958553791887e-06,
      "loss": 0.0173,
      "step": 26250
    },
    {
      "epoch": 0.7236552028218695,
      "grad_norm": 0.04868539422750473,
      "learning_rate": 5.52744708994709e-06,
      "loss": 0.1484,
      "step": 26260
    },
    {
      "epoch": 0.7239307760141094,
      "grad_norm": 0.012413294985890388,
      "learning_rate": 5.521935626102293e-06,
      "loss": 0.1844,
      "step": 26270
    },
    {
      "epoch": 0.7242063492063492,
      "grad_norm": 51.673118591308594,
      "learning_rate": 5.516424162257497e-06,
      "loss": 0.2716,
      "step": 26280
    },
    {
      "epoch": 0.7244819223985891,
      "grad_norm": 0.0681726410984993,
      "learning_rate": 5.510912698412699e-06,
      "loss": 0.0509,
      "step": 26290
    },
    {
      "epoch": 0.7247574955908289,
      "grad_norm": 0.007065221201628447,
      "learning_rate": 5.505401234567902e-06,
      "loss": 0.2226,
      "step": 26300
    },
    {
      "epoch": 0.7250330687830688,
      "grad_norm": 0.1749410182237625,
      "learning_rate": 5.499889770723105e-06,
      "loss": 0.1198,
      "step": 26310
    },
    {
      "epoch": 0.7253086419753086,
      "grad_norm": 0.09022419154644012,
      "learning_rate": 5.494378306878307e-06,
      "loss": 0.1229,
      "step": 26320
    },
    {
      "epoch": 0.7255842151675485,
      "grad_norm": 11.17947006225586,
      "learning_rate": 5.4888668430335105e-06,
      "loss": 0.16,
      "step": 26330
    },
    {
      "epoch": 0.7258597883597884,
      "grad_norm": 0.07748814672231674,
      "learning_rate": 5.483355379188713e-06,
      "loss": 0.0394,
      "step": 26340
    },
    {
      "epoch": 0.7261353615520282,
      "grad_norm": 0.11314312368631363,
      "learning_rate": 5.477843915343915e-06,
      "loss": 0.0266,
      "step": 26350
    },
    {
      "epoch": 0.7264109347442681,
      "grad_norm": 0.012757137417793274,
      "learning_rate": 5.472332451499119e-06,
      "loss": 0.0027,
      "step": 26360
    },
    {
      "epoch": 0.7266865079365079,
      "grad_norm": 0.009335573762655258,
      "learning_rate": 5.466820987654321e-06,
      "loss": 0.0092,
      "step": 26370
    },
    {
      "epoch": 0.7269620811287478,
      "grad_norm": 4.4157843589782715,
      "learning_rate": 5.461309523809523e-06,
      "loss": 0.0923,
      "step": 26380
    },
    {
      "epoch": 0.7272376543209876,
      "grad_norm": 0.010226624086499214,
      "learning_rate": 5.4557980599647275e-06,
      "loss": 0.0511,
      "step": 26390
    },
    {
      "epoch": 0.7275132275132276,
      "grad_norm": 0.010945419780910015,
      "learning_rate": 5.45028659611993e-06,
      "loss": 0.1125,
      "step": 26400
    },
    {
      "epoch": 0.7277888007054674,
      "grad_norm": 0.01244808454066515,
      "learning_rate": 5.444775132275133e-06,
      "loss": 0.0576,
      "step": 26410
    },
    {
      "epoch": 0.7280643738977073,
      "grad_norm": 135.6715545654297,
      "learning_rate": 5.439263668430336e-06,
      "loss": 0.0333,
      "step": 26420
    },
    {
      "epoch": 0.7283399470899471,
      "grad_norm": 0.016997741535305977,
      "learning_rate": 5.433752204585538e-06,
      "loss": 0.0996,
      "step": 26430
    },
    {
      "epoch": 0.7286155202821869,
      "grad_norm": 0.009066401980817318,
      "learning_rate": 5.428240740740741e-06,
      "loss": 0.0854,
      "step": 26440
    },
    {
      "epoch": 0.7288910934744268,
      "grad_norm": 0.017217179760336876,
      "learning_rate": 5.422729276895944e-06,
      "loss": 0.3257,
      "step": 26450
    },
    {
      "epoch": 0.7291666666666666,
      "grad_norm": 0.009657076559960842,
      "learning_rate": 5.417217813051147e-06,
      "loss": 0.0074,
      "step": 26460
    },
    {
      "epoch": 0.7294422398589065,
      "grad_norm": 0.014003098011016846,
      "learning_rate": 5.411706349206349e-06,
      "loss": 0.0011,
      "step": 26470
    },
    {
      "epoch": 0.7297178130511464,
      "grad_norm": 0.6140738129615784,
      "learning_rate": 5.406194885361552e-06,
      "loss": 0.1172,
      "step": 26480
    },
    {
      "epoch": 0.7299933862433863,
      "grad_norm": 1.4066129922866821,
      "learning_rate": 5.400683421516756e-06,
      "loss": 0.09,
      "step": 26490
    },
    {
      "epoch": 0.7302689594356261,
      "grad_norm": 0.0066889310255646706,
      "learning_rate": 5.395171957671958e-06,
      "loss": 0.1159,
      "step": 26500
    },
    {
      "epoch": 0.730544532627866,
      "grad_norm": 33.56494903564453,
      "learning_rate": 5.3896604938271615e-06,
      "loss": 0.0024,
      "step": 26510
    },
    {
      "epoch": 0.7308201058201058,
      "grad_norm": 176.563232421875,
      "learning_rate": 5.384149029982364e-06,
      "loss": 0.0485,
      "step": 26520
    },
    {
      "epoch": 0.7310956790123457,
      "grad_norm": 4.391957759857178,
      "learning_rate": 5.378637566137566e-06,
      "loss": 0.4066,
      "step": 26530
    },
    {
      "epoch": 0.7313712522045855,
      "grad_norm": 0.40700191259384155,
      "learning_rate": 5.37312610229277e-06,
      "loss": 0.2726,
      "step": 26540
    },
    {
      "epoch": 0.7316468253968254,
      "grad_norm": 0.00866501685231924,
      "learning_rate": 5.367614638447972e-06,
      "loss": 0.0181,
      "step": 26550
    },
    {
      "epoch": 0.7319223985890653,
      "grad_norm": 0.009430388920009136,
      "learning_rate": 5.3621031746031744e-06,
      "loss": 0.1979,
      "step": 26560
    },
    {
      "epoch": 0.7321979717813051,
      "grad_norm": 276.4803466796875,
      "learning_rate": 5.356591710758378e-06,
      "loss": 0.1466,
      "step": 26570
    },
    {
      "epoch": 0.732473544973545,
      "grad_norm": 146.758544921875,
      "learning_rate": 5.35108024691358e-06,
      "loss": 0.242,
      "step": 26580
    },
    {
      "epoch": 0.7327491181657848,
      "grad_norm": 0.02287629246711731,
      "learning_rate": 5.345568783068783e-06,
      "loss": 0.0009,
      "step": 26590
    },
    {
      "epoch": 0.7330246913580247,
      "grad_norm": 0.06465166807174683,
      "learning_rate": 5.340057319223987e-06,
      "loss": 0.1607,
      "step": 26600
    },
    {
      "epoch": 0.7333002645502645,
      "grad_norm": 0.04089635610580444,
      "learning_rate": 5.33454585537919e-06,
      "loss": 0.0006,
      "step": 26610
    },
    {
      "epoch": 0.7335758377425045,
      "grad_norm": 0.010810631327331066,
      "learning_rate": 5.329034391534392e-06,
      "loss": 0.0007,
      "step": 26620
    },
    {
      "epoch": 0.7338514109347443,
      "grad_norm": 169.09397888183594,
      "learning_rate": 5.323522927689595e-06,
      "loss": 0.2158,
      "step": 26630
    },
    {
      "epoch": 0.7341269841269841,
      "grad_norm": 91.26085662841797,
      "learning_rate": 5.318011463844798e-06,
      "loss": 0.0551,
      "step": 26640
    },
    {
      "epoch": 0.734402557319224,
      "grad_norm": 0.5465455055236816,
      "learning_rate": 5.3125e-06,
      "loss": 0.0156,
      "step": 26650
    },
    {
      "epoch": 0.7346781305114638,
      "grad_norm": 127.0729751586914,
      "learning_rate": 5.306988536155203e-06,
      "loss": 0.045,
      "step": 26660
    },
    {
      "epoch": 0.7349537037037037,
      "grad_norm": 0.036454617977142334,
      "learning_rate": 5.301477072310406e-06,
      "loss": 0.0415,
      "step": 26670
    },
    {
      "epoch": 0.7352292768959435,
      "grad_norm": 4.229168891906738,
      "learning_rate": 5.2959656084656085e-06,
      "loss": 0.001,
      "step": 26680
    },
    {
      "epoch": 0.7355048500881834,
      "grad_norm": 0.05668887123465538,
      "learning_rate": 5.290454144620812e-06,
      "loss": 0.0019,
      "step": 26690
    },
    {
      "epoch": 0.7357804232804233,
      "grad_norm": 68.67180633544922,
      "learning_rate": 5.284942680776014e-06,
      "loss": 0.2654,
      "step": 26700
    },
    {
      "epoch": 0.7360559964726632,
      "grad_norm": 0.10780580341815948,
      "learning_rate": 5.279431216931218e-06,
      "loss": 0.0029,
      "step": 26710
    },
    {
      "epoch": 0.736331569664903,
      "grad_norm": 0.7720962762832642,
      "learning_rate": 5.273919753086421e-06,
      "loss": 0.1176,
      "step": 26720
    },
    {
      "epoch": 0.7366071428571429,
      "grad_norm": 0.007839766331017017,
      "learning_rate": 5.268408289241623e-06,
      "loss": 0.1514,
      "step": 26730
    },
    {
      "epoch": 0.7368827160493827,
      "grad_norm": 4.9280805587768555,
      "learning_rate": 5.262896825396826e-06,
      "loss": 0.2603,
      "step": 26740
    },
    {
      "epoch": 0.7371582892416225,
      "grad_norm": 35.939571380615234,
      "learning_rate": 5.257385361552029e-06,
      "loss": 0.1578,
      "step": 26750
    },
    {
      "epoch": 0.7374338624338624,
      "grad_norm": 0.05668122321367264,
      "learning_rate": 5.251873897707231e-06,
      "loss": 0.202,
      "step": 26760
    },
    {
      "epoch": 0.7377094356261023,
      "grad_norm": 0.015194099396467209,
      "learning_rate": 5.246362433862434e-06,
      "loss": 0.0883,
      "step": 26770
    },
    {
      "epoch": 0.7379850088183422,
      "grad_norm": 0.02075672708451748,
      "learning_rate": 5.240850970017637e-06,
      "loss": 0.1176,
      "step": 26780
    },
    {
      "epoch": 0.738260582010582,
      "grad_norm": 0.024190234020352364,
      "learning_rate": 5.235339506172839e-06,
      "loss": 0.1723,
      "step": 26790
    },
    {
      "epoch": 0.7385361552028219,
      "grad_norm": 0.020180409774184227,
      "learning_rate": 5.2298280423280425e-06,
      "loss": 0.0137,
      "step": 26800
    },
    {
      "epoch": 0.7388117283950617,
      "grad_norm": 146.57012939453125,
      "learning_rate": 5.224316578483245e-06,
      "loss": 0.0672,
      "step": 26810
    },
    {
      "epoch": 0.7390873015873016,
      "grad_norm": 0.11272729933261871,
      "learning_rate": 5.218805114638449e-06,
      "loss": 0.0006,
      "step": 26820
    },
    {
      "epoch": 0.7393628747795414,
      "grad_norm": 0.01457610446959734,
      "learning_rate": 5.213293650793651e-06,
      "loss": 0.3961,
      "step": 26830
    },
    {
      "epoch": 0.7396384479717814,
      "grad_norm": 0.10672981292009354,
      "learning_rate": 5.207782186948855e-06,
      "loss": 0.1474,
      "step": 26840
    },
    {
      "epoch": 0.7399140211640212,
      "grad_norm": 19.769811630249023,
      "learning_rate": 5.202270723104057e-06,
      "loss": 0.3353,
      "step": 26850
    },
    {
      "epoch": 0.740189594356261,
      "grad_norm": 0.8650919795036316,
      "learning_rate": 5.1967592592592595e-06,
      "loss": 0.1715,
      "step": 26860
    },
    {
      "epoch": 0.7404651675485009,
      "grad_norm": 0.012705684639513493,
      "learning_rate": 5.191247795414463e-06,
      "loss": 0.0853,
      "step": 26870
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 0.011472613550722599,
      "learning_rate": 5.185736331569665e-06,
      "loss": 0.0987,
      "step": 26880
    },
    {
      "epoch": 0.7410163139329806,
      "grad_norm": 0.8976831436157227,
      "learning_rate": 5.1802248677248676e-06,
      "loss": 0.1918,
      "step": 26890
    },
    {
      "epoch": 0.7412918871252204,
      "grad_norm": 0.24388428032398224,
      "learning_rate": 5.174713403880071e-06,
      "loss": 0.0087,
      "step": 26900
    },
    {
      "epoch": 0.7415674603174603,
      "grad_norm": 11.381996154785156,
      "learning_rate": 5.169201940035273e-06,
      "loss": 0.1334,
      "step": 26910
    },
    {
      "epoch": 0.7418430335097002,
      "grad_norm": 96.83391571044922,
      "learning_rate": 5.163690476190477e-06,
      "loss": 0.0298,
      "step": 26920
    },
    {
      "epoch": 0.7421186067019401,
      "grad_norm": 0.011150847189128399,
      "learning_rate": 5.15817901234568e-06,
      "loss": 0.0037,
      "step": 26930
    },
    {
      "epoch": 0.7423941798941799,
      "grad_norm": 28.08852195739746,
      "learning_rate": 5.152667548500882e-06,
      "loss": 0.1814,
      "step": 26940
    },
    {
      "epoch": 0.7426697530864198,
      "grad_norm": 0.039150819182395935,
      "learning_rate": 5.1471560846560854e-06,
      "loss": 0.0574,
      "step": 26950
    },
    {
      "epoch": 0.7429453262786596,
      "grad_norm": 0.47267845273017883,
      "learning_rate": 5.141644620811288e-06,
      "loss": 0.0015,
      "step": 26960
    },
    {
      "epoch": 0.7432208994708994,
      "grad_norm": 0.00850011222064495,
      "learning_rate": 5.136133156966491e-06,
      "loss": 0.0464,
      "step": 26970
    },
    {
      "epoch": 0.7434964726631393,
      "grad_norm": 4.654575347900391,
      "learning_rate": 5.1306216931216935e-06,
      "loss": 0.013,
      "step": 26980
    },
    {
      "epoch": 0.7437720458553791,
      "grad_norm": 0.010195254348218441,
      "learning_rate": 5.125110229276896e-06,
      "loss": 0.0155,
      "step": 26990
    },
    {
      "epoch": 0.7440476190476191,
      "grad_norm": 0.015425650402903557,
      "learning_rate": 5.119598765432099e-06,
      "loss": 0.1173,
      "step": 27000
    },
    {
      "epoch": 0.7443231922398589,
      "grad_norm": 0.020597046241164207,
      "learning_rate": 5.114087301587302e-06,
      "loss": 0.0325,
      "step": 27010
    },
    {
      "epoch": 0.7445987654320988,
      "grad_norm": 0.06719103455543518,
      "learning_rate": 5.108575837742504e-06,
      "loss": 0.0006,
      "step": 27020
    },
    {
      "epoch": 0.7448743386243386,
      "grad_norm": 0.015530744567513466,
      "learning_rate": 5.103064373897708e-06,
      "loss": 0.1229,
      "step": 27030
    },
    {
      "epoch": 0.7451499118165785,
      "grad_norm": 0.027815639972686768,
      "learning_rate": 5.0975529100529105e-06,
      "loss": 0.1193,
      "step": 27040
    },
    {
      "epoch": 0.7454254850088183,
      "grad_norm": 0.08832460641860962,
      "learning_rate": 5.092041446208114e-06,
      "loss": 0.2386,
      "step": 27050
    },
    {
      "epoch": 0.7457010582010583,
      "grad_norm": 128.69570922851562,
      "learning_rate": 5.086529982363316e-06,
      "loss": 0.1851,
      "step": 27060
    },
    {
      "epoch": 0.7459766313932981,
      "grad_norm": 0.00897875428199768,
      "learning_rate": 5.081018518518519e-06,
      "loss": 0.042,
      "step": 27070
    },
    {
      "epoch": 0.7462522045855379,
      "grad_norm": 0.017833780497312546,
      "learning_rate": 5.075507054673722e-06,
      "loss": 0.1005,
      "step": 27080
    },
    {
      "epoch": 0.7465277777777778,
      "grad_norm": 80.63008880615234,
      "learning_rate": 5.069995590828924e-06,
      "loss": 0.0669,
      "step": 27090
    },
    {
      "epoch": 0.7468033509700176,
      "grad_norm": 0.017347104847431183,
      "learning_rate": 5.0644841269841275e-06,
      "loss": 0.0223,
      "step": 27100
    },
    {
      "epoch": 0.7470789241622575,
      "grad_norm": 0.008070936426520348,
      "learning_rate": 5.05897266313933e-06,
      "loss": 0.1855,
      "step": 27110
    },
    {
      "epoch": 0.7473544973544973,
      "grad_norm": 0.03641942888498306,
      "learning_rate": 5.053461199294532e-06,
      "loss": 0.0792,
      "step": 27120
    },
    {
      "epoch": 0.7476300705467372,
      "grad_norm": 17.04848861694336,
      "learning_rate": 5.047949735449736e-06,
      "loss": 0.1501,
      "step": 27130
    },
    {
      "epoch": 0.747905643738977,
      "grad_norm": 0.07461213320493698,
      "learning_rate": 5.042438271604939e-06,
      "loss": 0.09,
      "step": 27140
    },
    {
      "epoch": 0.748181216931217,
      "grad_norm": 39.04463577270508,
      "learning_rate": 5.036926807760142e-06,
      "loss": 0.0699,
      "step": 27150
    },
    {
      "epoch": 0.7484567901234568,
      "grad_norm": 26.41646957397461,
      "learning_rate": 5.0314153439153445e-06,
      "loss": 0.1273,
      "step": 27160
    },
    {
      "epoch": 0.7487323633156967,
      "grad_norm": 0.2063855230808258,
      "learning_rate": 5.025903880070547e-06,
      "loss": 0.0282,
      "step": 27170
    },
    {
      "epoch": 0.7490079365079365,
      "grad_norm": 0.007813720032572746,
      "learning_rate": 5.02039241622575e-06,
      "loss": 0.0134,
      "step": 27180
    },
    {
      "epoch": 0.7492835097001763,
      "grad_norm": 0.009242582134902477,
      "learning_rate": 5.014880952380953e-06,
      "loss": 0.0531,
      "step": 27190
    },
    {
      "epoch": 0.7495590828924162,
      "grad_norm": 0.011497482657432556,
      "learning_rate": 5.009369488536155e-06,
      "loss": 0.001,
      "step": 27200
    },
    {
      "epoch": 0.749834656084656,
      "grad_norm": 9.014826774597168,
      "learning_rate": 5.003858024691358e-06,
      "loss": 0.0016,
      "step": 27210
    },
    {
      "epoch": 0.750110229276896,
      "grad_norm": 14.361109733581543,
      "learning_rate": 4.9983465608465616e-06,
      "loss": 0.2543,
      "step": 27220
    },
    {
      "epoch": 0.7503858024691358,
      "grad_norm": 0.017889617010951042,
      "learning_rate": 4.992835097001764e-06,
      "loss": 0.0026,
      "step": 27230
    },
    {
      "epoch": 0.7506613756613757,
      "grad_norm": 0.00914801750332117,
      "learning_rate": 4.987323633156967e-06,
      "loss": 0.0975,
      "step": 27240
    },
    {
      "epoch": 0.7509369488536155,
      "grad_norm": 0.30556532740592957,
      "learning_rate": 4.98181216931217e-06,
      "loss": 0.0719,
      "step": 27250
    },
    {
      "epoch": 0.7512125220458554,
      "grad_norm": 0.008768511936068535,
      "learning_rate": 4.976300705467372e-06,
      "loss": 0.108,
      "step": 27260
    },
    {
      "epoch": 0.7514880952380952,
      "grad_norm": 0.011373499408364296,
      "learning_rate": 4.970789241622575e-06,
      "loss": 0.1487,
      "step": 27270
    },
    {
      "epoch": 0.7517636684303352,
      "grad_norm": 0.03714817762374878,
      "learning_rate": 4.9652777777777786e-06,
      "loss": 0.0573,
      "step": 27280
    },
    {
      "epoch": 0.752039241622575,
      "grad_norm": 0.7121070027351379,
      "learning_rate": 4.959766313932981e-06,
      "loss": 0.001,
      "step": 27290
    },
    {
      "epoch": 0.7523148148148148,
      "grad_norm": 0.014668573625385761,
      "learning_rate": 4.954254850088183e-06,
      "loss": 0.0006,
      "step": 27300
    },
    {
      "epoch": 0.7525903880070547,
      "grad_norm": 0.009938913397490978,
      "learning_rate": 4.948743386243387e-06,
      "loss": 0.0265,
      "step": 27310
    },
    {
      "epoch": 0.7528659611992945,
      "grad_norm": 0.009109732694923878,
      "learning_rate": 4.94323192239859e-06,
      "loss": 0.1017,
      "step": 27320
    },
    {
      "epoch": 0.7531415343915344,
      "grad_norm": 0.026415182277560234,
      "learning_rate": 4.937720458553792e-06,
      "loss": 0.1046,
      "step": 27330
    },
    {
      "epoch": 0.7534171075837742,
      "grad_norm": 0.014092186465859413,
      "learning_rate": 4.932208994708996e-06,
      "loss": 0.0008,
      "step": 27340
    },
    {
      "epoch": 0.7536926807760141,
      "grad_norm": 0.2573002874851227,
      "learning_rate": 4.926697530864198e-06,
      "loss": 0.045,
      "step": 27350
    },
    {
      "epoch": 0.753968253968254,
      "grad_norm": 0.05890819802880287,
      "learning_rate": 4.9211860670194e-06,
      "loss": 0.0096,
      "step": 27360
    },
    {
      "epoch": 0.7542438271604939,
      "grad_norm": 0.0144279720261693,
      "learning_rate": 4.915674603174604e-06,
      "loss": 0.0499,
      "step": 27370
    },
    {
      "epoch": 0.7545194003527337,
      "grad_norm": 0.12208374589681625,
      "learning_rate": 4.910163139329807e-06,
      "loss": 0.0308,
      "step": 27380
    },
    {
      "epoch": 0.7547949735449735,
      "grad_norm": 0.015395434573292732,
      "learning_rate": 4.904651675485009e-06,
      "loss": 0.0854,
      "step": 27390
    },
    {
      "epoch": 0.7550705467372134,
      "grad_norm": 0.034781791269779205,
      "learning_rate": 4.899140211640212e-06,
      "loss": 0.251,
      "step": 27400
    },
    {
      "epoch": 0.7553461199294532,
      "grad_norm": 0.1316349357366562,
      "learning_rate": 4.893628747795415e-06,
      "loss": 0.0775,
      "step": 27410
    },
    {
      "epoch": 0.7556216931216931,
      "grad_norm": 0.007825498469173908,
      "learning_rate": 4.888117283950617e-06,
      "loss": 0.3378,
      "step": 27420
    },
    {
      "epoch": 0.755897266313933,
      "grad_norm": 0.014052445068955421,
      "learning_rate": 4.882605820105821e-06,
      "loss": 0.0009,
      "step": 27430
    },
    {
      "epoch": 0.7561728395061729,
      "grad_norm": 19.419767379760742,
      "learning_rate": 4.877094356261023e-06,
      "loss": 0.1039,
      "step": 27440
    },
    {
      "epoch": 0.7564484126984127,
      "grad_norm": 0.011309163644909859,
      "learning_rate": 4.871582892416226e-06,
      "loss": 0.1578,
      "step": 27450
    },
    {
      "epoch": 0.7567239858906526,
      "grad_norm": 147.9769287109375,
      "learning_rate": 4.866071428571429e-06,
      "loss": 0.1367,
      "step": 27460
    },
    {
      "epoch": 0.7569995590828924,
      "grad_norm": 0.04360151290893555,
      "learning_rate": 4.860559964726632e-06,
      "loss": 0.0017,
      "step": 27470
    },
    {
      "epoch": 0.7572751322751323,
      "grad_norm": 0.21461543440818787,
      "learning_rate": 4.855048500881835e-06,
      "loss": 0.0143,
      "step": 27480
    },
    {
      "epoch": 0.7575507054673721,
      "grad_norm": 0.012914882972836494,
      "learning_rate": 4.849537037037038e-06,
      "loss": 0.1047,
      "step": 27490
    },
    {
      "epoch": 0.7578262786596119,
      "grad_norm": 0.010510330088436604,
      "learning_rate": 4.84402557319224e-06,
      "loss": 0.0467,
      "step": 27500
    },
    {
      "epoch": 0.7581018518518519,
      "grad_norm": 1.8852057456970215,
      "learning_rate": 4.838514109347443e-06,
      "loss": 0.147,
      "step": 27510
    },
    {
      "epoch": 0.7583774250440917,
      "grad_norm": 149.09156799316406,
      "learning_rate": 4.833002645502646e-06,
      "loss": 0.037,
      "step": 27520
    },
    {
      "epoch": 0.7586529982363316,
      "grad_norm": 0.1105884239077568,
      "learning_rate": 4.827491181657848e-06,
      "loss": 0.0828,
      "step": 27530
    },
    {
      "epoch": 0.7589285714285714,
      "grad_norm": 0.34623003005981445,
      "learning_rate": 4.8219797178130514e-06,
      "loss": 0.0021,
      "step": 27540
    },
    {
      "epoch": 0.7592041446208113,
      "grad_norm": 0.07178271561861038,
      "learning_rate": 4.816468253968255e-06,
      "loss": 0.0879,
      "step": 27550
    },
    {
      "epoch": 0.7594797178130511,
      "grad_norm": 0.009594416245818138,
      "learning_rate": 4.810956790123457e-06,
      "loss": 0.0005,
      "step": 27560
    },
    {
      "epoch": 0.759755291005291,
      "grad_norm": 0.008839539252221584,
      "learning_rate": 4.8054453262786595e-06,
      "loss": 0.1871,
      "step": 27570
    },
    {
      "epoch": 0.7600308641975309,
      "grad_norm": 0.017814284190535545,
      "learning_rate": 4.799933862433863e-06,
      "loss": 0.0006,
      "step": 27580
    },
    {
      "epoch": 0.7603064373897708,
      "grad_norm": 0.008246664889156818,
      "learning_rate": 4.794422398589066e-06,
      "loss": 0.0008,
      "step": 27590
    },
    {
      "epoch": 0.7605820105820106,
      "grad_norm": 244.44264221191406,
      "learning_rate": 4.7889109347442684e-06,
      "loss": 0.1013,
      "step": 27600
    },
    {
      "epoch": 0.7608575837742504,
      "grad_norm": 0.04898681119084358,
      "learning_rate": 4.783399470899472e-06,
      "loss": 0.2527,
      "step": 27610
    },
    {
      "epoch": 0.7611331569664903,
      "grad_norm": 0.03277682885527611,
      "learning_rate": 4.777888007054674e-06,
      "loss": 0.0023,
      "step": 27620
    },
    {
      "epoch": 0.7614087301587301,
      "grad_norm": 0.009554026648402214,
      "learning_rate": 4.7723765432098765e-06,
      "loss": 0.2222,
      "step": 27630
    },
    {
      "epoch": 0.76168430335097,
      "grad_norm": 113.0984878540039,
      "learning_rate": 4.76686507936508e-06,
      "loss": 0.0659,
      "step": 27640
    },
    {
      "epoch": 0.7619598765432098,
      "grad_norm": 0.02717515453696251,
      "learning_rate": 4.761353615520283e-06,
      "loss": 0.1687,
      "step": 27650
    },
    {
      "epoch": 0.7622354497354498,
      "grad_norm": 0.026008620858192444,
      "learning_rate": 4.7558421516754855e-06,
      "loss": 0.0009,
      "step": 27660
    },
    {
      "epoch": 0.7625110229276896,
      "grad_norm": 0.011668117716908455,
      "learning_rate": 4.750330687830688e-06,
      "loss": 0.0085,
      "step": 27670
    },
    {
      "epoch": 0.7627865961199295,
      "grad_norm": 214.63888549804688,
      "learning_rate": 4.744819223985891e-06,
      "loss": 0.0634,
      "step": 27680
    },
    {
      "epoch": 0.7630621693121693,
      "grad_norm": 0.02101580798625946,
      "learning_rate": 4.7393077601410935e-06,
      "loss": 0.1104,
      "step": 27690
    },
    {
      "epoch": 0.7633377425044092,
      "grad_norm": 0.046682387590408325,
      "learning_rate": 4.733796296296297e-06,
      "loss": 0.0007,
      "step": 27700
    },
    {
      "epoch": 0.763613315696649,
      "grad_norm": 0.010805312544107437,
      "learning_rate": 4.728284832451499e-06,
      "loss": 0.0603,
      "step": 27710
    },
    {
      "epoch": 0.7638888888888888,
      "grad_norm": 3.131364583969116,
      "learning_rate": 4.7227733686067025e-06,
      "loss": 0.0008,
      "step": 27720
    },
    {
      "epoch": 0.7641644620811288,
      "grad_norm": 0.06479589641094208,
      "learning_rate": 4.717261904761905e-06,
      "loss": 0.0871,
      "step": 27730
    },
    {
      "epoch": 0.7644400352733686,
      "grad_norm": 0.02214154042303562,
      "learning_rate": 4.711750440917108e-06,
      "loss": 0.0876,
      "step": 27740
    },
    {
      "epoch": 0.7647156084656085,
      "grad_norm": 78.67908477783203,
      "learning_rate": 4.706238977072311e-06,
      "loss": 0.2591,
      "step": 27750
    },
    {
      "epoch": 0.7649911816578483,
      "grad_norm": 14.46987247467041,
      "learning_rate": 4.700727513227514e-06,
      "loss": 0.1249,
      "step": 27760
    },
    {
      "epoch": 0.7652667548500882,
      "grad_norm": 0.03394963592290878,
      "learning_rate": 4.695216049382716e-06,
      "loss": 0.2348,
      "step": 27770
    },
    {
      "epoch": 0.765542328042328,
      "grad_norm": 0.01416437141597271,
      "learning_rate": 4.6897045855379195e-06,
      "loss": 0.0796,
      "step": 27780
    },
    {
      "epoch": 0.7658179012345679,
      "grad_norm": 0.009327679872512817,
      "learning_rate": 4.684193121693122e-06,
      "loss": 0.0782,
      "step": 27790
    },
    {
      "epoch": 0.7660934744268078,
      "grad_norm": 0.015540095046162605,
      "learning_rate": 4.678681657848324e-06,
      "loss": 0.0836,
      "step": 27800
    },
    {
      "epoch": 0.7663690476190477,
      "grad_norm": 3.6214938163757324,
      "learning_rate": 4.6731701940035276e-06,
      "loss": 0.0868,
      "step": 27810
    },
    {
      "epoch": 0.7666446208112875,
      "grad_norm": 0.021617475897073746,
      "learning_rate": 4.667658730158731e-06,
      "loss": 0.1043,
      "step": 27820
    },
    {
      "epoch": 0.7669201940035273,
      "grad_norm": 0.00828920304775238,
      "learning_rate": 4.662147266313933e-06,
      "loss": 0.0048,
      "step": 27830
    },
    {
      "epoch": 0.7671957671957672,
      "grad_norm": 0.09896969795227051,
      "learning_rate": 4.656635802469136e-06,
      "loss": 0.004,
      "step": 27840
    },
    {
      "epoch": 0.767471340388007,
      "grad_norm": 0.011585785076022148,
      "learning_rate": 4.651124338624339e-06,
      "loss": 0.1406,
      "step": 27850
    },
    {
      "epoch": 0.7677469135802469,
      "grad_norm": 0.02267814427614212,
      "learning_rate": 4.645612874779542e-06,
      "loss": 0.1485,
      "step": 27860
    },
    {
      "epoch": 0.7680224867724867,
      "grad_norm": 0.13934558629989624,
      "learning_rate": 4.6401014109347446e-06,
      "loss": 0.0902,
      "step": 27870
    },
    {
      "epoch": 0.7682980599647267,
      "grad_norm": 0.04983494430780411,
      "learning_rate": 4.634589947089948e-06,
      "loss": 0.1811,
      "step": 27880
    },
    {
      "epoch": 0.7685736331569665,
      "grad_norm": 128.26321411132812,
      "learning_rate": 4.62907848324515e-06,
      "loss": 0.0571,
      "step": 27890
    },
    {
      "epoch": 0.7688492063492064,
      "grad_norm": 0.5302504301071167,
      "learning_rate": 4.623567019400353e-06,
      "loss": 0.0099,
      "step": 27900
    },
    {
      "epoch": 0.7691247795414462,
      "grad_norm": 126.21845245361328,
      "learning_rate": 4.618055555555556e-06,
      "loss": 0.014,
      "step": 27910
    },
    {
      "epoch": 0.7694003527336861,
      "grad_norm": 77.7208480834961,
      "learning_rate": 4.612544091710759e-06,
      "loss": 0.007,
      "step": 27920
    },
    {
      "epoch": 0.7696759259259259,
      "grad_norm": 47.48902893066406,
      "learning_rate": 4.6070326278659616e-06,
      "loss": 0.0802,
      "step": 27930
    },
    {
      "epoch": 0.7699514991181657,
      "grad_norm": 0.014266878366470337,
      "learning_rate": 4.601521164021164e-06,
      "loss": 0.1194,
      "step": 27940
    },
    {
      "epoch": 0.7702270723104057,
      "grad_norm": 0.015342424623668194,
      "learning_rate": 4.596009700176367e-06,
      "loss": 0.0048,
      "step": 27950
    },
    {
      "epoch": 0.7705026455026455,
      "grad_norm": 0.027312710881233215,
      "learning_rate": 4.59049823633157e-06,
      "loss": 0.2245,
      "step": 27960
    },
    {
      "epoch": 0.7707782186948854,
      "grad_norm": 17.145715713500977,
      "learning_rate": 4.584986772486773e-06,
      "loss": 0.0892,
      "step": 27970
    },
    {
      "epoch": 0.7710537918871252,
      "grad_norm": 0.5962693095207214,
      "learning_rate": 4.579475308641976e-06,
      "loss": 0.1016,
      "step": 27980
    },
    {
      "epoch": 0.7713293650793651,
      "grad_norm": 41.86701202392578,
      "learning_rate": 4.573963844797179e-06,
      "loss": 0.0806,
      "step": 27990
    },
    {
      "epoch": 0.7716049382716049,
      "grad_norm": 0.04131355881690979,
      "learning_rate": 4.568452380952381e-06,
      "loss": 0.1566,
      "step": 28000
    },
    {
      "epoch": 0.7718805114638448,
      "grad_norm": 0.29402297735214233,
      "learning_rate": 4.562940917107584e-06,
      "loss": 0.036,
      "step": 28010
    },
    {
      "epoch": 0.7721560846560847,
      "grad_norm": 0.011081663891673088,
      "learning_rate": 4.5574294532627875e-06,
      "loss": 0.0666,
      "step": 28020
    },
    {
      "epoch": 0.7724316578483245,
      "grad_norm": 0.024179767817258835,
      "learning_rate": 4.55191798941799e-06,
      "loss": 0.0006,
      "step": 28030
    },
    {
      "epoch": 0.7727072310405644,
      "grad_norm": 0.007978284731507301,
      "learning_rate": 4.546406525573192e-06,
      "loss": 0.0064,
      "step": 28040
    },
    {
      "epoch": 0.7729828042328042,
      "grad_norm": 0.18055690824985504,
      "learning_rate": 4.540895061728396e-06,
      "loss": 0.0014,
      "step": 28050
    },
    {
      "epoch": 0.7732583774250441,
      "grad_norm": 0.006899564526975155,
      "learning_rate": 4.535383597883598e-06,
      "loss": 0.1533,
      "step": 28060
    },
    {
      "epoch": 0.7735339506172839,
      "grad_norm": 0.008241221308708191,
      "learning_rate": 4.529872134038801e-06,
      "loss": 0.0007,
      "step": 28070
    },
    {
      "epoch": 0.7738095238095238,
      "grad_norm": 0.04671906307339668,
      "learning_rate": 4.524360670194004e-06,
      "loss": 0.073,
      "step": 28080
    },
    {
      "epoch": 0.7740850970017636,
      "grad_norm": 0.039177265018224716,
      "learning_rate": 4.518849206349207e-06,
      "loss": 0.2079,
      "step": 28090
    },
    {
      "epoch": 0.7743606701940036,
      "grad_norm": 0.12275117635726929,
      "learning_rate": 4.513337742504409e-06,
      "loss": 0.0702,
      "step": 28100
    },
    {
      "epoch": 0.7746362433862434,
      "grad_norm": 0.015500680543482304,
      "learning_rate": 4.507826278659613e-06,
      "loss": 0.1487,
      "step": 28110
    },
    {
      "epoch": 0.7749118165784833,
      "grad_norm": 0.23646388947963715,
      "learning_rate": 4.502314814814815e-06,
      "loss": 0.016,
      "step": 28120
    },
    {
      "epoch": 0.7751873897707231,
      "grad_norm": 0.4470802843570709,
      "learning_rate": 4.496803350970018e-06,
      "loss": 0.0554,
      "step": 28130
    },
    {
      "epoch": 0.7754629629629629,
      "grad_norm": 0.03192317858338356,
      "learning_rate": 4.491291887125221e-06,
      "loss": 0.0817,
      "step": 28140
    },
    {
      "epoch": 0.7757385361552028,
      "grad_norm": 5.437849998474121,
      "learning_rate": 4.485780423280424e-06,
      "loss": 0.0959,
      "step": 28150
    },
    {
      "epoch": 0.7760141093474426,
      "grad_norm": 0.04556557163596153,
      "learning_rate": 4.480268959435626e-06,
      "loss": 0.0015,
      "step": 28160
    },
    {
      "epoch": 0.7762896825396826,
      "grad_norm": 0.2992737591266632,
      "learning_rate": 4.474757495590829e-06,
      "loss": 0.2025,
      "step": 28170
    },
    {
      "epoch": 0.7765652557319224,
      "grad_norm": 14.99229621887207,
      "learning_rate": 4.469246031746032e-06,
      "loss": 0.1091,
      "step": 28180
    },
    {
      "epoch": 0.7768408289241623,
      "grad_norm": 0.007726965006440878,
      "learning_rate": 4.463734567901235e-06,
      "loss": 0.0006,
      "step": 28190
    },
    {
      "epoch": 0.7771164021164021,
      "grad_norm": 0.010860965587198734,
      "learning_rate": 4.458223104056438e-06,
      "loss": 0.1061,
      "step": 28200
    },
    {
      "epoch": 0.777391975308642,
      "grad_norm": 0.36755135655403137,
      "learning_rate": 4.45271164021164e-06,
      "loss": 0.164,
      "step": 28210
    },
    {
      "epoch": 0.7776675485008818,
      "grad_norm": 0.1332434117794037,
      "learning_rate": 4.447200176366843e-06,
      "loss": 0.0647,
      "step": 28220
    },
    {
      "epoch": 0.7779431216931217,
      "grad_norm": 1.2839089632034302,
      "learning_rate": 4.441688712522047e-06,
      "loss": 0.0582,
      "step": 28230
    },
    {
      "epoch": 0.7782186948853616,
      "grad_norm": 0.29989996552467346,
      "learning_rate": 4.436177248677249e-06,
      "loss": 0.0927,
      "step": 28240
    },
    {
      "epoch": 0.7784942680776014,
      "grad_norm": 0.012439580634236336,
      "learning_rate": 4.430665784832452e-06,
      "loss": 0.0339,
      "step": 28250
    },
    {
      "epoch": 0.7787698412698413,
      "grad_norm": 0.02932334877550602,
      "learning_rate": 4.425154320987655e-06,
      "loss": 0.3339,
      "step": 28260
    },
    {
      "epoch": 0.7790454144620811,
      "grad_norm": 0.06318388134241104,
      "learning_rate": 4.419642857142857e-06,
      "loss": 0.0005,
      "step": 28270
    },
    {
      "epoch": 0.779320987654321,
      "grad_norm": 0.015112476423382759,
      "learning_rate": 4.41413139329806e-06,
      "loss": 0.0816,
      "step": 28280
    },
    {
      "epoch": 0.7795965608465608,
      "grad_norm": 64.02616119384766,
      "learning_rate": 4.408619929453264e-06,
      "loss": 0.2005,
      "step": 28290
    },
    {
      "epoch": 0.7798721340388007,
      "grad_norm": 0.024604644626379013,
      "learning_rate": 4.403108465608466e-06,
      "loss": 0.201,
      "step": 28300
    },
    {
      "epoch": 0.7801477072310405,
      "grad_norm": 27.853260040283203,
      "learning_rate": 4.3975970017636685e-06,
      "loss": 0.0974,
      "step": 28310
    },
    {
      "epoch": 0.7804232804232805,
      "grad_norm": 0.02005724236369133,
      "learning_rate": 4.392085537918872e-06,
      "loss": 0.0007,
      "step": 28320
    },
    {
      "epoch": 0.7806988536155203,
      "grad_norm": 0.11213182657957077,
      "learning_rate": 4.386574074074074e-06,
      "loss": 0.0684,
      "step": 28330
    },
    {
      "epoch": 0.7809744268077602,
      "grad_norm": 0.23287008702754974,
      "learning_rate": 4.381062610229277e-06,
      "loss": 0.0102,
      "step": 28340
    },
    {
      "epoch": 0.78125,
      "grad_norm": 14.197975158691406,
      "learning_rate": 4.37555114638448e-06,
      "loss": 0.1402,
      "step": 28350
    },
    {
      "epoch": 0.7815255731922398,
      "grad_norm": 0.5451391935348511,
      "learning_rate": 4.370039682539683e-06,
      "loss": 0.0014,
      "step": 28360
    },
    {
      "epoch": 0.7818011463844797,
      "grad_norm": 0.023180276155471802,
      "learning_rate": 4.3645282186948855e-06,
      "loss": 0.1773,
      "step": 28370
    },
    {
      "epoch": 0.7820767195767195,
      "grad_norm": 0.009298313409090042,
      "learning_rate": 4.359016754850089e-06,
      "loss": 0.0088,
      "step": 28380
    },
    {
      "epoch": 0.7823522927689595,
      "grad_norm": 0.00893714465200901,
      "learning_rate": 4.353505291005292e-06,
      "loss": 0.0866,
      "step": 28390
    },
    {
      "epoch": 0.7826278659611993,
      "grad_norm": 0.8014139533042908,
      "learning_rate": 4.347993827160494e-06,
      "loss": 0.1254,
      "step": 28400
    },
    {
      "epoch": 0.7829034391534392,
      "grad_norm": 0.00916815735399723,
      "learning_rate": 4.342482363315697e-06,
      "loss": 0.0793,
      "step": 28410
    },
    {
      "epoch": 0.783179012345679,
      "grad_norm": 0.012074899859726429,
      "learning_rate": 4.3369708994709e-06,
      "loss": 0.1071,
      "step": 28420
    },
    {
      "epoch": 0.7834545855379189,
      "grad_norm": 0.011486241593956947,
      "learning_rate": 4.3314594356261025e-06,
      "loss": 0.0019,
      "step": 28430
    },
    {
      "epoch": 0.7837301587301587,
      "grad_norm": 0.012340277433395386,
      "learning_rate": 4.325947971781305e-06,
      "loss": 0.23,
      "step": 28440
    },
    {
      "epoch": 0.7840057319223986,
      "grad_norm": 0.007136359810829163,
      "learning_rate": 4.320436507936508e-06,
      "loss": 0.1204,
      "step": 28450
    },
    {
      "epoch": 0.7842813051146384,
      "grad_norm": 0.07047872990369797,
      "learning_rate": 4.314925044091711e-06,
      "loss": 0.0018,
      "step": 28460
    },
    {
      "epoch": 0.7845568783068783,
      "grad_norm": 0.01820955239236355,
      "learning_rate": 4.309413580246914e-06,
      "loss": 0.0919,
      "step": 28470
    },
    {
      "epoch": 0.7848324514991182,
      "grad_norm": 0.01180971134454012,
      "learning_rate": 4.303902116402116e-06,
      "loss": 0.0849,
      "step": 28480
    },
    {
      "epoch": 0.785108024691358,
      "grad_norm": 7.400102138519287,
      "learning_rate": 4.2983906525573195e-06,
      "loss": 0.0324,
      "step": 28490
    },
    {
      "epoch": 0.7853835978835979,
      "grad_norm": 0.06729795038700104,
      "learning_rate": 4.292879188712523e-06,
      "loss": 0.0661,
      "step": 28500
    },
    {
      "epoch": 0.7856591710758377,
      "grad_norm": 200.01011657714844,
      "learning_rate": 4.287367724867725e-06,
      "loss": 0.0527,
      "step": 28510
    },
    {
      "epoch": 0.7859347442680776,
      "grad_norm": 4.266212463378906,
      "learning_rate": 4.281856261022928e-06,
      "loss": 0.0911,
      "step": 28520
    },
    {
      "epoch": 0.7862103174603174,
      "grad_norm": 0.01852339133620262,
      "learning_rate": 4.276344797178131e-06,
      "loss": 0.1608,
      "step": 28530
    },
    {
      "epoch": 0.7864858906525574,
      "grad_norm": 0.08297774940729141,
      "learning_rate": 4.270833333333333e-06,
      "loss": 0.0376,
      "step": 28540
    },
    {
      "epoch": 0.7867614638447972,
      "grad_norm": 3.6389286518096924,
      "learning_rate": 4.2653218694885365e-06,
      "loss": 0.0073,
      "step": 28550
    },
    {
      "epoch": 0.7870370370370371,
      "grad_norm": 0.012308401986956596,
      "learning_rate": 4.25981040564374e-06,
      "loss": 0.0395,
      "step": 28560
    },
    {
      "epoch": 0.7873126102292769,
      "grad_norm": 14.006065368652344,
      "learning_rate": 4.254298941798942e-06,
      "loss": 0.1203,
      "step": 28570
    },
    {
      "epoch": 0.7875881834215167,
      "grad_norm": 6.773681163787842,
      "learning_rate": 4.248787477954145e-06,
      "loss": 0.0866,
      "step": 28580
    },
    {
      "epoch": 0.7878637566137566,
      "grad_norm": 0.024788977578282356,
      "learning_rate": 4.243276014109348e-06,
      "loss": 0.0999,
      "step": 28590
    },
    {
      "epoch": 0.7881393298059964,
      "grad_norm": 0.009913849644362926,
      "learning_rate": 4.23776455026455e-06,
      "loss": 0.0894,
      "step": 28600
    },
    {
      "epoch": 0.7884149029982364,
      "grad_norm": 0.02824019454419613,
      "learning_rate": 4.2322530864197535e-06,
      "loss": 0.0007,
      "step": 28610
    },
    {
      "epoch": 0.7886904761904762,
      "grad_norm": 0.012720385566353798,
      "learning_rate": 4.226741622574957e-06,
      "loss": 0.0016,
      "step": 28620
    },
    {
      "epoch": 0.7889660493827161,
      "grad_norm": 0.04277777299284935,
      "learning_rate": 4.221230158730159e-06,
      "loss": 0.208,
      "step": 28630
    },
    {
      "epoch": 0.7892416225749559,
      "grad_norm": 0.8972601890563965,
      "learning_rate": 4.215718694885362e-06,
      "loss": 0.0008,
      "step": 28640
    },
    {
      "epoch": 0.7895171957671958,
      "grad_norm": 0.577749490737915,
      "learning_rate": 4.210207231040565e-06,
      "loss": 0.1632,
      "step": 28650
    },
    {
      "epoch": 0.7897927689594356,
      "grad_norm": 6.675245761871338,
      "learning_rate": 4.204695767195768e-06,
      "loss": 0.0015,
      "step": 28660
    },
    {
      "epoch": 0.7900683421516755,
      "grad_norm": 0.010915827937424183,
      "learning_rate": 4.1991843033509705e-06,
      "loss": 0.0083,
      "step": 28670
    },
    {
      "epoch": 0.7903439153439153,
      "grad_norm": 0.40122807025909424,
      "learning_rate": 4.193672839506173e-06,
      "loss": 0.0107,
      "step": 28680
    },
    {
      "epoch": 0.7906194885361552,
      "grad_norm": 0.013214992359280586,
      "learning_rate": 4.188161375661376e-06,
      "loss": 0.145,
      "step": 28690
    },
    {
      "epoch": 0.7908950617283951,
      "grad_norm": 3.1641693115234375,
      "learning_rate": 4.182649911816579e-06,
      "loss": 0.1047,
      "step": 28700
    },
    {
      "epoch": 0.7911706349206349,
      "grad_norm": 0.013827985152602196,
      "learning_rate": 4.177138447971782e-06,
      "loss": 0.0364,
      "step": 28710
    },
    {
      "epoch": 0.7914462081128748,
      "grad_norm": 0.008312443271279335,
      "learning_rate": 4.171626984126984e-06,
      "loss": 0.0577,
      "step": 28720
    },
    {
      "epoch": 0.7917217813051146,
      "grad_norm": 0.015841921791434288,
      "learning_rate": 4.1661155202821875e-06,
      "loss": 0.0152,
      "step": 28730
    },
    {
      "epoch": 0.7919973544973545,
      "grad_norm": 11.158668518066406,
      "learning_rate": 4.16060405643739e-06,
      "loss": 0.0845,
      "step": 28740
    },
    {
      "epoch": 0.7922729276895943,
      "grad_norm": 109.53180694580078,
      "learning_rate": 4.155092592592593e-06,
      "loss": 0.0609,
      "step": 28750
    },
    {
      "epoch": 0.7925485008818343,
      "grad_norm": 4.101057052612305,
      "learning_rate": 4.149581128747796e-06,
      "loss": 0.1509,
      "step": 28760
    },
    {
      "epoch": 0.7928240740740741,
      "grad_norm": 0.029287908226251602,
      "learning_rate": 4.144069664902999e-06,
      "loss": 0.0849,
      "step": 28770
    },
    {
      "epoch": 0.7930996472663139,
      "grad_norm": 0.07755322754383087,
      "learning_rate": 4.138558201058201e-06,
      "loss": 0.1602,
      "step": 28780
    },
    {
      "epoch": 0.7933752204585538,
      "grad_norm": 81.79735565185547,
      "learning_rate": 4.1330467372134045e-06,
      "loss": 0.0606,
      "step": 28790
    },
    {
      "epoch": 0.7936507936507936,
      "grad_norm": 0.015145748853683472,
      "learning_rate": 4.127535273368607e-06,
      "loss": 0.0076,
      "step": 28800
    },
    {
      "epoch": 0.7939263668430335,
      "grad_norm": 0.04200716316699982,
      "learning_rate": 4.122023809523809e-06,
      "loss": 0.0026,
      "step": 28810
    },
    {
      "epoch": 0.7942019400352733,
      "grad_norm": 0.026460856199264526,
      "learning_rate": 4.116512345679013e-06,
      "loss": 0.0831,
      "step": 28820
    },
    {
      "epoch": 0.7944775132275133,
      "grad_norm": 145.9230499267578,
      "learning_rate": 4.111000881834216e-06,
      "loss": 0.0561,
      "step": 28830
    },
    {
      "epoch": 0.7947530864197531,
      "grad_norm": 0.020582295954227448,
      "learning_rate": 4.105489417989418e-06,
      "loss": 0.0276,
      "step": 28840
    },
    {
      "epoch": 0.795028659611993,
      "grad_norm": 0.019783537834882736,
      "learning_rate": 4.099977954144621e-06,
      "loss": 0.1327,
      "step": 28850
    },
    {
      "epoch": 0.7953042328042328,
      "grad_norm": 0.015504241921007633,
      "learning_rate": 4.094466490299824e-06,
      "loss": 0.0902,
      "step": 28860
    },
    {
      "epoch": 0.7955798059964727,
      "grad_norm": 120.13577270507812,
      "learning_rate": 4.088955026455027e-06,
      "loss": 0.1284,
      "step": 28870
    },
    {
      "epoch": 0.7958553791887125,
      "grad_norm": 0.009195132181048393,
      "learning_rate": 4.08344356261023e-06,
      "loss": 0.0011,
      "step": 28880
    },
    {
      "epoch": 0.7961309523809523,
      "grad_norm": 129.1412353515625,
      "learning_rate": 4.077932098765433e-06,
      "loss": 0.0127,
      "step": 28890
    },
    {
      "epoch": 0.7964065255731922,
      "grad_norm": 176.0830078125,
      "learning_rate": 4.072420634920635e-06,
      "loss": 0.1336,
      "step": 28900
    },
    {
      "epoch": 0.7966820987654321,
      "grad_norm": 0.07239653915166855,
      "learning_rate": 4.066909171075838e-06,
      "loss": 0.0177,
      "step": 28910
    },
    {
      "epoch": 0.796957671957672,
      "grad_norm": 0.07058675587177277,
      "learning_rate": 4.061397707231041e-06,
      "loss": 0.0889,
      "step": 28920
    },
    {
      "epoch": 0.7972332451499118,
      "grad_norm": 23.716960906982422,
      "learning_rate": 4.055886243386244e-06,
      "loss": 0.0827,
      "step": 28930
    },
    {
      "epoch": 0.7975088183421517,
      "grad_norm": 1.999988317489624,
      "learning_rate": 4.050374779541447e-06,
      "loss": 0.1501,
      "step": 28940
    },
    {
      "epoch": 0.7977843915343915,
      "grad_norm": 0.5154247283935547,
      "learning_rate": 4.044863315696649e-06,
      "loss": 0.0035,
      "step": 28950
    },
    {
      "epoch": 0.7980599647266314,
      "grad_norm": 0.012937905266880989,
      "learning_rate": 4.039351851851852e-06,
      "loss": 0.1649,
      "step": 28960
    },
    {
      "epoch": 0.7983355379188712,
      "grad_norm": 3.805933713912964,
      "learning_rate": 4.033840388007055e-06,
      "loss": 0.0906,
      "step": 28970
    },
    {
      "epoch": 0.7986111111111112,
      "grad_norm": 13.551199913024902,
      "learning_rate": 4.028328924162258e-06,
      "loss": 0.0086,
      "step": 28980
    },
    {
      "epoch": 0.798886684303351,
      "grad_norm": 0.01283832173794508,
      "learning_rate": 4.02281746031746e-06,
      "loss": 0.0823,
      "step": 28990
    },
    {
      "epoch": 0.7991622574955908,
      "grad_norm": 0.02085154317319393,
      "learning_rate": 4.017305996472664e-06,
      "loss": 0.0006,
      "step": 29000
    },
    {
      "epoch": 0.7994378306878307,
      "grad_norm": 109.78608703613281,
      "learning_rate": 4.011794532627866e-06,
      "loss": 0.1858,
      "step": 29010
    },
    {
      "epoch": 0.7997134038800705,
      "grad_norm": 0.0712086483836174,
      "learning_rate": 4.006283068783069e-06,
      "loss": 0.0762,
      "step": 29020
    },
    {
      "epoch": 0.7999889770723104,
      "grad_norm": 0.023982517421245575,
      "learning_rate": 4.000771604938272e-06,
      "loss": 0.0189,
      "step": 29030
    },
    {
      "epoch": 0.8002645502645502,
      "grad_norm": 0.1868630200624466,
      "learning_rate": 3.995260141093475e-06,
      "loss": 0.017,
      "step": 29040
    },
    {
      "epoch": 0.8005401234567902,
      "grad_norm": 0.02187691256403923,
      "learning_rate": 3.989748677248677e-06,
      "loss": 0.0006,
      "step": 29050
    },
    {
      "epoch": 0.80081569664903,
      "grad_norm": 0.01692645065486431,
      "learning_rate": 3.984237213403881e-06,
      "loss": 0.1729,
      "step": 29060
    },
    {
      "epoch": 0.8010912698412699,
      "grad_norm": 1.6455751657485962,
      "learning_rate": 3.978725749559083e-06,
      "loss": 0.0718,
      "step": 29070
    },
    {
      "epoch": 0.8013668430335097,
      "grad_norm": 0.0191048551350832,
      "learning_rate": 3.9732142857142855e-06,
      "loss": 0.0006,
      "step": 29080
    },
    {
      "epoch": 0.8016424162257496,
      "grad_norm": 0.03924060985445976,
      "learning_rate": 3.967702821869489e-06,
      "loss": 0.2272,
      "step": 29090
    },
    {
      "epoch": 0.8019179894179894,
      "grad_norm": 0.9156673550605774,
      "learning_rate": 3.962191358024692e-06,
      "loss": 0.0194,
      "step": 29100
    },
    {
      "epoch": 0.8021935626102292,
      "grad_norm": 4.098355293273926,
      "learning_rate": 3.956679894179894e-06,
      "loss": 0.0937,
      "step": 29110
    },
    {
      "epoch": 0.8024691358024691,
      "grad_norm": 0.01623750850558281,
      "learning_rate": 3.951168430335097e-06,
      "loss": 0.0011,
      "step": 29120
    },
    {
      "epoch": 0.802744708994709,
      "grad_norm": 0.1600894331932068,
      "learning_rate": 3.9456569664903e-06,
      "loss": 0.2715,
      "step": 29130
    },
    {
      "epoch": 0.8030202821869489,
      "grad_norm": 0.023031985387206078,
      "learning_rate": 3.940145502645503e-06,
      "loss": 0.0143,
      "step": 29140
    },
    {
      "epoch": 0.8032958553791887,
      "grad_norm": 3.405109167098999,
      "learning_rate": 3.934634038800706e-06,
      "loss": 0.1357,
      "step": 29150
    },
    {
      "epoch": 0.8035714285714286,
      "grad_norm": 0.029242580756545067,
      "learning_rate": 3.929122574955909e-06,
      "loss": 0.0907,
      "step": 29160
    },
    {
      "epoch": 0.8038470017636684,
      "grad_norm": 71.97021484375,
      "learning_rate": 3.9236111111111114e-06,
      "loss": 0.1762,
      "step": 29170
    },
    {
      "epoch": 0.8041225749559083,
      "grad_norm": 0.0514187328517437,
      "learning_rate": 3.918099647266314e-06,
      "loss": 0.0011,
      "step": 29180
    },
    {
      "epoch": 0.8043981481481481,
      "grad_norm": 0.008193645626306534,
      "learning_rate": 3.912588183421517e-06,
      "loss": 0.0373,
      "step": 29190
    },
    {
      "epoch": 0.8046737213403881,
      "grad_norm": 0.24296870827674866,
      "learning_rate": 3.90707671957672e-06,
      "loss": 0.1609,
      "step": 29200
    },
    {
      "epoch": 0.8049492945326279,
      "grad_norm": 170.95623779296875,
      "learning_rate": 3.901565255731923e-06,
      "loss": 0.0488,
      "step": 29210
    },
    {
      "epoch": 0.8052248677248677,
      "grad_norm": 0.006761098280549049,
      "learning_rate": 3.896053791887125e-06,
      "loss": 0.0031,
      "step": 29220
    },
    {
      "epoch": 0.8055004409171076,
      "grad_norm": 0.018756859004497528,
      "learning_rate": 3.8905423280423284e-06,
      "loss": 0.0837,
      "step": 29230
    },
    {
      "epoch": 0.8057760141093474,
      "grad_norm": 0.010665008798241615,
      "learning_rate": 3.885030864197531e-06,
      "loss": 0.1047,
      "step": 29240
    },
    {
      "epoch": 0.8060515873015873,
      "grad_norm": 0.007970075123012066,
      "learning_rate": 3.879519400352734e-06,
      "loss": 0.0078,
      "step": 29250
    },
    {
      "epoch": 0.8063271604938271,
      "grad_norm": 0.012140522710978985,
      "learning_rate": 3.8740079365079365e-06,
      "loss": 0.0863,
      "step": 29260
    },
    {
      "epoch": 0.806602733686067,
      "grad_norm": 0.0953555703163147,
      "learning_rate": 3.86849647266314e-06,
      "loss": 0.0007,
      "step": 29270
    },
    {
      "epoch": 0.8068783068783069,
      "grad_norm": 0.36587509512901306,
      "learning_rate": 3.862985008818342e-06,
      "loss": 0.0016,
      "step": 29280
    },
    {
      "epoch": 0.8071538800705468,
      "grad_norm": 0.030729323625564575,
      "learning_rate": 3.8574735449735454e-06,
      "loss": 0.1494,
      "step": 29290
    },
    {
      "epoch": 0.8074294532627866,
      "grad_norm": 5.846183776855469,
      "learning_rate": 3.851962081128749e-06,
      "loss": 0.002,
      "step": 29300
    },
    {
      "epoch": 0.8077050264550265,
      "grad_norm": 154.47999572753906,
      "learning_rate": 3.846450617283951e-06,
      "loss": 0.022,
      "step": 29310
    },
    {
      "epoch": 0.8079805996472663,
      "grad_norm": 0.00703377416357398,
      "learning_rate": 3.8409391534391535e-06,
      "loss": 0.0006,
      "step": 29320
    },
    {
      "epoch": 0.8082561728395061,
      "grad_norm": 1.2527233362197876,
      "learning_rate": 3.835427689594357e-06,
      "loss": 0.0652,
      "step": 29330
    },
    {
      "epoch": 0.808531746031746,
      "grad_norm": 0.007867191918194294,
      "learning_rate": 3.829916225749559e-06,
      "loss": 0.0882,
      "step": 29340
    },
    {
      "epoch": 0.8088073192239859,
      "grad_norm": 0.03682748228311539,
      "learning_rate": 3.824404761904762e-06,
      "loss": 0.0038,
      "step": 29350
    },
    {
      "epoch": 0.8090828924162258,
      "grad_norm": 111.36609649658203,
      "learning_rate": 3.818893298059965e-06,
      "loss": 0.1087,
      "step": 29360
    },
    {
      "epoch": 0.8093584656084656,
      "grad_norm": 0.008658267557621002,
      "learning_rate": 3.813381834215168e-06,
      "loss": 0.1618,
      "step": 29370
    },
    {
      "epoch": 0.8096340388007055,
      "grad_norm": 0.01713321916759014,
      "learning_rate": 3.8078703703703705e-06,
      "loss": 0.1061,
      "step": 29380
    },
    {
      "epoch": 0.8099096119929453,
      "grad_norm": 0.12231656163930893,
      "learning_rate": 3.8023589065255734e-06,
      "loss": 0.0007,
      "step": 29390
    },
    {
      "epoch": 0.8101851851851852,
      "grad_norm": 16.555561065673828,
      "learning_rate": 3.796847442680776e-06,
      "loss": 0.0933,
      "step": 29400
    },
    {
      "epoch": 0.810460758377425,
      "grad_norm": 45.616973876953125,
      "learning_rate": 3.7913359788359795e-06,
      "loss": 0.105,
      "step": 29410
    },
    {
      "epoch": 0.8107363315696648,
      "grad_norm": 28.81703758239746,
      "learning_rate": 3.785824514991182e-06,
      "loss": 0.0858,
      "step": 29420
    },
    {
      "epoch": 0.8110119047619048,
      "grad_norm": 0.1539665162563324,
      "learning_rate": 3.7803130511463847e-06,
      "loss": 0.0921,
      "step": 29430
    },
    {
      "epoch": 0.8112874779541446,
      "grad_norm": 0.706116795539856,
      "learning_rate": 3.7748015873015875e-06,
      "loss": 0.0008,
      "step": 29440
    },
    {
      "epoch": 0.8115630511463845,
      "grad_norm": 0.11773184686899185,
      "learning_rate": 3.7692901234567904e-06,
      "loss": 0.2723,
      "step": 29450
    },
    {
      "epoch": 0.8118386243386243,
      "grad_norm": 0.07118368148803711,
      "learning_rate": 3.7637786596119936e-06,
      "loss": 0.0006,
      "step": 29460
    },
    {
      "epoch": 0.8121141975308642,
      "grad_norm": 0.25422728061676025,
      "learning_rate": 3.758267195767196e-06,
      "loss": 0.0022,
      "step": 29470
    },
    {
      "epoch": 0.812389770723104,
      "grad_norm": 0.08452426642179489,
      "learning_rate": 3.752755731922399e-06,
      "loss": 0.0502,
      "step": 29480
    },
    {
      "epoch": 0.812665343915344,
      "grad_norm": 0.020380068570375443,
      "learning_rate": 3.7472442680776017e-06,
      "loss": 0.1014,
      "step": 29490
    },
    {
      "epoch": 0.8129409171075838,
      "grad_norm": 0.05992693826556206,
      "learning_rate": 3.7417328042328046e-06,
      "loss": 0.1479,
      "step": 29500
    },
    {
      "epoch": 0.8132164902998237,
      "grad_norm": 0.008517456240952015,
      "learning_rate": 3.736221340388007e-06,
      "loss": 0.0006,
      "step": 29510
    },
    {
      "epoch": 0.8134920634920635,
      "grad_norm": 0.2746509611606598,
      "learning_rate": 3.7307098765432102e-06,
      "loss": 0.0028,
      "step": 29520
    },
    {
      "epoch": 0.8137676366843033,
      "grad_norm": 10.14691162109375,
      "learning_rate": 3.725198412698413e-06,
      "loss": 0.0041,
      "step": 29530
    },
    {
      "epoch": 0.8140432098765432,
      "grad_norm": 0.007126557175070047,
      "learning_rate": 3.719686948853616e-06,
      "loss": 0.0744,
      "step": 29540
    },
    {
      "epoch": 0.814318783068783,
      "grad_norm": 66.72798156738281,
      "learning_rate": 3.7141754850088187e-06,
      "loss": 0.0302,
      "step": 29550
    },
    {
      "epoch": 0.814594356261023,
      "grad_norm": 28.28145980834961,
      "learning_rate": 3.708664021164021e-06,
      "loss": 0.0843,
      "step": 29560
    },
    {
      "epoch": 0.8148699294532628,
      "grad_norm": 0.05254947394132614,
      "learning_rate": 3.7031525573192244e-06,
      "loss": 0.0015,
      "step": 29570
    },
    {
      "epoch": 0.8151455026455027,
      "grad_norm": 0.008812771178781986,
      "learning_rate": 3.6976410934744272e-06,
      "loss": 0.0236,
      "step": 29580
    },
    {
      "epoch": 0.8154210758377425,
      "grad_norm": 0.01695217937231064,
      "learning_rate": 3.69212962962963e-06,
      "loss": 0.0994,
      "step": 29590
    },
    {
      "epoch": 0.8156966490299824,
      "grad_norm": 0.012831169180572033,
      "learning_rate": 3.6866181657848325e-06,
      "loss": 0.0008,
      "step": 29600
    },
    {
      "epoch": 0.8159722222222222,
      "grad_norm": 0.008910798467695713,
      "learning_rate": 3.6811067019400353e-06,
      "loss": 0.0007,
      "step": 29610
    },
    {
      "epoch": 0.8162477954144621,
      "grad_norm": 0.013306455686688423,
      "learning_rate": 3.6755952380952386e-06,
      "loss": 0.037,
      "step": 29620
    },
    {
      "epoch": 0.8165233686067019,
      "grad_norm": 34.42580032348633,
      "learning_rate": 3.6700837742504414e-06,
      "loss": 0.0039,
      "step": 29630
    },
    {
      "epoch": 0.8167989417989417,
      "grad_norm": 0.3395945429801941,
      "learning_rate": 3.6645723104056443e-06,
      "loss": 0.0744,
      "step": 29640
    },
    {
      "epoch": 0.8170745149911817,
      "grad_norm": 131.95169067382812,
      "learning_rate": 3.6590608465608467e-06,
      "loss": 0.0314,
      "step": 29650
    },
    {
      "epoch": 0.8173500881834215,
      "grad_norm": 1.231211543083191,
      "learning_rate": 3.6535493827160495e-06,
      "loss": 0.1417,
      "step": 29660
    },
    {
      "epoch": 0.8176256613756614,
      "grad_norm": 0.0860213041305542,
      "learning_rate": 3.6480379188712523e-06,
      "loss": 0.0833,
      "step": 29670
    },
    {
      "epoch": 0.8179012345679012,
      "grad_norm": 1.750852108001709,
      "learning_rate": 3.6425264550264556e-06,
      "loss": 0.0724,
      "step": 29680
    },
    {
      "epoch": 0.8181768077601411,
      "grad_norm": 0.006336663383990526,
      "learning_rate": 3.6370149911816584e-06,
      "loss": 0.0926,
      "step": 29690
    },
    {
      "epoch": 0.8184523809523809,
      "grad_norm": 102.06066131591797,
      "learning_rate": 3.631503527336861e-06,
      "loss": 0.0598,
      "step": 29700
    },
    {
      "epoch": 0.8187279541446209,
      "grad_norm": 0.016689911484718323,
      "learning_rate": 3.6259920634920637e-06,
      "loss": 0.0856,
      "step": 29710
    },
    {
      "epoch": 0.8190035273368607,
      "grad_norm": 0.011451128870248795,
      "learning_rate": 3.6204805996472665e-06,
      "loss": 0.009,
      "step": 29720
    },
    {
      "epoch": 0.8192791005291006,
      "grad_norm": 0.013157382607460022,
      "learning_rate": 3.6149691358024698e-06,
      "loss": 0.0063,
      "step": 29730
    },
    {
      "epoch": 0.8195546737213404,
      "grad_norm": 0.051670171320438385,
      "learning_rate": 3.609457671957672e-06,
      "loss": 0.0147,
      "step": 29740
    },
    {
      "epoch": 0.8198302469135802,
      "grad_norm": 1.5913954973220825,
      "learning_rate": 3.603946208112875e-06,
      "loss": 0.1566,
      "step": 29750
    },
    {
      "epoch": 0.8201058201058201,
      "grad_norm": 0.07750280946493149,
      "learning_rate": 3.598434744268078e-06,
      "loss": 0.0321,
      "step": 29760
    },
    {
      "epoch": 0.8203813932980599,
      "grad_norm": 3.9128546714782715,
      "learning_rate": 3.5929232804232807e-06,
      "loss": 0.0635,
      "step": 29770
    },
    {
      "epoch": 0.8206569664902998,
      "grad_norm": 136.18341064453125,
      "learning_rate": 3.587411816578484e-06,
      "loss": 0.0902,
      "step": 29780
    },
    {
      "epoch": 0.8209325396825397,
      "grad_norm": 14.544326782226562,
      "learning_rate": 3.5819003527336864e-06,
      "loss": 0.202,
      "step": 29790
    },
    {
      "epoch": 0.8212081128747796,
      "grad_norm": 0.013266034424304962,
      "learning_rate": 3.576388888888889e-06,
      "loss": 0.0987,
      "step": 29800
    },
    {
      "epoch": 0.8214836860670194,
      "grad_norm": 0.025659898295998573,
      "learning_rate": 3.570877425044092e-06,
      "loss": 0.0489,
      "step": 29810
    },
    {
      "epoch": 0.8217592592592593,
      "grad_norm": 0.10764170438051224,
      "learning_rate": 3.565365961199295e-06,
      "loss": 0.0046,
      "step": 29820
    },
    {
      "epoch": 0.8220348324514991,
      "grad_norm": 0.02281355857849121,
      "learning_rate": 3.5598544973544973e-06,
      "loss": 0.2512,
      "step": 29830
    },
    {
      "epoch": 0.822310405643739,
      "grad_norm": 0.006681277882307768,
      "learning_rate": 3.5543430335097005e-06,
      "loss": 0.1592,
      "step": 29840
    },
    {
      "epoch": 0.8225859788359788,
      "grad_norm": 0.006949286442250013,
      "learning_rate": 3.5488315696649034e-06,
      "loss": 0.0612,
      "step": 29850
    },
    {
      "epoch": 0.8228615520282186,
      "grad_norm": 6.077714920043945,
      "learning_rate": 3.543320105820106e-06,
      "loss": 0.0508,
      "step": 29860
    },
    {
      "epoch": 0.8231371252204586,
      "grad_norm": 0.02555374801158905,
      "learning_rate": 3.537808641975309e-06,
      "loss": 0.2467,
      "step": 29870
    },
    {
      "epoch": 0.8234126984126984,
      "grad_norm": 0.007390071637928486,
      "learning_rate": 3.5322971781305114e-06,
      "loss": 0.0121,
      "step": 29880
    },
    {
      "epoch": 0.8236882716049383,
      "grad_norm": 0.02367294766008854,
      "learning_rate": 3.5267857142857147e-06,
      "loss": 0.0714,
      "step": 29890
    },
    {
      "epoch": 0.8239638447971781,
      "grad_norm": 0.03744304180145264,
      "learning_rate": 3.5212742504409175e-06,
      "loss": 0.0515,
      "step": 29900
    },
    {
      "epoch": 0.824239417989418,
      "grad_norm": 0.21964532136917114,
      "learning_rate": 3.5157627865961204e-06,
      "loss": 0.0815,
      "step": 29910
    },
    {
      "epoch": 0.8245149911816578,
      "grad_norm": 6.908413887023926,
      "learning_rate": 3.5102513227513228e-06,
      "loss": 0.1132,
      "step": 29920
    },
    {
      "epoch": 0.8247905643738977,
      "grad_norm": 0.010710553266108036,
      "learning_rate": 3.5047398589065256e-06,
      "loss": 0.0022,
      "step": 29930
    },
    {
      "epoch": 0.8250661375661376,
      "grad_norm": 0.008788780309259892,
      "learning_rate": 3.499228395061729e-06,
      "loss": 0.0188,
      "step": 29940
    },
    {
      "epoch": 0.8253417107583775,
      "grad_norm": 0.14101898670196533,
      "learning_rate": 3.4937169312169317e-06,
      "loss": 0.0766,
      "step": 29950
    },
    {
      "epoch": 0.8256172839506173,
      "grad_norm": 20.291460037231445,
      "learning_rate": 3.4882054673721345e-06,
      "loss": 0.1908,
      "step": 29960
    },
    {
      "epoch": 0.8258928571428571,
      "grad_norm": 0.033965155482292175,
      "learning_rate": 3.482694003527337e-06,
      "loss": 0.1494,
      "step": 29970
    },
    {
      "epoch": 0.826168430335097,
      "grad_norm": 0.010353767313063145,
      "learning_rate": 3.47718253968254e-06,
      "loss": 0.0031,
      "step": 29980
    },
    {
      "epoch": 0.8264440035273368,
      "grad_norm": 66.36054229736328,
      "learning_rate": 3.4716710758377426e-06,
      "loss": 0.1941,
      "step": 29990
    },
    {
      "epoch": 0.8267195767195767,
      "grad_norm": 0.10538261383771896,
      "learning_rate": 3.466159611992946e-06,
      "loss": 0.1248,
      "step": 30000
    },
    {
      "epoch": 0.8269951499118166,
      "grad_norm": 0.009157083928585052,
      "learning_rate": 3.4606481481481487e-06,
      "loss": 0.0867,
      "step": 30010
    },
    {
      "epoch": 0.8272707231040565,
      "grad_norm": 0.025092603638768196,
      "learning_rate": 3.455136684303351e-06,
      "loss": 0.1222,
      "step": 30020
    },
    {
      "epoch": 0.8275462962962963,
      "grad_norm": 0.011705446057021618,
      "learning_rate": 3.449625220458554e-06,
      "loss": 0.0847,
      "step": 30030
    },
    {
      "epoch": 0.8278218694885362,
      "grad_norm": 0.008428252302110195,
      "learning_rate": 3.444113756613757e-06,
      "loss": 0.0618,
      "step": 30040
    },
    {
      "epoch": 0.828097442680776,
      "grad_norm": 0.02167043648660183,
      "learning_rate": 3.43860229276896e-06,
      "loss": 0.001,
      "step": 30050
    },
    {
      "epoch": 0.8283730158730159,
      "grad_norm": 0.020335782319307327,
      "learning_rate": 3.4330908289241625e-06,
      "loss": 0.0179,
      "step": 30060
    },
    {
      "epoch": 0.8286485890652557,
      "grad_norm": 0.03155714273452759,
      "learning_rate": 3.4275793650793653e-06,
      "loss": 0.0857,
      "step": 30070
    },
    {
      "epoch": 0.8289241622574955,
      "grad_norm": 0.019642800092697144,
      "learning_rate": 3.422067901234568e-06,
      "loss": 0.0008,
      "step": 30080
    },
    {
      "epoch": 0.8291997354497355,
      "grad_norm": 0.010799607262015343,
      "learning_rate": 3.416556437389771e-06,
      "loss": 0.0092,
      "step": 30090
    },
    {
      "epoch": 0.8294753086419753,
      "grad_norm": 191.3784942626953,
      "learning_rate": 3.4110449735449734e-06,
      "loss": 0.0189,
      "step": 30100
    },
    {
      "epoch": 0.8297508818342152,
      "grad_norm": 149.9202880859375,
      "learning_rate": 3.4055335097001767e-06,
      "loss": 0.1167,
      "step": 30110
    },
    {
      "epoch": 0.830026455026455,
      "grad_norm": 0.012499953620135784,
      "learning_rate": 3.4000220458553795e-06,
      "loss": 0.0723,
      "step": 30120
    },
    {
      "epoch": 0.8303020282186949,
      "grad_norm": 0.0732114240527153,
      "learning_rate": 3.3945105820105823e-06,
      "loss": 0.1531,
      "step": 30130
    },
    {
      "epoch": 0.8305776014109347,
      "grad_norm": 0.011155152693390846,
      "learning_rate": 3.388999118165785e-06,
      "loss": 0.2237,
      "step": 30140
    },
    {
      "epoch": 0.8308531746031746,
      "grad_norm": 8.2420015335083,
      "learning_rate": 3.3834876543209876e-06,
      "loss": 0.0071,
      "step": 30150
    },
    {
      "epoch": 0.8311287477954145,
      "grad_norm": 0.01267052348703146,
      "learning_rate": 3.377976190476191e-06,
      "loss": 0.3215,
      "step": 30160
    },
    {
      "epoch": 0.8314043209876543,
      "grad_norm": 0.006539220456033945,
      "learning_rate": 3.3724647266313937e-06,
      "loss": 0.076,
      "step": 30170
    },
    {
      "epoch": 0.8316798941798942,
      "grad_norm": 20.57601547241211,
      "learning_rate": 3.3669532627865965e-06,
      "loss": 0.0892,
      "step": 30180
    },
    {
      "epoch": 0.831955467372134,
      "grad_norm": 0.0497610904276371,
      "learning_rate": 3.361441798941799e-06,
      "loss": 0.1496,
      "step": 30190
    },
    {
      "epoch": 0.8322310405643739,
      "grad_norm": 23.14676856994629,
      "learning_rate": 3.3559303350970017e-06,
      "loss": 0.2482,
      "step": 30200
    },
    {
      "epoch": 0.8325066137566137,
      "grad_norm": 0.011004328727722168,
      "learning_rate": 3.350418871252205e-06,
      "loss": 0.0006,
      "step": 30210
    },
    {
      "epoch": 0.8327821869488536,
      "grad_norm": 0.008946774527430534,
      "learning_rate": 3.344907407407408e-06,
      "loss": 0.1823,
      "step": 30220
    },
    {
      "epoch": 0.8330577601410935,
      "grad_norm": 45.97511672973633,
      "learning_rate": 3.3393959435626107e-06,
      "loss": 0.0073,
      "step": 30230
    },
    {
      "epoch": 0.8333333333333334,
      "grad_norm": 0.033007435500621796,
      "learning_rate": 3.333884479717813e-06,
      "loss": 0.1214,
      "step": 30240
    },
    {
      "epoch": 0.8336089065255732,
      "grad_norm": 0.1081652119755745,
      "learning_rate": 3.328373015873016e-06,
      "loss": 0.1257,
      "step": 30250
    },
    {
      "epoch": 0.8338844797178131,
      "grad_norm": 0.012762418948113918,
      "learning_rate": 3.3228615520282188e-06,
      "loss": 0.085,
      "step": 30260
    },
    {
      "epoch": 0.8341600529100529,
      "grad_norm": 0.16442611813545227,
      "learning_rate": 3.317350088183422e-06,
      "loss": 0.1117,
      "step": 30270
    },
    {
      "epoch": 0.8344356261022927,
      "grad_norm": 0.8256509900093079,
      "learning_rate": 3.311838624338625e-06,
      "loss": 0.298,
      "step": 30280
    },
    {
      "epoch": 0.8347111992945326,
      "grad_norm": 0.017062291502952576,
      "learning_rate": 3.3063271604938273e-06,
      "loss": 0.0683,
      "step": 30290
    },
    {
      "epoch": 0.8349867724867724,
      "grad_norm": 0.010844230651855469,
      "learning_rate": 3.30081569664903e-06,
      "loss": 0.0007,
      "step": 30300
    },
    {
      "epoch": 0.8352623456790124,
      "grad_norm": 0.037072647362947464,
      "learning_rate": 3.295304232804233e-06,
      "loss": 0.015,
      "step": 30310
    },
    {
      "epoch": 0.8355379188712522,
      "grad_norm": 101.60749816894531,
      "learning_rate": 3.289792768959436e-06,
      "loss": 0.1474,
      "step": 30320
    },
    {
      "epoch": 0.8358134920634921,
      "grad_norm": 54.038368225097656,
      "learning_rate": 3.284281305114639e-06,
      "loss": 0.1836,
      "step": 30330
    },
    {
      "epoch": 0.8360890652557319,
      "grad_norm": 0.02045336365699768,
      "learning_rate": 3.2787698412698414e-06,
      "loss": 0.02,
      "step": 30340
    },
    {
      "epoch": 0.8363646384479718,
      "grad_norm": 0.019121110439300537,
      "learning_rate": 3.2732583774250443e-06,
      "loss": 0.0007,
      "step": 30350
    },
    {
      "epoch": 0.8366402116402116,
      "grad_norm": 0.043343182653188705,
      "learning_rate": 3.267746913580247e-06,
      "loss": 0.0005,
      "step": 30360
    },
    {
      "epoch": 0.8369157848324515,
      "grad_norm": 1.1213023662567139,
      "learning_rate": 3.2622354497354504e-06,
      "loss": 0.0031,
      "step": 30370
    },
    {
      "epoch": 0.8371913580246914,
      "grad_norm": 0.035878390073776245,
      "learning_rate": 3.2567239858906528e-06,
      "loss": 0.0007,
      "step": 30380
    },
    {
      "epoch": 0.8374669312169312,
      "grad_norm": 0.01695399172604084,
      "learning_rate": 3.2512125220458556e-06,
      "loss": 0.0006,
      "step": 30390
    },
    {
      "epoch": 0.8377425044091711,
      "grad_norm": 0.04839932918548584,
      "learning_rate": 3.2457010582010584e-06,
      "loss": 0.233,
      "step": 30400
    },
    {
      "epoch": 0.8380180776014109,
      "grad_norm": 0.009343707002699375,
      "learning_rate": 3.2401895943562613e-06,
      "loss": 0.0992,
      "step": 30410
    },
    {
      "epoch": 0.8382936507936508,
      "grad_norm": 5.208810806274414,
      "learning_rate": 3.2346781305114637e-06,
      "loss": 0.2865,
      "step": 30420
    },
    {
      "epoch": 0.8385692239858906,
      "grad_norm": 0.1663656234741211,
      "learning_rate": 3.229166666666667e-06,
      "loss": 0.0515,
      "step": 30430
    },
    {
      "epoch": 0.8388447971781305,
      "grad_norm": 117.92744445800781,
      "learning_rate": 3.2236552028218698e-06,
      "loss": 0.113,
      "step": 30440
    },
    {
      "epoch": 0.8391203703703703,
      "grad_norm": 110.60982513427734,
      "learning_rate": 3.2181437389770726e-06,
      "loss": 0.1245,
      "step": 30450
    },
    {
      "epoch": 0.8393959435626103,
      "grad_norm": 0.0146240945905447,
      "learning_rate": 3.2126322751322755e-06,
      "loss": 0.094,
      "step": 30460
    },
    {
      "epoch": 0.8396715167548501,
      "grad_norm": 208.2011260986328,
      "learning_rate": 3.207120811287478e-06,
      "loss": 0.0504,
      "step": 30470
    },
    {
      "epoch": 0.83994708994709,
      "grad_norm": 0.12693531811237335,
      "learning_rate": 3.201609347442681e-06,
      "loss": 0.0655,
      "step": 30480
    },
    {
      "epoch": 0.8402226631393298,
      "grad_norm": 0.007085822988301516,
      "learning_rate": 3.196097883597884e-06,
      "loss": 0.1688,
      "step": 30490
    },
    {
      "epoch": 0.8404982363315696,
      "grad_norm": 0.020574769005179405,
      "learning_rate": 3.190586419753087e-06,
      "loss": 0.0448,
      "step": 30500
    },
    {
      "epoch": 0.8407738095238095,
      "grad_norm": 0.014586086384952068,
      "learning_rate": 3.185074955908289e-06,
      "loss": 0.1907,
      "step": 30510
    },
    {
      "epoch": 0.8410493827160493,
      "grad_norm": 39.189022064208984,
      "learning_rate": 3.179563492063492e-06,
      "loss": 0.3116,
      "step": 30520
    },
    {
      "epoch": 0.8413249559082893,
      "grad_norm": 0.031537748873233795,
      "learning_rate": 3.1740520282186953e-06,
      "loss": 0.0011,
      "step": 30530
    },
    {
      "epoch": 0.8416005291005291,
      "grad_norm": 0.016481641680002213,
      "learning_rate": 3.168540564373898e-06,
      "loss": 0.163,
      "step": 30540
    },
    {
      "epoch": 0.841876102292769,
      "grad_norm": 0.02033972181379795,
      "learning_rate": 3.163029100529101e-06,
      "loss": 0.1293,
      "step": 30550
    },
    {
      "epoch": 0.8421516754850088,
      "grad_norm": 0.010622699744999409,
      "learning_rate": 3.1575176366843034e-06,
      "loss": 0.0495,
      "step": 30560
    },
    {
      "epoch": 0.8424272486772487,
      "grad_norm": 0.007660068105906248,
      "learning_rate": 3.1520061728395062e-06,
      "loss": 0.0066,
      "step": 30570
    },
    {
      "epoch": 0.8427028218694885,
      "grad_norm": 0.00888323038816452,
      "learning_rate": 3.146494708994709e-06,
      "loss": 0.1299,
      "step": 30580
    },
    {
      "epoch": 0.8429783950617284,
      "grad_norm": 0.012250307016074657,
      "learning_rate": 3.1409832451499123e-06,
      "loss": 0.0975,
      "step": 30590
    },
    {
      "epoch": 0.8432539682539683,
      "grad_norm": 0.008957942016422749,
      "learning_rate": 3.135471781305115e-06,
      "loss": 0.0403,
      "step": 30600
    },
    {
      "epoch": 0.8435295414462081,
      "grad_norm": 0.007596638984978199,
      "learning_rate": 3.1299603174603176e-06,
      "loss": 0.0026,
      "step": 30610
    },
    {
      "epoch": 0.843805114638448,
      "grad_norm": 0.012776598334312439,
      "learning_rate": 3.1244488536155204e-06,
      "loss": 0.0477,
      "step": 30620
    },
    {
      "epoch": 0.8440806878306878,
      "grad_norm": 0.00781918689608574,
      "learning_rate": 3.1189373897707232e-06,
      "loss": 0.2542,
      "step": 30630
    },
    {
      "epoch": 0.8443562610229277,
      "grad_norm": 0.8043577075004578,
      "learning_rate": 3.1134259259259265e-06,
      "loss": 0.1029,
      "step": 30640
    },
    {
      "epoch": 0.8446318342151675,
      "grad_norm": 11.817665100097656,
      "learning_rate": 3.1079144620811293e-06,
      "loss": 0.0863,
      "step": 30650
    },
    {
      "epoch": 0.8449074074074074,
      "grad_norm": 0.02371843159198761,
      "learning_rate": 3.1024029982363317e-06,
      "loss": 0.1157,
      "step": 30660
    },
    {
      "epoch": 0.8451829805996472,
      "grad_norm": 3.71395206451416,
      "learning_rate": 3.0968915343915346e-06,
      "loss": 0.0157,
      "step": 30670
    },
    {
      "epoch": 0.8454585537918872,
      "grad_norm": 14.592804908752441,
      "learning_rate": 3.0913800705467374e-06,
      "loss": 0.1874,
      "step": 30680
    },
    {
      "epoch": 0.845734126984127,
      "grad_norm": 0.007096721790730953,
      "learning_rate": 3.0858686067019407e-06,
      "loss": 0.0455,
      "step": 30690
    },
    {
      "epoch": 0.8460097001763669,
      "grad_norm": 0.009502005763351917,
      "learning_rate": 3.080357142857143e-06,
      "loss": 0.0442,
      "step": 30700
    },
    {
      "epoch": 0.8462852733686067,
      "grad_norm": 0.009686694480478764,
      "learning_rate": 3.074845679012346e-06,
      "loss": 0.0183,
      "step": 30710
    },
    {
      "epoch": 0.8465608465608465,
      "grad_norm": 0.016240213066339493,
      "learning_rate": 3.0693342151675487e-06,
      "loss": 0.0673,
      "step": 30720
    },
    {
      "epoch": 0.8468364197530864,
      "grad_norm": 134.91444396972656,
      "learning_rate": 3.0638227513227516e-06,
      "loss": 0.1855,
      "step": 30730
    },
    {
      "epoch": 0.8471119929453262,
      "grad_norm": 0.01985248737037182,
      "learning_rate": 3.058311287477954e-06,
      "loss": 0.0005,
      "step": 30740
    },
    {
      "epoch": 0.8473875661375662,
      "grad_norm": 0.03418781980872154,
      "learning_rate": 3.0527998236331572e-06,
      "loss": 0.1064,
      "step": 30750
    },
    {
      "epoch": 0.847663139329806,
      "grad_norm": 0.02518560364842415,
      "learning_rate": 3.04728835978836e-06,
      "loss": 0.0836,
      "step": 30760
    },
    {
      "epoch": 0.8479387125220459,
      "grad_norm": 0.22310775518417358,
      "learning_rate": 3.041776895943563e-06,
      "loss": 0.1545,
      "step": 30770
    },
    {
      "epoch": 0.8482142857142857,
      "grad_norm": 0.025271238759160042,
      "learning_rate": 3.0362654320987658e-06,
      "loss": 0.1443,
      "step": 30780
    },
    {
      "epoch": 0.8484898589065256,
      "grad_norm": 65.96251678466797,
      "learning_rate": 3.030753968253968e-06,
      "loss": 0.2597,
      "step": 30790
    },
    {
      "epoch": 0.8487654320987654,
      "grad_norm": 0.03969576954841614,
      "learning_rate": 3.0252425044091714e-06,
      "loss": 0.0012,
      "step": 30800
    },
    {
      "epoch": 0.8490410052910053,
      "grad_norm": 0.00934236403554678,
      "learning_rate": 3.0197310405643743e-06,
      "loss": 0.0875,
      "step": 30810
    },
    {
      "epoch": 0.8493165784832452,
      "grad_norm": 0.009001355618238449,
      "learning_rate": 3.014219576719577e-06,
      "loss": 0.252,
      "step": 30820
    },
    {
      "epoch": 0.849592151675485,
      "grad_norm": 125.048828125,
      "learning_rate": 3.0087081128747795e-06,
      "loss": 0.2265,
      "step": 30830
    },
    {
      "epoch": 0.8498677248677249,
      "grad_norm": 0.008335862308740616,
      "learning_rate": 3.0031966490299823e-06,
      "loss": 0.1755,
      "step": 30840
    },
    {
      "epoch": 0.8501432980599647,
      "grad_norm": 0.06174914911389351,
      "learning_rate": 2.9976851851851856e-06,
      "loss": 0.0646,
      "step": 30850
    },
    {
      "epoch": 0.8504188712522046,
      "grad_norm": 3.6620676517486572,
      "learning_rate": 2.9921737213403884e-06,
      "loss": 0.109,
      "step": 30860
    },
    {
      "epoch": 0.8506944444444444,
      "grad_norm": 56.271217346191406,
      "learning_rate": 2.9866622574955913e-06,
      "loss": 0.1106,
      "step": 30870
    },
    {
      "epoch": 0.8509700176366843,
      "grad_norm": 0.04177871346473694,
      "learning_rate": 2.9811507936507937e-06,
      "loss": 0.0402,
      "step": 30880
    },
    {
      "epoch": 0.8512455908289241,
      "grad_norm": 78.97677612304688,
      "learning_rate": 2.9756393298059965e-06,
      "loss": 0.0926,
      "step": 30890
    },
    {
      "epoch": 0.8515211640211641,
      "grad_norm": 0.058077678084373474,
      "learning_rate": 2.9701278659611993e-06,
      "loss": 0.0671,
      "step": 30900
    },
    {
      "epoch": 0.8517967372134039,
      "grad_norm": 0.3998667299747467,
      "learning_rate": 2.9646164021164026e-06,
      "loss": 0.0497,
      "step": 30910
    },
    {
      "epoch": 0.8520723104056437,
      "grad_norm": 0.027365323156118393,
      "learning_rate": 2.9591049382716054e-06,
      "loss": 0.0744,
      "step": 30920
    },
    {
      "epoch": 0.8523478835978836,
      "grad_norm": 0.010489687323570251,
      "learning_rate": 2.953593474426808e-06,
      "loss": 0.1431,
      "step": 30930
    },
    {
      "epoch": 0.8526234567901234,
      "grad_norm": 0.01087669562548399,
      "learning_rate": 2.9480820105820107e-06,
      "loss": 0.0013,
      "step": 30940
    },
    {
      "epoch": 0.8528990299823633,
      "grad_norm": 0.011912578716874123,
      "learning_rate": 2.9425705467372135e-06,
      "loss": 0.1919,
      "step": 30950
    },
    {
      "epoch": 0.8531746031746031,
      "grad_norm": 38.25611877441406,
      "learning_rate": 2.9370590828924168e-06,
      "loss": 0.183,
      "step": 30960
    },
    {
      "epoch": 0.8534501763668431,
      "grad_norm": 0.023526962846517563,
      "learning_rate": 2.9315476190476196e-06,
      "loss": 0.008,
      "step": 30970
    },
    {
      "epoch": 0.8537257495590829,
      "grad_norm": 0.007649594452232122,
      "learning_rate": 2.926036155202822e-06,
      "loss": 0.0938,
      "step": 30980
    },
    {
      "epoch": 0.8540013227513228,
      "grad_norm": 138.56752014160156,
      "learning_rate": 2.920524691358025e-06,
      "loss": 0.0418,
      "step": 30990
    },
    {
      "epoch": 0.8542768959435626,
      "grad_norm": 42.02815246582031,
      "learning_rate": 2.9150132275132277e-06,
      "loss": 0.0448,
      "step": 31000
    },
    {
      "epoch": 0.8545524691358025,
      "grad_norm": 0.0226757675409317,
      "learning_rate": 2.909501763668431e-06,
      "loss": 0.0028,
      "step": 31010
    },
    {
      "epoch": 0.8548280423280423,
      "grad_norm": 0.009882030077278614,
      "learning_rate": 2.9039902998236334e-06,
      "loss": 0.0103,
      "step": 31020
    },
    {
      "epoch": 0.8551036155202821,
      "grad_norm": 0.007310186047106981,
      "learning_rate": 2.898478835978836e-06,
      "loss": 0.0189,
      "step": 31030
    },
    {
      "epoch": 0.855379188712522,
      "grad_norm": 0.009424817748367786,
      "learning_rate": 2.892967372134039e-06,
      "loss": 0.0032,
      "step": 31040
    },
    {
      "epoch": 0.8556547619047619,
      "grad_norm": 0.023697784170508385,
      "learning_rate": 2.887455908289242e-06,
      "loss": 0.0019,
      "step": 31050
    },
    {
      "epoch": 0.8559303350970018,
      "grad_norm": 0.03122481144964695,
      "learning_rate": 2.8819444444444443e-06,
      "loss": 0.0026,
      "step": 31060
    },
    {
      "epoch": 0.8562059082892416,
      "grad_norm": 0.01966201700270176,
      "learning_rate": 2.8764329805996475e-06,
      "loss": 0.2009,
      "step": 31070
    },
    {
      "epoch": 0.8564814814814815,
      "grad_norm": 0.010791665874421597,
      "learning_rate": 2.8709215167548504e-06,
      "loss": 0.054,
      "step": 31080
    },
    {
      "epoch": 0.8567570546737213,
      "grad_norm": 157.36476135253906,
      "learning_rate": 2.8654100529100532e-06,
      "loss": 0.1202,
      "step": 31090
    },
    {
      "epoch": 0.8570326278659612,
      "grad_norm": 1.1777070760726929,
      "learning_rate": 2.859898589065256e-06,
      "loss": 0.0006,
      "step": 31100
    },
    {
      "epoch": 0.857308201058201,
      "grad_norm": 0.019648166373372078,
      "learning_rate": 2.8543871252204585e-06,
      "loss": 0.081,
      "step": 31110
    },
    {
      "epoch": 0.857583774250441,
      "grad_norm": 9.101902961730957,
      "learning_rate": 2.8488756613756617e-06,
      "loss": 0.1483,
      "step": 31120
    },
    {
      "epoch": 0.8578593474426808,
      "grad_norm": 0.00685462448745966,
      "learning_rate": 2.8433641975308646e-06,
      "loss": 0.0094,
      "step": 31130
    },
    {
      "epoch": 0.8581349206349206,
      "grad_norm": 0.1272754967212677,
      "learning_rate": 2.8378527336860674e-06,
      "loss": 0.1915,
      "step": 31140
    },
    {
      "epoch": 0.8584104938271605,
      "grad_norm": 0.011554618366062641,
      "learning_rate": 2.83234126984127e-06,
      "loss": 0.0827,
      "step": 31150
    },
    {
      "epoch": 0.8586860670194003,
      "grad_norm": 0.020141638815402985,
      "learning_rate": 2.8268298059964726e-06,
      "loss": 0.0552,
      "step": 31160
    },
    {
      "epoch": 0.8589616402116402,
      "grad_norm": 0.009189801290631294,
      "learning_rate": 2.821318342151676e-06,
      "loss": 0.0541,
      "step": 31170
    },
    {
      "epoch": 0.85923721340388,
      "grad_norm": 7.082774639129639,
      "learning_rate": 2.8158068783068787e-06,
      "loss": 0.0015,
      "step": 31180
    },
    {
      "epoch": 0.85951278659612,
      "grad_norm": 0.0066185216419398785,
      "learning_rate": 2.8102954144620816e-06,
      "loss": 0.0024,
      "step": 31190
    },
    {
      "epoch": 0.8597883597883598,
      "grad_norm": 0.34750649333000183,
      "learning_rate": 2.804783950617284e-06,
      "loss": 0.0386,
      "step": 31200
    },
    {
      "epoch": 0.8600639329805997,
      "grad_norm": 142.88319396972656,
      "learning_rate": 2.799272486772487e-06,
      "loss": 0.0863,
      "step": 31210
    },
    {
      "epoch": 0.8603395061728395,
      "grad_norm": 0.10046399384737015,
      "learning_rate": 2.7937610229276896e-06,
      "loss": 0.1119,
      "step": 31220
    },
    {
      "epoch": 0.8606150793650794,
      "grad_norm": 12.789823532104492,
      "learning_rate": 2.788249559082893e-06,
      "loss": 0.1752,
      "step": 31230
    },
    {
      "epoch": 0.8608906525573192,
      "grad_norm": 0.04307232052087784,
      "learning_rate": 2.7827380952380957e-06,
      "loss": 0.0851,
      "step": 31240
    },
    {
      "epoch": 0.861166225749559,
      "grad_norm": 0.02096516825258732,
      "learning_rate": 2.777226631393298e-06,
      "loss": 0.0208,
      "step": 31250
    },
    {
      "epoch": 0.861441798941799,
      "grad_norm": 0.04335416480898857,
      "learning_rate": 2.771715167548501e-06,
      "loss": 0.1271,
      "step": 31260
    },
    {
      "epoch": 0.8617173721340388,
      "grad_norm": 0.007002823054790497,
      "learning_rate": 2.766203703703704e-06,
      "loss": 0.216,
      "step": 31270
    },
    {
      "epoch": 0.8619929453262787,
      "grad_norm": 0.06776019930839539,
      "learning_rate": 2.760692239858907e-06,
      "loss": 0.0146,
      "step": 31280
    },
    {
      "epoch": 0.8622685185185185,
      "grad_norm": 9.98179817199707,
      "learning_rate": 2.75518077601411e-06,
      "loss": 0.0016,
      "step": 31290
    },
    {
      "epoch": 0.8625440917107584,
      "grad_norm": 154.6234130859375,
      "learning_rate": 2.7496693121693123e-06,
      "loss": 0.0523,
      "step": 31300
    },
    {
      "epoch": 0.8628196649029982,
      "grad_norm": 9.318845748901367,
      "learning_rate": 2.744157848324515e-06,
      "loss": 0.003,
      "step": 31310
    },
    {
      "epoch": 0.8630952380952381,
      "grad_norm": 0.0320378802716732,
      "learning_rate": 2.738646384479718e-06,
      "loss": 0.0187,
      "step": 31320
    },
    {
      "epoch": 0.863370811287478,
      "grad_norm": 0.011428307741880417,
      "learning_rate": 2.7331349206349204e-06,
      "loss": 0.0015,
      "step": 31330
    },
    {
      "epoch": 0.8636463844797179,
      "grad_norm": 0.04211347922682762,
      "learning_rate": 2.7276234567901237e-06,
      "loss": 0.0007,
      "step": 31340
    },
    {
      "epoch": 0.8639219576719577,
      "grad_norm": 5.223391532897949,
      "learning_rate": 2.7221119929453265e-06,
      "loss": 0.1927,
      "step": 31350
    },
    {
      "epoch": 0.8641975308641975,
      "grad_norm": 0.014269065111875534,
      "learning_rate": 2.7166005291005293e-06,
      "loss": 0.0951,
      "step": 31360
    },
    {
      "epoch": 0.8644731040564374,
      "grad_norm": 0.04658610373735428,
      "learning_rate": 2.711089065255732e-06,
      "loss": 0.2728,
      "step": 31370
    },
    {
      "epoch": 0.8647486772486772,
      "grad_norm": 0.11604256182909012,
      "learning_rate": 2.7055776014109346e-06,
      "loss": 0.2296,
      "step": 31380
    },
    {
      "epoch": 0.8650242504409171,
      "grad_norm": 0.011683114804327488,
      "learning_rate": 2.700066137566138e-06,
      "loss": 0.0342,
      "step": 31390
    },
    {
      "epoch": 0.8652998236331569,
      "grad_norm": 63.98749923706055,
      "learning_rate": 2.6945546737213407e-06,
      "loss": 0.0859,
      "step": 31400
    },
    {
      "epoch": 0.8655753968253969,
      "grad_norm": 0.35187414288520813,
      "learning_rate": 2.6890432098765435e-06,
      "loss": 0.0013,
      "step": 31410
    },
    {
      "epoch": 0.8658509700176367,
      "grad_norm": 0.007575969211757183,
      "learning_rate": 2.6835317460317463e-06,
      "loss": 0.2774,
      "step": 31420
    },
    {
      "epoch": 0.8661265432098766,
      "grad_norm": 0.011079294607043266,
      "learning_rate": 2.6780202821869488e-06,
      "loss": 0.0972,
      "step": 31430
    },
    {
      "epoch": 0.8664021164021164,
      "grad_norm": 107.76929473876953,
      "learning_rate": 2.672508818342152e-06,
      "loss": 0.2043,
      "step": 31440
    },
    {
      "epoch": 0.8666776895943563,
      "grad_norm": 12.983541488647461,
      "learning_rate": 2.666997354497355e-06,
      "loss": 0.0955,
      "step": 31450
    },
    {
      "epoch": 0.8669532627865961,
      "grad_norm": 71.80510711669922,
      "learning_rate": 2.6614858906525577e-06,
      "loss": 0.138,
      "step": 31460
    },
    {
      "epoch": 0.8672288359788359,
      "grad_norm": 3.927597999572754,
      "learning_rate": 2.65597442680776e-06,
      "loss": 0.0998,
      "step": 31470
    },
    {
      "epoch": 0.8675044091710759,
      "grad_norm": 0.00902046449482441,
      "learning_rate": 2.650462962962963e-06,
      "loss": 0.0586,
      "step": 31480
    },
    {
      "epoch": 0.8677799823633157,
      "grad_norm": 133.5558319091797,
      "learning_rate": 2.6449514991181658e-06,
      "loss": 0.0412,
      "step": 31490
    },
    {
      "epoch": 0.8680555555555556,
      "grad_norm": 0.008601036854088306,
      "learning_rate": 2.639440035273369e-06,
      "loss": 0.0436,
      "step": 31500
    },
    {
      "epoch": 0.8683311287477954,
      "grad_norm": 0.02251385524868965,
      "learning_rate": 2.633928571428572e-06,
      "loss": 0.0936,
      "step": 31510
    },
    {
      "epoch": 0.8686067019400353,
      "grad_norm": 0.06651312112808228,
      "learning_rate": 2.6284171075837743e-06,
      "loss": 0.0483,
      "step": 31520
    },
    {
      "epoch": 0.8688822751322751,
      "grad_norm": 0.010724268853664398,
      "learning_rate": 2.622905643738977e-06,
      "loss": 0.0548,
      "step": 31530
    },
    {
      "epoch": 0.869157848324515,
      "grad_norm": 0.007808349095284939,
      "learning_rate": 2.61739417989418e-06,
      "loss": 0.0909,
      "step": 31540
    },
    {
      "epoch": 0.8694334215167548,
      "grad_norm": 0.03316527605056763,
      "learning_rate": 2.611882716049383e-06,
      "loss": 0.0359,
      "step": 31550
    },
    {
      "epoch": 0.8697089947089947,
      "grad_norm": 69.89598083496094,
      "learning_rate": 2.606371252204586e-06,
      "loss": 0.0088,
      "step": 31560
    },
    {
      "epoch": 0.8699845679012346,
      "grad_norm": 0.07394669204950333,
      "learning_rate": 2.6008597883597885e-06,
      "loss": 0.1113,
      "step": 31570
    },
    {
      "epoch": 0.8702601410934744,
      "grad_norm": 0.04747648909687996,
      "learning_rate": 2.5953483245149913e-06,
      "loss": 0.0006,
      "step": 31580
    },
    {
      "epoch": 0.8705357142857143,
      "grad_norm": 0.011162138544023037,
      "learning_rate": 2.589836860670194e-06,
      "loss": 0.0756,
      "step": 31590
    },
    {
      "epoch": 0.8708112874779541,
      "grad_norm": 0.010951264761388302,
      "learning_rate": 2.5843253968253974e-06,
      "loss": 0.1229,
      "step": 31600
    },
    {
      "epoch": 0.871086860670194,
      "grad_norm": 0.0063920882530510426,
      "learning_rate": 2.5788139329806002e-06,
      "loss": 0.0222,
      "step": 31610
    },
    {
      "epoch": 0.8713624338624338,
      "grad_norm": 161.15066528320312,
      "learning_rate": 2.5733024691358026e-06,
      "loss": 0.1501,
      "step": 31620
    },
    {
      "epoch": 0.8716380070546738,
      "grad_norm": 0.11195066571235657,
      "learning_rate": 2.5677910052910055e-06,
      "loss": 0.0991,
      "step": 31630
    },
    {
      "epoch": 0.8719135802469136,
      "grad_norm": 0.008137832395732403,
      "learning_rate": 2.5622795414462083e-06,
      "loss": 0.0005,
      "step": 31640
    },
    {
      "epoch": 0.8721891534391535,
      "grad_norm": 0.03777899593114853,
      "learning_rate": 2.5567680776014107e-06,
      "loss": 0.1401,
      "step": 31650
    },
    {
      "epoch": 0.8724647266313933,
      "grad_norm": 0.024182891473174095,
      "learning_rate": 2.551256613756614e-06,
      "loss": 0.0521,
      "step": 31660
    },
    {
      "epoch": 0.8727402998236331,
      "grad_norm": 0.00719311460852623,
      "learning_rate": 2.545745149911817e-06,
      "loss": 0.0741,
      "step": 31670
    },
    {
      "epoch": 0.873015873015873,
      "grad_norm": 0.02091505192220211,
      "learning_rate": 2.5402336860670196e-06,
      "loss": 0.0272,
      "step": 31680
    },
    {
      "epoch": 0.8732914462081128,
      "grad_norm": 0.6349891424179077,
      "learning_rate": 2.5347222222222225e-06,
      "loss": 0.0463,
      "step": 31690
    },
    {
      "epoch": 0.8735670194003528,
      "grad_norm": 19.293500900268555,
      "learning_rate": 2.529210758377425e-06,
      "loss": 0.1,
      "step": 31700
    },
    {
      "epoch": 0.8738425925925926,
      "grad_norm": 0.11905057728290558,
      "learning_rate": 2.523699294532628e-06,
      "loss": 0.0418,
      "step": 31710
    },
    {
      "epoch": 0.8741181657848325,
      "grad_norm": 0.00853788759559393,
      "learning_rate": 2.518187830687831e-06,
      "loss": 0.085,
      "step": 31720
    },
    {
      "epoch": 0.8743937389770723,
      "grad_norm": 13.739009857177734,
      "learning_rate": 2.512676366843034e-06,
      "loss": 0.0143,
      "step": 31730
    },
    {
      "epoch": 0.8746693121693122,
      "grad_norm": 0.3880687654018402,
      "learning_rate": 2.5071649029982366e-06,
      "loss": 0.0327,
      "step": 31740
    },
    {
      "epoch": 0.874944885361552,
      "grad_norm": 0.051425088196992874,
      "learning_rate": 2.501653439153439e-06,
      "loss": 0.0104,
      "step": 31750
    },
    {
      "epoch": 0.8752204585537919,
      "grad_norm": 0.02528192661702633,
      "learning_rate": 2.4961419753086423e-06,
      "loss": 0.1755,
      "step": 31760
    },
    {
      "epoch": 0.8754960317460317,
      "grad_norm": 0.01337543036788702,
      "learning_rate": 2.4906305114638447e-06,
      "loss": 0.2458,
      "step": 31770
    },
    {
      "epoch": 0.8757716049382716,
      "grad_norm": 168.69244384765625,
      "learning_rate": 2.485119047619048e-06,
      "loss": 0.0272,
      "step": 31780
    },
    {
      "epoch": 0.8760471781305115,
      "grad_norm": 0.04618275538086891,
      "learning_rate": 2.4796075837742504e-06,
      "loss": 0.0689,
      "step": 31790
    },
    {
      "epoch": 0.8763227513227513,
      "grad_norm": 0.0761592835187912,
      "learning_rate": 2.4740961199294537e-06,
      "loss": 0.1405,
      "step": 31800
    },
    {
      "epoch": 0.8765983245149912,
      "grad_norm": 0.051730308681726456,
      "learning_rate": 2.4685846560846565e-06,
      "loss": 0.0012,
      "step": 31810
    },
    {
      "epoch": 0.876873897707231,
      "grad_norm": 0.009769166819751263,
      "learning_rate": 2.463073192239859e-06,
      "loss": 0.2846,
      "step": 31820
    },
    {
      "epoch": 0.8771494708994709,
      "grad_norm": 8.421223640441895,
      "learning_rate": 2.457561728395062e-06,
      "loss": 0.1565,
      "step": 31830
    },
    {
      "epoch": 0.8774250440917107,
      "grad_norm": 0.05176090449094772,
      "learning_rate": 2.4520502645502646e-06,
      "loss": 0.0019,
      "step": 31840
    },
    {
      "epoch": 0.8777006172839507,
      "grad_norm": 0.008714779280126095,
      "learning_rate": 2.4465388007054674e-06,
      "loss": 0.0823,
      "step": 31850
    },
    {
      "epoch": 0.8779761904761905,
      "grad_norm": 0.05124587193131447,
      "learning_rate": 2.4410273368606702e-06,
      "loss": 0.2308,
      "step": 31860
    },
    {
      "epoch": 0.8782517636684304,
      "grad_norm": 0.05010560154914856,
      "learning_rate": 2.435515873015873e-06,
      "loss": 0.1496,
      "step": 31870
    },
    {
      "epoch": 0.8785273368606702,
      "grad_norm": 0.009133415296673775,
      "learning_rate": 2.4300044091710763e-06,
      "loss": 0.1197,
      "step": 31880
    },
    {
      "epoch": 0.87880291005291,
      "grad_norm": 0.03882020711898804,
      "learning_rate": 2.4244929453262787e-06,
      "loss": 0.0162,
      "step": 31890
    },
    {
      "epoch": 0.8790784832451499,
      "grad_norm": 0.02034643106162548,
      "learning_rate": 2.4189814814814816e-06,
      "loss": 0.1748,
      "step": 31900
    },
    {
      "epoch": 0.8793540564373897,
      "grad_norm": 0.01950574852526188,
      "learning_rate": 2.4134700176366844e-06,
      "loss": 0.0006,
      "step": 31910
    },
    {
      "epoch": 0.8796296296296297,
      "grad_norm": 0.08107422292232513,
      "learning_rate": 2.4079585537918873e-06,
      "loss": 0.068,
      "step": 31920
    },
    {
      "epoch": 0.8799052028218695,
      "grad_norm": 0.010431855916976929,
      "learning_rate": 2.40244708994709e-06,
      "loss": 0.1219,
      "step": 31930
    },
    {
      "epoch": 0.8801807760141094,
      "grad_norm": 173.58436584472656,
      "learning_rate": 2.396935626102293e-06,
      "loss": 0.0537,
      "step": 31940
    },
    {
      "epoch": 0.8804563492063492,
      "grad_norm": 0.3369230628013611,
      "learning_rate": 2.3914241622574958e-06,
      "loss": 0.0978,
      "step": 31950
    },
    {
      "epoch": 0.8807319223985891,
      "grad_norm": 0.019505726173520088,
      "learning_rate": 2.3859126984126986e-06,
      "loss": 0.0026,
      "step": 31960
    },
    {
      "epoch": 0.8810074955908289,
      "grad_norm": 0.013268020004034042,
      "learning_rate": 2.3804012345679014e-06,
      "loss": 0.1679,
      "step": 31970
    },
    {
      "epoch": 0.8812830687830688,
      "grad_norm": 0.24747449159622192,
      "learning_rate": 2.3748897707231043e-06,
      "loss": 0.0009,
      "step": 31980
    },
    {
      "epoch": 0.8815586419753086,
      "grad_norm": 0.09200137108564377,
      "learning_rate": 2.369378306878307e-06,
      "loss": 0.0006,
      "step": 31990
    },
    {
      "epoch": 0.8818342151675485,
      "grad_norm": 161.97195434570312,
      "learning_rate": 2.36386684303351e-06,
      "loss": 0.2162,
      "step": 32000
    },
    {
      "epoch": 0.8821097883597884,
      "grad_norm": 0.008533261716365814,
      "learning_rate": 2.3583553791887128e-06,
      "loss": 0.0984,
      "step": 32010
    },
    {
      "epoch": 0.8823853615520282,
      "grad_norm": 0.020609291270375252,
      "learning_rate": 2.3528439153439156e-06,
      "loss": 0.0833,
      "step": 32020
    },
    {
      "epoch": 0.8826609347442681,
      "grad_norm": 121.38349151611328,
      "learning_rate": 2.3473324514991184e-06,
      "loss": 0.2946,
      "step": 32030
    },
    {
      "epoch": 0.8829365079365079,
      "grad_norm": 93.74434661865234,
      "learning_rate": 2.3418209876543213e-06,
      "loss": 0.0216,
      "step": 32040
    },
    {
      "epoch": 0.8832120811287478,
      "grad_norm": 1.0918588638305664,
      "learning_rate": 2.336309523809524e-06,
      "loss": 0.0904,
      "step": 32050
    },
    {
      "epoch": 0.8834876543209876,
      "grad_norm": 0.759881854057312,
      "learning_rate": 2.330798059964727e-06,
      "loss": 0.0527,
      "step": 32060
    },
    {
      "epoch": 0.8837632275132276,
      "grad_norm": 0.028323080390691757,
      "learning_rate": 2.3252865961199298e-06,
      "loss": 0.0007,
      "step": 32070
    },
    {
      "epoch": 0.8840388007054674,
      "grad_norm": 0.031875498592853546,
      "learning_rate": 2.3197751322751326e-06,
      "loss": 0.0011,
      "step": 32080
    },
    {
      "epoch": 0.8843143738977073,
      "grad_norm": 12.199268341064453,
      "learning_rate": 2.314263668430335e-06,
      "loss": 0.2077,
      "step": 32090
    },
    {
      "epoch": 0.8845899470899471,
      "grad_norm": 0.011999131180346012,
      "learning_rate": 2.3087522045855383e-06,
      "loss": 0.1035,
      "step": 32100
    },
    {
      "epoch": 0.8848655202821869,
      "grad_norm": 0.010195431299507618,
      "learning_rate": 2.3032407407407407e-06,
      "loss": 0.0009,
      "step": 32110
    },
    {
      "epoch": 0.8851410934744268,
      "grad_norm": 124.95687866210938,
      "learning_rate": 2.297729276895944e-06,
      "loss": 0.0175,
      "step": 32120
    },
    {
      "epoch": 0.8854166666666666,
      "grad_norm": 0.007129834033548832,
      "learning_rate": 2.292217813051147e-06,
      "loss": 0.0339,
      "step": 32130
    },
    {
      "epoch": 0.8856922398589065,
      "grad_norm": 1.5371047258377075,
      "learning_rate": 2.286706349206349e-06,
      "loss": 0.0924,
      "step": 32140
    },
    {
      "epoch": 0.8859678130511464,
      "grad_norm": 0.012346566654741764,
      "learning_rate": 2.2811948853615525e-06,
      "loss": 0.0764,
      "step": 32150
    },
    {
      "epoch": 0.8862433862433863,
      "grad_norm": 0.19444648921489716,
      "learning_rate": 2.275683421516755e-06,
      "loss": 0.0973,
      "step": 32160
    },
    {
      "epoch": 0.8865189594356261,
      "grad_norm": 2.8236067295074463,
      "learning_rate": 2.2701719576719577e-06,
      "loss": 0.0009,
      "step": 32170
    },
    {
      "epoch": 0.886794532627866,
      "grad_norm": 0.04644471034407616,
      "learning_rate": 2.2646604938271605e-06,
      "loss": 0.0199,
      "step": 32180
    },
    {
      "epoch": 0.8870701058201058,
      "grad_norm": 0.013001031242311,
      "learning_rate": 2.2591490299823634e-06,
      "loss": 0.0007,
      "step": 32190
    },
    {
      "epoch": 0.8873456790123457,
      "grad_norm": 0.029538355767726898,
      "learning_rate": 2.2536375661375666e-06,
      "loss": 0.0884,
      "step": 32200
    },
    {
      "epoch": 0.8876212522045855,
      "grad_norm": 0.009632053785026073,
      "learning_rate": 2.248126102292769e-06,
      "loss": 0.0359,
      "step": 32210
    },
    {
      "epoch": 0.8878968253968254,
      "grad_norm": 148.90863037109375,
      "learning_rate": 2.242614638447972e-06,
      "loss": 0.1392,
      "step": 32220
    },
    {
      "epoch": 0.8881723985890653,
      "grad_norm": 0.013125550001859665,
      "learning_rate": 2.2371031746031747e-06,
      "loss": 0.1167,
      "step": 32230
    },
    {
      "epoch": 0.8884479717813051,
      "grad_norm": 0.008668789640069008,
      "learning_rate": 2.2315917107583776e-06,
      "loss": 0.098,
      "step": 32240
    },
    {
      "epoch": 0.888723544973545,
      "grad_norm": 11.560667037963867,
      "learning_rate": 2.2260802469135804e-06,
      "loss": 0.1821,
      "step": 32250
    },
    {
      "epoch": 0.8889991181657848,
      "grad_norm": 0.044304195791482925,
      "learning_rate": 2.2205687830687832e-06,
      "loss": 0.0024,
      "step": 32260
    },
    {
      "epoch": 0.8892746913580247,
      "grad_norm": 2.4232590198516846,
      "learning_rate": 2.215057319223986e-06,
      "loss": 0.098,
      "step": 32270
    },
    {
      "epoch": 0.8895502645502645,
      "grad_norm": 1.4819633960723877,
      "learning_rate": 2.209545855379189e-06,
      "loss": 0.0929,
      "step": 32280
    },
    {
      "epoch": 0.8898258377425045,
      "grad_norm": 0.046324472874403,
      "learning_rate": 2.2040343915343917e-06,
      "loss": 0.0735,
      "step": 32290
    },
    {
      "epoch": 0.8901014109347443,
      "grad_norm": 0.021319279447197914,
      "learning_rate": 2.1985229276895946e-06,
      "loss": 0.224,
      "step": 32300
    },
    {
      "epoch": 0.8903769841269841,
      "grad_norm": 10.728830337524414,
      "learning_rate": 2.1930114638447974e-06,
      "loss": 0.0844,
      "step": 32310
    },
    {
      "epoch": 0.890652557319224,
      "grad_norm": 0.04226046800613403,
      "learning_rate": 2.1875000000000002e-06,
      "loss": 0.0604,
      "step": 32320
    },
    {
      "epoch": 0.8909281305114638,
      "grad_norm": 10.950453758239746,
      "learning_rate": 2.181988536155203e-06,
      "loss": 0.1298,
      "step": 32330
    },
    {
      "epoch": 0.8912037037037037,
      "grad_norm": 0.026450617238879204,
      "learning_rate": 2.176477072310406e-06,
      "loss": 0.1504,
      "step": 32340
    },
    {
      "epoch": 0.8914792768959435,
      "grad_norm": 0.0075193000957369804,
      "learning_rate": 2.1709656084656087e-06,
      "loss": 0.061,
      "step": 32350
    },
    {
      "epoch": 0.8917548500881834,
      "grad_norm": 0.012758397497236729,
      "learning_rate": 2.165454144620811e-06,
      "loss": 0.0886,
      "step": 32360
    },
    {
      "epoch": 0.8920304232804233,
      "grad_norm": 0.017626101151108742,
      "learning_rate": 2.1599426807760144e-06,
      "loss": 0.1109,
      "step": 32370
    },
    {
      "epoch": 0.8923059964726632,
      "grad_norm": 0.01396045833826065,
      "learning_rate": 2.1544312169312172e-06,
      "loss": 0.0006,
      "step": 32380
    },
    {
      "epoch": 0.892581569664903,
      "grad_norm": 0.007040354888886213,
      "learning_rate": 2.14891975308642e-06,
      "loss": 0.0834,
      "step": 32390
    },
    {
      "epoch": 0.8928571428571429,
      "grad_norm": 22.98527717590332,
      "learning_rate": 2.143408289241623e-06,
      "loss": 0.0106,
      "step": 32400
    },
    {
      "epoch": 0.8931327160493827,
      "grad_norm": 0.024093560874462128,
      "learning_rate": 2.1378968253968253e-06,
      "loss": 0.2278,
      "step": 32410
    },
    {
      "epoch": 0.8934082892416225,
      "grad_norm": 0.013176895678043365,
      "learning_rate": 2.1323853615520286e-06,
      "loss": 0.3288,
      "step": 32420
    },
    {
      "epoch": 0.8936838624338624,
      "grad_norm": 0.007922222837805748,
      "learning_rate": 2.126873897707231e-06,
      "loss": 0.1119,
      "step": 32430
    },
    {
      "epoch": 0.8939594356261023,
      "grad_norm": 0.2563558518886566,
      "learning_rate": 2.121362433862434e-06,
      "loss": 0.0877,
      "step": 32440
    },
    {
      "epoch": 0.8942350088183422,
      "grad_norm": 0.018077831715345383,
      "learning_rate": 2.115850970017637e-06,
      "loss": 0.2777,
      "step": 32450
    },
    {
      "epoch": 0.894510582010582,
      "grad_norm": 0.007346495985984802,
      "learning_rate": 2.1103395061728395e-06,
      "loss": 0.1799,
      "step": 32460
    },
    {
      "epoch": 0.8947861552028219,
      "grad_norm": 141.33555603027344,
      "learning_rate": 2.1048280423280428e-06,
      "loss": 0.1181,
      "step": 32470
    },
    {
      "epoch": 0.8950617283950617,
      "grad_norm": 0.007401254028081894,
      "learning_rate": 2.099316578483245e-06,
      "loss": 0.0137,
      "step": 32480
    },
    {
      "epoch": 0.8953373015873016,
      "grad_norm": 22.041919708251953,
      "learning_rate": 2.093805114638448e-06,
      "loss": 0.0729,
      "step": 32490
    },
    {
      "epoch": 0.8956128747795414,
      "grad_norm": 4.813798904418945,
      "learning_rate": 2.088293650793651e-06,
      "loss": 0.0317,
      "step": 32500
    },
    {
      "epoch": 0.8958884479717814,
      "grad_norm": 0.06277433782815933,
      "learning_rate": 2.0827821869488537e-06,
      "loss": 0.0558,
      "step": 32510
    },
    {
      "epoch": 0.8961640211640212,
      "grad_norm": 11.459161758422852,
      "learning_rate": 2.0772707231040565e-06,
      "loss": 0.2412,
      "step": 32520
    },
    {
      "epoch": 0.896439594356261,
      "grad_norm": 0.717009961605072,
      "learning_rate": 2.0717592592592593e-06,
      "loss": 0.1585,
      "step": 32530
    },
    {
      "epoch": 0.8967151675485009,
      "grad_norm": 0.045709095895290375,
      "learning_rate": 2.066247795414462e-06,
      "loss": 0.1735,
      "step": 32540
    },
    {
      "epoch": 0.8969907407407407,
      "grad_norm": 0.09949289262294769,
      "learning_rate": 2.060736331569665e-06,
      "loss": 0.0359,
      "step": 32550
    },
    {
      "epoch": 0.8972663139329806,
      "grad_norm": 60.60081100463867,
      "learning_rate": 2.055224867724868e-06,
      "loss": 0.1225,
      "step": 32560
    },
    {
      "epoch": 0.8975418871252204,
      "grad_norm": 148.32586669921875,
      "learning_rate": 2.0497134038800707e-06,
      "loss": 0.08,
      "step": 32570
    },
    {
      "epoch": 0.8978174603174603,
      "grad_norm": 0.00934970285743475,
      "learning_rate": 2.0442019400352735e-06,
      "loss": 0.0041,
      "step": 32580
    },
    {
      "epoch": 0.8980930335097002,
      "grad_norm": 4.166445255279541,
      "learning_rate": 2.0386904761904764e-06,
      "loss": 0.0154,
      "step": 32590
    },
    {
      "epoch": 0.8983686067019401,
      "grad_norm": 0.06027695909142494,
      "learning_rate": 2.033179012345679e-06,
      "loss": 0.0848,
      "step": 32600
    },
    {
      "epoch": 0.8986441798941799,
      "grad_norm": 110.27365112304688,
      "learning_rate": 2.027667548500882e-06,
      "loss": 0.0723,
      "step": 32610
    },
    {
      "epoch": 0.8989197530864198,
      "grad_norm": 0.05291695520281792,
      "learning_rate": 2.022156084656085e-06,
      "loss": 0.0034,
      "step": 32620
    },
    {
      "epoch": 0.8991953262786596,
      "grad_norm": 0.19239716231822968,
      "learning_rate": 2.0166446208112877e-06,
      "loss": 0.0935,
      "step": 32630
    },
    {
      "epoch": 0.8994708994708994,
      "grad_norm": 0.047892943024635315,
      "learning_rate": 2.0111331569664905e-06,
      "loss": 0.092,
      "step": 32640
    },
    {
      "epoch": 0.8997464726631393,
      "grad_norm": 0.011224736459553242,
      "learning_rate": 2.0056216931216934e-06,
      "loss": 0.2299,
      "step": 32650
    },
    {
      "epoch": 0.9000220458553791,
      "grad_norm": 0.01685442216694355,
      "learning_rate": 2.000110229276896e-06,
      "loss": 0.0005,
      "step": 32660
    },
    {
      "epoch": 0.9002976190476191,
      "grad_norm": 0.017973007634282112,
      "learning_rate": 1.994598765432099e-06,
      "loss": 0.2161,
      "step": 32670
    },
    {
      "epoch": 0.9005731922398589,
      "grad_norm": 0.008793909102678299,
      "learning_rate": 1.9890873015873014e-06,
      "loss": 0.1252,
      "step": 32680
    },
    {
      "epoch": 0.9008487654320988,
      "grad_norm": 0.007616290356963873,
      "learning_rate": 1.9835758377425047e-06,
      "loss": 0.1779,
      "step": 32690
    },
    {
      "epoch": 0.9011243386243386,
      "grad_norm": 0.21219195425510406,
      "learning_rate": 1.9780643738977075e-06,
      "loss": 0.0743,
      "step": 32700
    },
    {
      "epoch": 0.9013999118165785,
      "grad_norm": 0.007155238185077906,
      "learning_rate": 1.9725529100529104e-06,
      "loss": 0.0006,
      "step": 32710
    },
    {
      "epoch": 0.9016754850088183,
      "grad_norm": 19.220500946044922,
      "learning_rate": 1.967041446208113e-06,
      "loss": 0.0029,
      "step": 32720
    },
    {
      "epoch": 0.9019510582010583,
      "grad_norm": 0.011165709234774113,
      "learning_rate": 1.9615299823633156e-06,
      "loss": 0.102,
      "step": 32730
    },
    {
      "epoch": 0.9022266313932981,
      "grad_norm": 0.007603996898978949,
      "learning_rate": 1.956018518518519e-06,
      "loss": 0.0487,
      "step": 32740
    },
    {
      "epoch": 0.9025022045855379,
      "grad_norm": 0.8859846591949463,
      "learning_rate": 1.9505070546737213e-06,
      "loss": 0.1047,
      "step": 32750
    },
    {
      "epoch": 0.9027777777777778,
      "grad_norm": 0.12924693524837494,
      "learning_rate": 1.944995590828924e-06,
      "loss": 0.0455,
      "step": 32760
    },
    {
      "epoch": 0.9030533509700176,
      "grad_norm": 0.028167057782411575,
      "learning_rate": 1.9394841269841274e-06,
      "loss": 0.0214,
      "step": 32770
    },
    {
      "epoch": 0.9033289241622575,
      "grad_norm": 0.2694851756095886,
      "learning_rate": 1.93397266313933e-06,
      "loss": 0.0223,
      "step": 32780
    },
    {
      "epoch": 0.9036044973544973,
      "grad_norm": 18.104442596435547,
      "learning_rate": 1.928461199294533e-06,
      "loss": 0.074,
      "step": 32790
    },
    {
      "epoch": 0.9038800705467372,
      "grad_norm": 0.014040518552064896,
      "learning_rate": 1.9229497354497355e-06,
      "loss": 0.0771,
      "step": 32800
    },
    {
      "epoch": 0.904155643738977,
      "grad_norm": 0.07153453677892685,
      "learning_rate": 1.9174382716049383e-06,
      "loss": 0.0403,
      "step": 32810
    },
    {
      "epoch": 0.904431216931217,
      "grad_norm": 22.21916389465332,
      "learning_rate": 1.911926807760141e-06,
      "loss": 0.1612,
      "step": 32820
    },
    {
      "epoch": 0.9047067901234568,
      "grad_norm": 147.0133819580078,
      "learning_rate": 1.906415343915344e-06,
      "loss": 0.0511,
      "step": 32830
    },
    {
      "epoch": 0.9049823633156967,
      "grad_norm": 1.409026026725769,
      "learning_rate": 1.9009038800705468e-06,
      "loss": 0.0597,
      "step": 32840
    },
    {
      "epoch": 0.9052579365079365,
      "grad_norm": 0.00962471216917038,
      "learning_rate": 1.8953924162257499e-06,
      "loss": 0.0009,
      "step": 32850
    },
    {
      "epoch": 0.9055335097001763,
      "grad_norm": 0.02087373286485672,
      "learning_rate": 1.8898809523809525e-06,
      "loss": 0.1382,
      "step": 32860
    },
    {
      "epoch": 0.9058090828924162,
      "grad_norm": 302.0904846191406,
      "learning_rate": 1.8843694885361555e-06,
      "loss": 0.3528,
      "step": 32870
    },
    {
      "epoch": 0.906084656084656,
      "grad_norm": 0.022929226979613304,
      "learning_rate": 1.8788580246913581e-06,
      "loss": 0.0018,
      "step": 32880
    },
    {
      "epoch": 0.906360229276896,
      "grad_norm": 0.01238131895661354,
      "learning_rate": 1.873346560846561e-06,
      "loss": 0.0795,
      "step": 32890
    },
    {
      "epoch": 0.9066358024691358,
      "grad_norm": 0.03263581171631813,
      "learning_rate": 1.8678350970017638e-06,
      "loss": 0.0012,
      "step": 32900
    },
    {
      "epoch": 0.9069113756613757,
      "grad_norm": 0.48720982670783997,
      "learning_rate": 1.8623236331569667e-06,
      "loss": 0.0011,
      "step": 32910
    },
    {
      "epoch": 0.9071869488536155,
      "grad_norm": 0.008487668819725513,
      "learning_rate": 1.8568121693121693e-06,
      "loss": 0.0322,
      "step": 32920
    },
    {
      "epoch": 0.9074625220458554,
      "grad_norm": 0.010471486486494541,
      "learning_rate": 1.8513007054673723e-06,
      "loss": 0.0005,
      "step": 32930
    },
    {
      "epoch": 0.9077380952380952,
      "grad_norm": 0.010177366435527802,
      "learning_rate": 1.8457892416225752e-06,
      "loss": 0.0016,
      "step": 32940
    },
    {
      "epoch": 0.9080136684303352,
      "grad_norm": 29.687477111816406,
      "learning_rate": 1.840277777777778e-06,
      "loss": 0.088,
      "step": 32950
    },
    {
      "epoch": 0.908289241622575,
      "grad_norm": 0.007391768507659435,
      "learning_rate": 1.8347663139329808e-06,
      "loss": 0.0304,
      "step": 32960
    },
    {
      "epoch": 0.9085648148148148,
      "grad_norm": 0.06279876828193665,
      "learning_rate": 1.8292548500881835e-06,
      "loss": 0.0666,
      "step": 32970
    },
    {
      "epoch": 0.9088403880070547,
      "grad_norm": 0.010824548080563545,
      "learning_rate": 1.8237433862433865e-06,
      "loss": 0.008,
      "step": 32980
    },
    {
      "epoch": 0.9091159611992945,
      "grad_norm": 54.709686279296875,
      "learning_rate": 1.8182319223985891e-06,
      "loss": 0.0131,
      "step": 32990
    },
    {
      "epoch": 0.9093915343915344,
      "grad_norm": 48.44334030151367,
      "learning_rate": 1.812720458553792e-06,
      "loss": 0.1348,
      "step": 33000
    },
    {
      "epoch": 0.9096671075837742,
      "grad_norm": 3.837327718734741,
      "learning_rate": 1.807208994708995e-06,
      "loss": 0.1461,
      "step": 33010
    },
    {
      "epoch": 0.9099426807760141,
      "grad_norm": 0.017696931958198547,
      "learning_rate": 1.8016975308641976e-06,
      "loss": 0.1783,
      "step": 33020
    },
    {
      "epoch": 0.910218253968254,
      "grad_norm": 0.13326187431812286,
      "learning_rate": 1.7961860670194007e-06,
      "loss": 0.0006,
      "step": 33030
    },
    {
      "epoch": 0.9104938271604939,
      "grad_norm": 0.021754322573542595,
      "learning_rate": 1.7906746031746033e-06,
      "loss": 0.1309,
      "step": 33040
    },
    {
      "epoch": 0.9107694003527337,
      "grad_norm": 0.030650604516267776,
      "learning_rate": 1.7851631393298061e-06,
      "loss": 0.0016,
      "step": 33050
    },
    {
      "epoch": 0.9110449735449735,
      "grad_norm": 0.093874491751194,
      "learning_rate": 1.779651675485009e-06,
      "loss": 0.2343,
      "step": 33060
    },
    {
      "epoch": 0.9113205467372134,
      "grad_norm": 0.030087154358625412,
      "learning_rate": 1.7741402116402118e-06,
      "loss": 0.0166,
      "step": 33070
    },
    {
      "epoch": 0.9115961199294532,
      "grad_norm": 0.035580847412347794,
      "learning_rate": 1.7686287477954144e-06,
      "loss": 0.1944,
      "step": 33080
    },
    {
      "epoch": 0.9118716931216931,
      "grad_norm": 1.096326470375061,
      "learning_rate": 1.7631172839506175e-06,
      "loss": 0.0007,
      "step": 33090
    },
    {
      "epoch": 0.912147266313933,
      "grad_norm": 0.0068023656494915485,
      "learning_rate": 1.7576058201058203e-06,
      "loss": 0.1156,
      "step": 33100
    },
    {
      "epoch": 0.9124228395061729,
      "grad_norm": 0.01618623174726963,
      "learning_rate": 1.7520943562610231e-06,
      "loss": 0.0006,
      "step": 33110
    },
    {
      "epoch": 0.9126984126984127,
      "grad_norm": 0.01836269721388817,
      "learning_rate": 1.746582892416226e-06,
      "loss": 0.0007,
      "step": 33120
    },
    {
      "epoch": 0.9129739858906526,
      "grad_norm": 0.011231346987187862,
      "learning_rate": 1.7410714285714286e-06,
      "loss": 0.1686,
      "step": 33130
    },
    {
      "epoch": 0.9132495590828924,
      "grad_norm": 0.014964312314987183,
      "learning_rate": 1.7355599647266316e-06,
      "loss": 0.0823,
      "step": 33140
    },
    {
      "epoch": 0.9135251322751323,
      "grad_norm": 0.05127633735537529,
      "learning_rate": 1.7300485008818343e-06,
      "loss": 0.1049,
      "step": 33150
    },
    {
      "epoch": 0.9138007054673721,
      "grad_norm": 0.25184664130210876,
      "learning_rate": 1.724537037037037e-06,
      "loss": 0.148,
      "step": 33160
    },
    {
      "epoch": 0.9140762786596119,
      "grad_norm": 0.00939062237739563,
      "learning_rate": 1.7190255731922402e-06,
      "loss": 0.072,
      "step": 33170
    },
    {
      "epoch": 0.9143518518518519,
      "grad_norm": 0.030753083527088165,
      "learning_rate": 1.7135141093474428e-06,
      "loss": 0.0673,
      "step": 33180
    },
    {
      "epoch": 0.9146274250440917,
      "grad_norm": 108.07727813720703,
      "learning_rate": 1.7080026455026458e-06,
      "loss": 0.1347,
      "step": 33190
    },
    {
      "epoch": 0.9149029982363316,
      "grad_norm": 0.012425253167748451,
      "learning_rate": 1.7024911816578484e-06,
      "loss": 0.0576,
      "step": 33200
    },
    {
      "epoch": 0.9151785714285714,
      "grad_norm": 0.013981575146317482,
      "learning_rate": 1.6969797178130513e-06,
      "loss": 0.0795,
      "step": 33210
    },
    {
      "epoch": 0.9154541446208113,
      "grad_norm": 0.0075228516943752766,
      "learning_rate": 1.6914682539682541e-06,
      "loss": 0.0018,
      "step": 33220
    },
    {
      "epoch": 0.9157297178130511,
      "grad_norm": 0.010575808584690094,
      "learning_rate": 1.685956790123457e-06,
      "loss": 0.0759,
      "step": 33230
    },
    {
      "epoch": 0.916005291005291,
      "grad_norm": 0.021556999534368515,
      "learning_rate": 1.6804453262786596e-06,
      "loss": 0.0007,
      "step": 33240
    },
    {
      "epoch": 0.9162808641975309,
      "grad_norm": 0.06851452589035034,
      "learning_rate": 1.6749338624338626e-06,
      "loss": 0.0865,
      "step": 33250
    },
    {
      "epoch": 0.9165564373897708,
      "grad_norm": 70.8831787109375,
      "learning_rate": 1.6694223985890655e-06,
      "loss": 0.0072,
      "step": 33260
    },
    {
      "epoch": 0.9168320105820106,
      "grad_norm": 0.006990905851125717,
      "learning_rate": 1.6639109347442683e-06,
      "loss": 0.1586,
      "step": 33270
    },
    {
      "epoch": 0.9171075837742504,
      "grad_norm": 0.010260564275085926,
      "learning_rate": 1.6583994708994711e-06,
      "loss": 0.0835,
      "step": 33280
    },
    {
      "epoch": 0.9173831569664903,
      "grad_norm": 0.006413906812667847,
      "learning_rate": 1.6528880070546737e-06,
      "loss": 0.0933,
      "step": 33290
    },
    {
      "epoch": 0.9176587301587301,
      "grad_norm": 0.02998911216855049,
      "learning_rate": 1.6473765432098768e-06,
      "loss": 0.0997,
      "step": 33300
    },
    {
      "epoch": 0.91793430335097,
      "grad_norm": 20.706119537353516,
      "learning_rate": 1.6418650793650794e-06,
      "loss": 0.1914,
      "step": 33310
    },
    {
      "epoch": 0.9182098765432098,
      "grad_norm": 0.01761396788060665,
      "learning_rate": 1.6363536155202823e-06,
      "loss": 0.0132,
      "step": 33320
    },
    {
      "epoch": 0.9184854497354498,
      "grad_norm": 11.538488388061523,
      "learning_rate": 1.6308421516754853e-06,
      "loss": 0.0013,
      "step": 33330
    },
    {
      "epoch": 0.9187610229276896,
      "grad_norm": 0.05485043302178383,
      "learning_rate": 1.625330687830688e-06,
      "loss": 0.1134,
      "step": 33340
    },
    {
      "epoch": 0.9190365961199295,
      "grad_norm": 0.02053971402347088,
      "learning_rate": 1.619819223985891e-06,
      "loss": 0.075,
      "step": 33350
    },
    {
      "epoch": 0.9193121693121693,
      "grad_norm": 0.012567082419991493,
      "learning_rate": 1.6143077601410936e-06,
      "loss": 0.0005,
      "step": 33360
    },
    {
      "epoch": 0.9195877425044092,
      "grad_norm": 0.013725554570555687,
      "learning_rate": 1.6087962962962964e-06,
      "loss": 0.0808,
      "step": 33370
    },
    {
      "epoch": 0.919863315696649,
      "grad_norm": 0.019590239971876144,
      "learning_rate": 1.6032848324514993e-06,
      "loss": 0.0091,
      "step": 33380
    },
    {
      "epoch": 0.9201388888888888,
      "grad_norm": 0.0077592735178768635,
      "learning_rate": 1.597773368606702e-06,
      "loss": 0.0098,
      "step": 33390
    },
    {
      "epoch": 0.9204144620811288,
      "grad_norm": 0.012975003570318222,
      "learning_rate": 1.5922619047619047e-06,
      "loss": 0.0806,
      "step": 33400
    },
    {
      "epoch": 0.9206900352733686,
      "grad_norm": 0.08765409886837006,
      "learning_rate": 1.5867504409171078e-06,
      "loss": 0.0472,
      "step": 33410
    },
    {
      "epoch": 0.9209656084656085,
      "grad_norm": 0.02170988917350769,
      "learning_rate": 1.5812389770723106e-06,
      "loss": 0.0372,
      "step": 33420
    },
    {
      "epoch": 0.9212411816578483,
      "grad_norm": 135.06210327148438,
      "learning_rate": 1.5757275132275134e-06,
      "loss": 0.0617,
      "step": 33430
    },
    {
      "epoch": 0.9215167548500882,
      "grad_norm": 170.60983276367188,
      "learning_rate": 1.5702160493827163e-06,
      "loss": 0.108,
      "step": 33440
    },
    {
      "epoch": 0.921792328042328,
      "grad_norm": 0.01635962538421154,
      "learning_rate": 1.564704585537919e-06,
      "loss": 0.0224,
      "step": 33450
    },
    {
      "epoch": 0.9220679012345679,
      "grad_norm": 108.16832733154297,
      "learning_rate": 1.559193121693122e-06,
      "loss": 0.0274,
      "step": 33460
    },
    {
      "epoch": 0.9223434744268078,
      "grad_norm": 0.8476047515869141,
      "learning_rate": 1.5536816578483246e-06,
      "loss": 0.0007,
      "step": 33470
    },
    {
      "epoch": 0.9226190476190477,
      "grad_norm": 0.018200160935521126,
      "learning_rate": 1.5481701940035274e-06,
      "loss": 0.1545,
      "step": 33480
    },
    {
      "epoch": 0.9228946208112875,
      "grad_norm": 0.010771442204713821,
      "learning_rate": 1.5426587301587305e-06,
      "loss": 0.001,
      "step": 33490
    },
    {
      "epoch": 0.9231701940035273,
      "grad_norm": 0.012744657695293427,
      "learning_rate": 1.537147266313933e-06,
      "loss": 0.0072,
      "step": 33500
    },
    {
      "epoch": 0.9234457671957672,
      "grad_norm": 0.008359781466424465,
      "learning_rate": 1.5316358024691357e-06,
      "loss": 0.0774,
      "step": 33510
    },
    {
      "epoch": 0.923721340388007,
      "grad_norm": 0.10577575862407684,
      "learning_rate": 1.5261243386243387e-06,
      "loss": 0.0464,
      "step": 33520
    },
    {
      "epoch": 0.9239969135802469,
      "grad_norm": 0.015101619996130466,
      "learning_rate": 1.5206128747795416e-06,
      "loss": 0.0007,
      "step": 33530
    },
    {
      "epoch": 0.9242724867724867,
      "grad_norm": 17.351810455322266,
      "learning_rate": 1.5151014109347444e-06,
      "loss": 0.1847,
      "step": 33540
    },
    {
      "epoch": 0.9245480599647267,
      "grad_norm": 99.56458282470703,
      "learning_rate": 1.5095899470899472e-06,
      "loss": 0.1592,
      "step": 33550
    },
    {
      "epoch": 0.9248236331569665,
      "grad_norm": 0.0559738352894783,
      "learning_rate": 1.5040784832451499e-06,
      "loss": 0.0917,
      "step": 33560
    },
    {
      "epoch": 0.9250992063492064,
      "grad_norm": 0.008898701518774033,
      "learning_rate": 1.498567019400353e-06,
      "loss": 0.0779,
      "step": 33570
    },
    {
      "epoch": 0.9253747795414462,
      "grad_norm": 0.009931044653058052,
      "learning_rate": 1.4930555555555555e-06,
      "loss": 0.0439,
      "step": 33580
    },
    {
      "epoch": 0.9256503527336861,
      "grad_norm": 0.32448458671569824,
      "learning_rate": 1.4875440917107584e-06,
      "loss": 0.0006,
      "step": 33590
    },
    {
      "epoch": 0.9259259259259259,
      "grad_norm": 0.03320187330245972,
      "learning_rate": 1.4820326278659614e-06,
      "loss": 0.0071,
      "step": 33600
    },
    {
      "epoch": 0.9262014991181657,
      "grad_norm": 150.33154296875,
      "learning_rate": 1.476521164021164e-06,
      "loss": 0.0129,
      "step": 33610
    },
    {
      "epoch": 0.9264770723104057,
      "grad_norm": 0.021400516852736473,
      "learning_rate": 1.471009700176367e-06,
      "loss": 0.0909,
      "step": 33620
    },
    {
      "epoch": 0.9267526455026455,
      "grad_norm": 0.007291731890290976,
      "learning_rate": 1.4654982363315697e-06,
      "loss": 0.0617,
      "step": 33630
    },
    {
      "epoch": 0.9270282186948854,
      "grad_norm": 1.7810187339782715,
      "learning_rate": 1.4599867724867726e-06,
      "loss": 0.0038,
      "step": 33640
    },
    {
      "epoch": 0.9273037918871252,
      "grad_norm": 0.050779372453689575,
      "learning_rate": 1.4544753086419756e-06,
      "loss": 0.0022,
      "step": 33650
    },
    {
      "epoch": 0.9275793650793651,
      "grad_norm": 83.61693572998047,
      "learning_rate": 1.4489638447971782e-06,
      "loss": 0.1853,
      "step": 33660
    },
    {
      "epoch": 0.9278549382716049,
      "grad_norm": 0.33359894156455994,
      "learning_rate": 1.4434523809523808e-06,
      "loss": 0.0032,
      "step": 33670
    },
    {
      "epoch": 0.9281305114638448,
      "grad_norm": 0.013839039951562881,
      "learning_rate": 1.4379409171075839e-06,
      "loss": 0.0812,
      "step": 33680
    },
    {
      "epoch": 0.9284060846560847,
      "grad_norm": 0.009903217665851116,
      "learning_rate": 1.4324294532627867e-06,
      "loss": 0.001,
      "step": 33690
    },
    {
      "epoch": 0.9286816578483245,
      "grad_norm": 0.014429755508899689,
      "learning_rate": 1.4269179894179896e-06,
      "loss": 0.0006,
      "step": 33700
    },
    {
      "epoch": 0.9289572310405644,
      "grad_norm": 0.03737993165850639,
      "learning_rate": 1.4214065255731924e-06,
      "loss": 0.0684,
      "step": 33710
    },
    {
      "epoch": 0.9292328042328042,
      "grad_norm": 0.06533566117286682,
      "learning_rate": 1.415895061728395e-06,
      "loss": 0.1972,
      "step": 33720
    },
    {
      "epoch": 0.9295083774250441,
      "grad_norm": 1.0038083791732788,
      "learning_rate": 1.410383597883598e-06,
      "loss": 0.0083,
      "step": 33730
    },
    {
      "epoch": 0.9297839506172839,
      "grad_norm": 83.46976470947266,
      "learning_rate": 1.4048721340388007e-06,
      "loss": 0.01,
      "step": 33740
    },
    {
      "epoch": 0.9300595238095238,
      "grad_norm": 0.6130551695823669,
      "learning_rate": 1.3993606701940035e-06,
      "loss": 0.1056,
      "step": 33750
    },
    {
      "epoch": 0.9303350970017636,
      "grad_norm": 0.017387397587299347,
      "learning_rate": 1.3938492063492066e-06,
      "loss": 0.0007,
      "step": 33760
    },
    {
      "epoch": 0.9306106701940036,
      "grad_norm": 0.1812533140182495,
      "learning_rate": 1.3883377425044092e-06,
      "loss": 0.1659,
      "step": 33770
    },
    {
      "epoch": 0.9308862433862434,
      "grad_norm": 0.0129703339189291,
      "learning_rate": 1.3828262786596122e-06,
      "loss": 0.0006,
      "step": 33780
    },
    {
      "epoch": 0.9311618165784833,
      "grad_norm": 0.006896294187754393,
      "learning_rate": 1.3773148148148149e-06,
      "loss": 0.0597,
      "step": 33790
    },
    {
      "epoch": 0.9314373897707231,
      "grad_norm": 0.00649027805775404,
      "learning_rate": 1.3718033509700177e-06,
      "loss": 0.1444,
      "step": 33800
    },
    {
      "epoch": 0.9317129629629629,
      "grad_norm": 0.009355241432785988,
      "learning_rate": 1.3662918871252207e-06,
      "loss": 0.3081,
      "step": 33810
    },
    {
      "epoch": 0.9319885361552028,
      "grad_norm": 0.0265865009278059,
      "learning_rate": 1.3607804232804234e-06,
      "loss": 0.1423,
      "step": 33820
    },
    {
      "epoch": 0.9322641093474426,
      "grad_norm": 0.005986761301755905,
      "learning_rate": 1.355268959435626e-06,
      "loss": 0.0042,
      "step": 33830
    },
    {
      "epoch": 0.9325396825396826,
      "grad_norm": 75.79625701904297,
      "learning_rate": 1.349757495590829e-06,
      "loss": 0.2388,
      "step": 33840
    },
    {
      "epoch": 0.9328152557319224,
      "grad_norm": 61.72986602783203,
      "learning_rate": 1.3442460317460319e-06,
      "loss": 0.0824,
      "step": 33850
    },
    {
      "epoch": 0.9330908289241623,
      "grad_norm": 0.056711483746767044,
      "learning_rate": 1.3387345679012347e-06,
      "loss": 0.0006,
      "step": 33860
    },
    {
      "epoch": 0.9333664021164021,
      "grad_norm": 0.01381670031696558,
      "learning_rate": 1.3332231040564375e-06,
      "loss": 0.0911,
      "step": 33870
    },
    {
      "epoch": 0.933641975308642,
      "grad_norm": 0.012090523727238178,
      "learning_rate": 1.3277116402116402e-06,
      "loss": 0.0638,
      "step": 33880
    },
    {
      "epoch": 0.9339175485008818,
      "grad_norm": 0.029256680980324745,
      "learning_rate": 1.3222001763668432e-06,
      "loss": 0.0986,
      "step": 33890
    },
    {
      "epoch": 0.9341931216931217,
      "grad_norm": 2.754230260848999,
      "learning_rate": 1.3166887125220458e-06,
      "loss": 0.0009,
      "step": 33900
    },
    {
      "epoch": 0.9344686948853616,
      "grad_norm": 0.0772242546081543,
      "learning_rate": 1.3111772486772487e-06,
      "loss": 0.0955,
      "step": 33910
    },
    {
      "epoch": 0.9347442680776014,
      "grad_norm": 0.055952288210392,
      "learning_rate": 1.3056657848324517e-06,
      "loss": 0.0385,
      "step": 33920
    },
    {
      "epoch": 0.9350198412698413,
      "grad_norm": 0.015753012150526047,
      "learning_rate": 1.3001543209876543e-06,
      "loss": 0.0912,
      "step": 33930
    },
    {
      "epoch": 0.9352954144620811,
      "grad_norm": 1.1060932874679565,
      "learning_rate": 1.2946428571428574e-06,
      "loss": 0.1047,
      "step": 33940
    },
    {
      "epoch": 0.935570987654321,
      "grad_norm": 0.14071041345596313,
      "learning_rate": 1.28913139329806e-06,
      "loss": 0.0404,
      "step": 33950
    },
    {
      "epoch": 0.9358465608465608,
      "grad_norm": 0.0071452828124165535,
      "learning_rate": 1.2836199294532629e-06,
      "loss": 0.0011,
      "step": 33960
    },
    {
      "epoch": 0.9361221340388007,
      "grad_norm": 174.63951110839844,
      "learning_rate": 1.278108465608466e-06,
      "loss": 0.1216,
      "step": 33970
    },
    {
      "epoch": 0.9363977072310405,
      "grad_norm": 0.24918168783187866,
      "learning_rate": 1.2725970017636685e-06,
      "loss": 0.0766,
      "step": 33980
    },
    {
      "epoch": 0.9366732804232805,
      "grad_norm": 0.0287022665143013,
      "learning_rate": 1.2670855379188711e-06,
      "loss": 0.0946,
      "step": 33990
    },
    {
      "epoch": 0.9369488536155203,
      "grad_norm": 0.008256846107542515,
      "learning_rate": 1.2615740740740742e-06,
      "loss": 0.0005,
      "step": 34000
    },
    {
      "epoch": 0.9372244268077602,
      "grad_norm": 11.947893142700195,
      "learning_rate": 1.256062610229277e-06,
      "loss": 0.1498,
      "step": 34010
    },
    {
      "epoch": 0.9375,
      "grad_norm": 113.34971618652344,
      "learning_rate": 1.2505511463844799e-06,
      "loss": 0.0067,
      "step": 34020
    },
    {
      "epoch": 0.9377755731922398,
      "grad_norm": 0.009892909787595272,
      "learning_rate": 1.2450396825396827e-06,
      "loss": 0.0017,
      "step": 34030
    },
    {
      "epoch": 0.9380511463844797,
      "grad_norm": 0.008854065090417862,
      "learning_rate": 1.2395282186948855e-06,
      "loss": 0.2235,
      "step": 34040
    },
    {
      "epoch": 0.9383267195767195,
      "grad_norm": 0.46695053577423096,
      "learning_rate": 1.2340167548500882e-06,
      "loss": 0.0705,
      "step": 34050
    },
    {
      "epoch": 0.9386022927689595,
      "grad_norm": 0.011363256722688675,
      "learning_rate": 1.228505291005291e-06,
      "loss": 0.0965,
      "step": 34060
    },
    {
      "epoch": 0.9388778659611993,
      "grad_norm": 0.3343678414821625,
      "learning_rate": 1.222993827160494e-06,
      "loss": 0.2016,
      "step": 34070
    },
    {
      "epoch": 0.9391534391534392,
      "grad_norm": 0.1132560521364212,
      "learning_rate": 1.2174823633156969e-06,
      "loss": 0.0221,
      "step": 34080
    },
    {
      "epoch": 0.939429012345679,
      "grad_norm": 41.56560516357422,
      "learning_rate": 1.2119708994708995e-06,
      "loss": 0.1696,
      "step": 34090
    },
    {
      "epoch": 0.9397045855379189,
      "grad_norm": 0.007715522311627865,
      "learning_rate": 1.2064594356261023e-06,
      "loss": 0.0006,
      "step": 34100
    },
    {
      "epoch": 0.9399801587301587,
      "grad_norm": 0.01005444023758173,
      "learning_rate": 1.2009479717813052e-06,
      "loss": 0.0007,
      "step": 34110
    },
    {
      "epoch": 0.9402557319223986,
      "grad_norm": 0.09302768856287003,
      "learning_rate": 1.195436507936508e-06,
      "loss": 0.0952,
      "step": 34120
    },
    {
      "epoch": 0.9405313051146384,
      "grad_norm": 4.198080062866211,
      "learning_rate": 1.1899250440917108e-06,
      "loss": 0.001,
      "step": 34130
    },
    {
      "epoch": 0.9408068783068783,
      "grad_norm": 63.825340270996094,
      "learning_rate": 1.1844135802469137e-06,
      "loss": 0.1593,
      "step": 34140
    },
    {
      "epoch": 0.9410824514991182,
      "grad_norm": 166.45462036132812,
      "learning_rate": 1.1789021164021165e-06,
      "loss": 0.0451,
      "step": 34150
    },
    {
      "epoch": 0.941358024691358,
      "grad_norm": 18.5799503326416,
      "learning_rate": 1.1733906525573193e-06,
      "loss": 0.2595,
      "step": 34160
    },
    {
      "epoch": 0.9416335978835979,
      "grad_norm": 56.92935562133789,
      "learning_rate": 1.1678791887125222e-06,
      "loss": 0.2028,
      "step": 34170
    },
    {
      "epoch": 0.9419091710758377,
      "grad_norm": 0.09330995380878448,
      "learning_rate": 1.162367724867725e-06,
      "loss": 0.0009,
      "step": 34180
    },
    {
      "epoch": 0.9421847442680776,
      "grad_norm": 0.12034309655427933,
      "learning_rate": 1.1568562610229278e-06,
      "loss": 0.0463,
      "step": 34190
    },
    {
      "epoch": 0.9424603174603174,
      "grad_norm": 0.17272087931632996,
      "learning_rate": 1.1513447971781307e-06,
      "loss": 0.2937,
      "step": 34200
    },
    {
      "epoch": 0.9427358906525574,
      "grad_norm": 0.45323482155799866,
      "learning_rate": 1.1458333333333333e-06,
      "loss": 0.0018,
      "step": 34210
    },
    {
      "epoch": 0.9430114638447972,
      "grad_norm": 0.013604107312858105,
      "learning_rate": 1.1403218694885361e-06,
      "loss": 0.0005,
      "step": 34220
    },
    {
      "epoch": 0.9432870370370371,
      "grad_norm": 0.024668985977768898,
      "learning_rate": 1.1348104056437392e-06,
      "loss": 0.001,
      "step": 34230
    },
    {
      "epoch": 0.9435626102292769,
      "grad_norm": 4.09150505065918,
      "learning_rate": 1.129298941798942e-06,
      "loss": 0.0922,
      "step": 34240
    },
    {
      "epoch": 0.9438381834215167,
      "grad_norm": 0.04499924182891846,
      "learning_rate": 1.1237874779541446e-06,
      "loss": 0.0273,
      "step": 34250
    },
    {
      "epoch": 0.9441137566137566,
      "grad_norm": 0.009336386807262897,
      "learning_rate": 1.1182760141093475e-06,
      "loss": 0.1304,
      "step": 34260
    },
    {
      "epoch": 0.9443893298059964,
      "grad_norm": 0.007515239529311657,
      "learning_rate": 1.1127645502645503e-06,
      "loss": 0.0007,
      "step": 34270
    },
    {
      "epoch": 0.9446649029982364,
      "grad_norm": 0.021399885416030884,
      "learning_rate": 1.1072530864197531e-06,
      "loss": 0.001,
      "step": 34280
    },
    {
      "epoch": 0.9449404761904762,
      "grad_norm": 0.14348246157169342,
      "learning_rate": 1.101741622574956e-06,
      "loss": 0.0005,
      "step": 34290
    },
    {
      "epoch": 0.9452160493827161,
      "grad_norm": 0.019023846834897995,
      "learning_rate": 1.0962301587301588e-06,
      "loss": 0.0752,
      "step": 34300
    },
    {
      "epoch": 0.9454916225749559,
      "grad_norm": 0.027786938473582268,
      "learning_rate": 1.0907186948853617e-06,
      "loss": 0.0007,
      "step": 34310
    },
    {
      "epoch": 0.9457671957671958,
      "grad_norm": 0.010547665879130363,
      "learning_rate": 1.0852072310405645e-06,
      "loss": 0.0406,
      "step": 34320
    },
    {
      "epoch": 0.9460427689594356,
      "grad_norm": 0.008941358886659145,
      "learning_rate": 1.0796957671957673e-06,
      "loss": 0.1989,
      "step": 34330
    },
    {
      "epoch": 0.9463183421516755,
      "grad_norm": 172.8450164794922,
      "learning_rate": 1.0741843033509702e-06,
      "loss": 0.0573,
      "step": 34340
    },
    {
      "epoch": 0.9465939153439153,
      "grad_norm": 0.024059263989329338,
      "learning_rate": 1.068672839506173e-06,
      "loss": 0.308,
      "step": 34350
    },
    {
      "epoch": 0.9468694885361552,
      "grad_norm": 0.008061838336288929,
      "learning_rate": 1.0631613756613758e-06,
      "loss": 0.0862,
      "step": 34360
    },
    {
      "epoch": 0.9471450617283951,
      "grad_norm": 0.024469463154673576,
      "learning_rate": 1.0576499118165785e-06,
      "loss": 0.0637,
      "step": 34370
    },
    {
      "epoch": 0.9474206349206349,
      "grad_norm": 0.008700747042894363,
      "learning_rate": 1.0521384479717813e-06,
      "loss": 0.0808,
      "step": 34380
    },
    {
      "epoch": 0.9476962081128748,
      "grad_norm": 0.017029108479619026,
      "learning_rate": 1.0466269841269843e-06,
      "loss": 0.1296,
      "step": 34390
    },
    {
      "epoch": 0.9479717813051146,
      "grad_norm": 0.007068760693073273,
      "learning_rate": 1.0411155202821872e-06,
      "loss": 0.0014,
      "step": 34400
    },
    {
      "epoch": 0.9482473544973545,
      "grad_norm": 0.01914304681122303,
      "learning_rate": 1.0356040564373898e-06,
      "loss": 0.0005,
      "step": 34410
    },
    {
      "epoch": 0.9485229276895943,
      "grad_norm": 1.4556368589401245,
      "learning_rate": 1.0300925925925926e-06,
      "loss": 0.0817,
      "step": 34420
    },
    {
      "epoch": 0.9487985008818343,
      "grad_norm": 22.13933563232422,
      "learning_rate": 1.0245811287477955e-06,
      "loss": 0.0017,
      "step": 34430
    },
    {
      "epoch": 0.9490740740740741,
      "grad_norm": 0.016617823392152786,
      "learning_rate": 1.0190696649029983e-06,
      "loss": 0.083,
      "step": 34440
    },
    {
      "epoch": 0.9493496472663139,
      "grad_norm": 0.13741184771060944,
      "learning_rate": 1.0135582010582011e-06,
      "loss": 0.0091,
      "step": 34450
    },
    {
      "epoch": 0.9496252204585538,
      "grad_norm": 55.35507583618164,
      "learning_rate": 1.008046737213404e-06,
      "loss": 0.0708,
      "step": 34460
    },
    {
      "epoch": 0.9499007936507936,
      "grad_norm": 0.4317385256290436,
      "learning_rate": 1.0025352733686068e-06,
      "loss": 0.0695,
      "step": 34470
    },
    {
      "epoch": 0.9501763668430335,
      "grad_norm": 0.021765131503343582,
      "learning_rate": 9.970238095238096e-07,
      "loss": 0.0647,
      "step": 34480
    },
    {
      "epoch": 0.9504519400352733,
      "grad_norm": 0.7156671285629272,
      "learning_rate": 9.915123456790125e-07,
      "loss": 0.2088,
      "step": 34490
    },
    {
      "epoch": 0.9507275132275133,
      "grad_norm": 0.018827753141522408,
      "learning_rate": 9.860008818342153e-07,
      "loss": 0.0095,
      "step": 34500
    },
    {
      "epoch": 0.9510030864197531,
      "grad_norm": 0.006100723519921303,
      "learning_rate": 9.804894179894181e-07,
      "loss": 0.1077,
      "step": 34510
    },
    {
      "epoch": 0.951278659611993,
      "grad_norm": 103.8182601928711,
      "learning_rate": 9.74977954144621e-07,
      "loss": 0.0706,
      "step": 34520
    },
    {
      "epoch": 0.9515542328042328,
      "grad_norm": 0.016482742503285408,
      "learning_rate": 9.694664902998236e-07,
      "loss": 0.2382,
      "step": 34530
    },
    {
      "epoch": 0.9518298059964727,
      "grad_norm": 0.019895775243639946,
      "learning_rate": 9.639550264550264e-07,
      "loss": 0.0606,
      "step": 34540
    },
    {
      "epoch": 0.9521053791887125,
      "grad_norm": 0.0062438203021883965,
      "learning_rate": 9.584435626102295e-07,
      "loss": 0.0014,
      "step": 34550
    },
    {
      "epoch": 0.9523809523809523,
      "grad_norm": 0.05435716360807419,
      "learning_rate": 9.529320987654322e-07,
      "loss": 0.0006,
      "step": 34560
    },
    {
      "epoch": 0.9526565255731922,
      "grad_norm": 0.08779120445251465,
      "learning_rate": 9.474206349206349e-07,
      "loss": 0.0867,
      "step": 34570
    },
    {
      "epoch": 0.9529320987654321,
      "grad_norm": 0.022035963833332062,
      "learning_rate": 9.419091710758378e-07,
      "loss": 0.172,
      "step": 34580
    },
    {
      "epoch": 0.953207671957672,
      "grad_norm": 68.18614959716797,
      "learning_rate": 9.363977072310407e-07,
      "loss": 0.084,
      "step": 34590
    },
    {
      "epoch": 0.9534832451499118,
      "grad_norm": 0.18615569174289703,
      "learning_rate": 9.308862433862433e-07,
      "loss": 0.0006,
      "step": 34600
    },
    {
      "epoch": 0.9537588183421517,
      "grad_norm": 0.010409580543637276,
      "learning_rate": 9.253747795414463e-07,
      "loss": 0.0886,
      "step": 34610
    },
    {
      "epoch": 0.9540343915343915,
      "grad_norm": 0.02785736508667469,
      "learning_rate": 9.198633156966491e-07,
      "loss": 0.1605,
      "step": 34620
    },
    {
      "epoch": 0.9543099647266314,
      "grad_norm": 0.032013848423957825,
      "learning_rate": 9.14351851851852e-07,
      "loss": 0.1236,
      "step": 34630
    },
    {
      "epoch": 0.9545855379188712,
      "grad_norm": 0.017628714442253113,
      "learning_rate": 9.088403880070547e-07,
      "loss": 0.1316,
      "step": 34640
    },
    {
      "epoch": 0.9548611111111112,
      "grad_norm": 0.023078879341483116,
      "learning_rate": 9.033289241622575e-07,
      "loss": 0.2124,
      "step": 34650
    },
    {
      "epoch": 0.955136684303351,
      "grad_norm": 0.03012371063232422,
      "learning_rate": 8.978174603174604e-07,
      "loss": 0.0838,
      "step": 34660
    },
    {
      "epoch": 0.9554122574955908,
      "grad_norm": 0.017499655485153198,
      "learning_rate": 8.923059964726633e-07,
      "loss": 0.1236,
      "step": 34670
    },
    {
      "epoch": 0.9556878306878307,
      "grad_norm": 1.1599347591400146,
      "learning_rate": 8.867945326278659e-07,
      "loss": 0.1086,
      "step": 34680
    },
    {
      "epoch": 0.9559634038800705,
      "grad_norm": 19.182878494262695,
      "learning_rate": 8.812830687830689e-07,
      "loss": 0.1052,
      "step": 34690
    },
    {
      "epoch": 0.9562389770723104,
      "grad_norm": 0.04029064625501633,
      "learning_rate": 8.757716049382717e-07,
      "loss": 0.0007,
      "step": 34700
    },
    {
      "epoch": 0.9565145502645502,
      "grad_norm": 67.4308090209961,
      "learning_rate": 8.702601410934745e-07,
      "loss": 0.1566,
      "step": 34710
    },
    {
      "epoch": 0.9567901234567902,
      "grad_norm": 0.05913461372256279,
      "learning_rate": 8.647486772486773e-07,
      "loss": 0.0194,
      "step": 34720
    },
    {
      "epoch": 0.95706569664903,
      "grad_norm": 77.60039520263672,
      "learning_rate": 8.592372134038801e-07,
      "loss": 0.0822,
      "step": 34730
    },
    {
      "epoch": 0.9573412698412699,
      "grad_norm": 0.014558819122612476,
      "learning_rate": 8.537257495590829e-07,
      "loss": 0.0563,
      "step": 34740
    },
    {
      "epoch": 0.9576168430335097,
      "grad_norm": 185.523681640625,
      "learning_rate": 8.482142857142859e-07,
      "loss": 0.0585,
      "step": 34750
    },
    {
      "epoch": 0.9578924162257496,
      "grad_norm": 236.91368103027344,
      "learning_rate": 8.427028218694885e-07,
      "loss": 0.1824,
      "step": 34760
    },
    {
      "epoch": 0.9581679894179894,
      "grad_norm": 0.007542707026004791,
      "learning_rate": 8.371913580246914e-07,
      "loss": 0.0932,
      "step": 34770
    },
    {
      "epoch": 0.9584435626102292,
      "grad_norm": 113.43522644042969,
      "learning_rate": 8.316798941798943e-07,
      "loss": 0.0345,
      "step": 34780
    },
    {
      "epoch": 0.9587191358024691,
      "grad_norm": 0.014422896318137646,
      "learning_rate": 8.261684303350971e-07,
      "loss": 0.0444,
      "step": 34790
    },
    {
      "epoch": 0.958994708994709,
      "grad_norm": 0.023340292274951935,
      "learning_rate": 8.206569664902998e-07,
      "loss": 0.0825,
      "step": 34800
    },
    {
      "epoch": 0.9592702821869489,
      "grad_norm": 0.023032736033201218,
      "learning_rate": 8.151455026455027e-07,
      "loss": 0.0688,
      "step": 34810
    },
    {
      "epoch": 0.9595458553791887,
      "grad_norm": 0.020005861297249794,
      "learning_rate": 8.096340388007055e-07,
      "loss": 0.0071,
      "step": 34820
    },
    {
      "epoch": 0.9598214285714286,
      "grad_norm": 133.70220947265625,
      "learning_rate": 8.041225749559084e-07,
      "loss": 0.2233,
      "step": 34830
    },
    {
      "epoch": 0.9600970017636684,
      "grad_norm": 117.13514709472656,
      "learning_rate": 7.986111111111111e-07,
      "loss": 0.0538,
      "step": 34840
    },
    {
      "epoch": 0.9603725749559083,
      "grad_norm": 9.088610649108887,
      "learning_rate": 7.93099647266314e-07,
      "loss": 0.1465,
      "step": 34850
    },
    {
      "epoch": 0.9606481481481481,
      "grad_norm": 0.09621089696884155,
      "learning_rate": 7.875881834215168e-07,
      "loss": 0.0018,
      "step": 34860
    },
    {
      "epoch": 0.9609237213403881,
      "grad_norm": 0.008766133338212967,
      "learning_rate": 7.820767195767197e-07,
      "loss": 0.1439,
      "step": 34870
    },
    {
      "epoch": 0.9611992945326279,
      "grad_norm": 0.03928537666797638,
      "learning_rate": 7.765652557319224e-07,
      "loss": 0.0058,
      "step": 34880
    },
    {
      "epoch": 0.9614748677248677,
      "grad_norm": 162.41587829589844,
      "learning_rate": 7.710537918871252e-07,
      "loss": 0.1541,
      "step": 34890
    },
    {
      "epoch": 0.9617504409171076,
      "grad_norm": 23.976367950439453,
      "learning_rate": 7.655423280423281e-07,
      "loss": 0.0031,
      "step": 34900
    },
    {
      "epoch": 0.9620260141093474,
      "grad_norm": 0.04621187597513199,
      "learning_rate": 7.60030864197531e-07,
      "loss": 0.0005,
      "step": 34910
    },
    {
      "epoch": 0.9623015873015873,
      "grad_norm": 54.4224967956543,
      "learning_rate": 7.545194003527336e-07,
      "loss": 0.0146,
      "step": 34920
    },
    {
      "epoch": 0.9625771604938271,
      "grad_norm": 1.0480271577835083,
      "learning_rate": 7.490079365079366e-07,
      "loss": 0.0019,
      "step": 34930
    },
    {
      "epoch": 0.962852733686067,
      "grad_norm": 0.5481019616127014,
      "learning_rate": 7.434964726631394e-07,
      "loss": 0.1734,
      "step": 34940
    },
    {
      "epoch": 0.9631283068783069,
      "grad_norm": 5.705573081970215,
      "learning_rate": 7.379850088183423e-07,
      "loss": 0.0021,
      "step": 34950
    },
    {
      "epoch": 0.9634038800705468,
      "grad_norm": 0.006970496848225594,
      "learning_rate": 7.32473544973545e-07,
      "loss": 0.1223,
      "step": 34960
    },
    {
      "epoch": 0.9636794532627866,
      "grad_norm": 0.9178127646446228,
      "learning_rate": 7.269620811287478e-07,
      "loss": 0.0009,
      "step": 34970
    },
    {
      "epoch": 0.9639550264550265,
      "grad_norm": 5.251488208770752,
      "learning_rate": 7.214506172839506e-07,
      "loss": 0.0968,
      "step": 34980
    },
    {
      "epoch": 0.9642305996472663,
      "grad_norm": 0.010408949106931686,
      "learning_rate": 7.159391534391536e-07,
      "loss": 0.2114,
      "step": 34990
    },
    {
      "epoch": 0.9645061728395061,
      "grad_norm": 0.00792634952813387,
      "learning_rate": 7.104276895943562e-07,
      "loss": 0.0023,
      "step": 35000
    },
    {
      "epoch": 0.964781746031746,
      "grad_norm": 0.040208600461483,
      "learning_rate": 7.049162257495592e-07,
      "loss": 0.1612,
      "step": 35010
    },
    {
      "epoch": 0.9650573192239859,
      "grad_norm": 0.022046199068427086,
      "learning_rate": 6.99404761904762e-07,
      "loss": 0.0008,
      "step": 35020
    },
    {
      "epoch": 0.9653328924162258,
      "grad_norm": 0.10765304416418076,
      "learning_rate": 6.938932980599648e-07,
      "loss": 0.1324,
      "step": 35030
    },
    {
      "epoch": 0.9656084656084656,
      "grad_norm": 25.739892959594727,
      "learning_rate": 6.883818342151676e-07,
      "loss": 0.081,
      "step": 35040
    },
    {
      "epoch": 0.9658840388007055,
      "grad_norm": 115.43440246582031,
      "learning_rate": 6.828703703703704e-07,
      "loss": 0.1066,
      "step": 35050
    },
    {
      "epoch": 0.9661596119929453,
      "grad_norm": 275.8321228027344,
      "learning_rate": 6.773589065255732e-07,
      "loss": 0.134,
      "step": 35060
    },
    {
      "epoch": 0.9664351851851852,
      "grad_norm": 0.08052204549312592,
      "learning_rate": 6.718474426807762e-07,
      "loss": 0.0007,
      "step": 35070
    },
    {
      "epoch": 0.966710758377425,
      "grad_norm": 0.06728988885879517,
      "learning_rate": 6.663359788359788e-07,
      "loss": 0.0246,
      "step": 35080
    },
    {
      "epoch": 0.9669863315696648,
      "grad_norm": 0.9424275159835815,
      "learning_rate": 6.608245149911817e-07,
      "loss": 0.0757,
      "step": 35090
    },
    {
      "epoch": 0.9672619047619048,
      "grad_norm": 0.007442119065672159,
      "learning_rate": 6.553130511463846e-07,
      "loss": 0.0006,
      "step": 35100
    },
    {
      "epoch": 0.9675374779541446,
      "grad_norm": 0.0280718132853508,
      "learning_rate": 6.498015873015874e-07,
      "loss": 0.0019,
      "step": 35110
    },
    {
      "epoch": 0.9678130511463845,
      "grad_norm": 0.05160116031765938,
      "learning_rate": 6.442901234567901e-07,
      "loss": 0.0016,
      "step": 35120
    },
    {
      "epoch": 0.9680886243386243,
      "grad_norm": 21.630146026611328,
      "learning_rate": 6.38778659611993e-07,
      "loss": 0.0613,
      "step": 35130
    },
    {
      "epoch": 0.9683641975308642,
      "grad_norm": 0.009919053874909878,
      "learning_rate": 6.332671957671958e-07,
      "loss": 0.0893,
      "step": 35140
    },
    {
      "epoch": 0.968639770723104,
      "grad_norm": 0.027195457369089127,
      "learning_rate": 6.277557319223987e-07,
      "loss": 0.0543,
      "step": 35150
    },
    {
      "epoch": 0.968915343915344,
      "grad_norm": 58.76631546020508,
      "learning_rate": 6.222442680776015e-07,
      "loss": 0.1655,
      "step": 35160
    },
    {
      "epoch": 0.9691909171075838,
      "grad_norm": 0.00693785585463047,
      "learning_rate": 6.167328042328043e-07,
      "loss": 0.1586,
      "step": 35170
    },
    {
      "epoch": 0.9694664902998237,
      "grad_norm": 0.02419610321521759,
      "learning_rate": 6.112213403880071e-07,
      "loss": 0.126,
      "step": 35180
    },
    {
      "epoch": 0.9697420634920635,
      "grad_norm": 0.009467781521379948,
      "learning_rate": 6.057098765432099e-07,
      "loss": 0.0029,
      "step": 35190
    },
    {
      "epoch": 0.9700176366843033,
      "grad_norm": 0.01628146879374981,
      "learning_rate": 6.001984126984128e-07,
      "loss": 0.0084,
      "step": 35200
    },
    {
      "epoch": 0.9702932098765432,
      "grad_norm": 63.928226470947266,
      "learning_rate": 5.946869488536155e-07,
      "loss": 0.1449,
      "step": 35210
    },
    {
      "epoch": 0.970568783068783,
      "grad_norm": 0.3667861819267273,
      "learning_rate": 5.891754850088184e-07,
      "loss": 0.0182,
      "step": 35220
    },
    {
      "epoch": 0.970844356261023,
      "grad_norm": 0.04104849323630333,
      "learning_rate": 5.836640211640212e-07,
      "loss": 0.1563,
      "step": 35230
    },
    {
      "epoch": 0.9711199294532628,
      "grad_norm": 0.016422901302576065,
      "learning_rate": 5.78152557319224e-07,
      "loss": 0.0247,
      "step": 35240
    },
    {
      "epoch": 0.9713955026455027,
      "grad_norm": 0.012773362919688225,
      "learning_rate": 5.726410934744269e-07,
      "loss": 0.0051,
      "step": 35250
    },
    {
      "epoch": 0.9716710758377425,
      "grad_norm": 0.01415175385773182,
      "learning_rate": 5.671296296296297e-07,
      "loss": 0.0007,
      "step": 35260
    },
    {
      "epoch": 0.9719466490299824,
      "grad_norm": 0.007827048189938068,
      "learning_rate": 5.616181657848324e-07,
      "loss": 0.0013,
      "step": 35270
    },
    {
      "epoch": 0.9722222222222222,
      "grad_norm": 54.42054748535156,
      "learning_rate": 5.561067019400354e-07,
      "loss": 0.0436,
      "step": 35280
    },
    {
      "epoch": 0.9724977954144621,
      "grad_norm": 0.008499132469296455,
      "learning_rate": 5.505952380952381e-07,
      "loss": 0.1335,
      "step": 35290
    },
    {
      "epoch": 0.9727733686067019,
      "grad_norm": 70.89588165283203,
      "learning_rate": 5.45083774250441e-07,
      "loss": 0.0078,
      "step": 35300
    },
    {
      "epoch": 0.9730489417989417,
      "grad_norm": 0.009505443274974823,
      "learning_rate": 5.395723104056438e-07,
      "loss": 0.0005,
      "step": 35310
    },
    {
      "epoch": 0.9733245149911817,
      "grad_norm": 0.00894276425242424,
      "learning_rate": 5.340608465608466e-07,
      "loss": 0.0301,
      "step": 35320
    },
    {
      "epoch": 0.9736000881834215,
      "grad_norm": 0.010992045514285564,
      "learning_rate": 5.285493827160495e-07,
      "loss": 0.1201,
      "step": 35330
    },
    {
      "epoch": 0.9738756613756614,
      "grad_norm": 0.009980056434869766,
      "learning_rate": 5.230379188712523e-07,
      "loss": 0.0724,
      "step": 35340
    },
    {
      "epoch": 0.9741512345679012,
      "grad_norm": 0.03964516520500183,
      "learning_rate": 5.17526455026455e-07,
      "loss": 0.0263,
      "step": 35350
    },
    {
      "epoch": 0.9744268077601411,
      "grad_norm": 0.024777187034487724,
      "learning_rate": 5.12014991181658e-07,
      "loss": 0.0949,
      "step": 35360
    },
    {
      "epoch": 0.9747023809523809,
      "grad_norm": 0.013092702254652977,
      "learning_rate": 5.065035273368607e-07,
      "loss": 0.0006,
      "step": 35370
    },
    {
      "epoch": 0.9749779541446209,
      "grad_norm": 0.007049208506941795,
      "learning_rate": 5.009920634920635e-07,
      "loss": 0.0924,
      "step": 35380
    },
    {
      "epoch": 0.9752535273368607,
      "grad_norm": 0.023496976122260094,
      "learning_rate": 4.954805996472664e-07,
      "loss": 0.0008,
      "step": 35390
    },
    {
      "epoch": 0.9755291005291006,
      "grad_norm": 0.01692931540310383,
      "learning_rate": 4.899691358024692e-07,
      "loss": 0.0009,
      "step": 35400
    },
    {
      "epoch": 0.9758046737213404,
      "grad_norm": 0.012060552835464478,
      "learning_rate": 4.84457671957672e-07,
      "loss": 0.4068,
      "step": 35410
    },
    {
      "epoch": 0.9760802469135802,
      "grad_norm": 0.9782208800315857,
      "learning_rate": 4.789462081128749e-07,
      "loss": 0.2112,
      "step": 35420
    },
    {
      "epoch": 0.9763558201058201,
      "grad_norm": 2.378711700439453,
      "learning_rate": 4.7343474426807764e-07,
      "loss": 0.0921,
      "step": 35430
    },
    {
      "epoch": 0.9766313932980599,
      "grad_norm": 0.043689094483852386,
      "learning_rate": 4.679232804232805e-07,
      "loss": 0.0062,
      "step": 35440
    },
    {
      "epoch": 0.9769069664902998,
      "grad_norm": 0.7583729028701782,
      "learning_rate": 4.6241181657848326e-07,
      "loss": 0.0429,
      "step": 35450
    },
    {
      "epoch": 0.9771825396825397,
      "grad_norm": 0.024653799831867218,
      "learning_rate": 4.5690035273368615e-07,
      "loss": 0.082,
      "step": 35460
    },
    {
      "epoch": 0.9774581128747796,
      "grad_norm": 0.010306570678949356,
      "learning_rate": 4.5138888888888893e-07,
      "loss": 0.0986,
      "step": 35470
    },
    {
      "epoch": 0.9777336860670194,
      "grad_norm": 0.006414453033357859,
      "learning_rate": 4.4587742504409177e-07,
      "loss": 0.0005,
      "step": 35480
    },
    {
      "epoch": 0.9780092592592593,
      "grad_norm": 0.00789635069668293,
      "learning_rate": 4.4036596119929455e-07,
      "loss": 0.0113,
      "step": 35490
    },
    {
      "epoch": 0.9782848324514991,
      "grad_norm": 4.651294708251953,
      "learning_rate": 4.3485449735449744e-07,
      "loss": 0.1746,
      "step": 35500
    },
    {
      "epoch": 0.978560405643739,
      "grad_norm": 0.006809623911976814,
      "learning_rate": 4.293430335097002e-07,
      "loss": 0.1715,
      "step": 35510
    },
    {
      "epoch": 0.9788359788359788,
      "grad_norm": 0.02868293598294258,
      "learning_rate": 4.2383156966490305e-07,
      "loss": 0.0208,
      "step": 35520
    },
    {
      "epoch": 0.9791115520282186,
      "grad_norm": 0.10398172587156296,
      "learning_rate": 4.1832010582010584e-07,
      "loss": 0.0764,
      "step": 35530
    },
    {
      "epoch": 0.9793871252204586,
      "grad_norm": 0.04933997243642807,
      "learning_rate": 4.128086419753087e-07,
      "loss": 0.1486,
      "step": 35540
    },
    {
      "epoch": 0.9796626984126984,
      "grad_norm": 0.10104482620954514,
      "learning_rate": 4.072971781305115e-07,
      "loss": 0.0527,
      "step": 35550
    },
    {
      "epoch": 0.9799382716049383,
      "grad_norm": 0.02302750013768673,
      "learning_rate": 4.0178571428571434e-07,
      "loss": 0.1046,
      "step": 35560
    },
    {
      "epoch": 0.9802138447971781,
      "grad_norm": 0.018589571118354797,
      "learning_rate": 3.962742504409171e-07,
      "loss": 0.0007,
      "step": 35570
    },
    {
      "epoch": 0.980489417989418,
      "grad_norm": 0.6856926083564758,
      "learning_rate": 3.9076278659612e-07,
      "loss": 0.0006,
      "step": 35580
    },
    {
      "epoch": 0.9807649911816578,
      "grad_norm": 49.76498031616211,
      "learning_rate": 3.852513227513228e-07,
      "loss": 0.1402,
      "step": 35590
    },
    {
      "epoch": 0.9810405643738977,
      "grad_norm": 10.050372123718262,
      "learning_rate": 3.7973985890652563e-07,
      "loss": 0.0979,
      "step": 35600
    },
    {
      "epoch": 0.9813161375661376,
      "grad_norm": 104.23851776123047,
      "learning_rate": 3.742283950617284e-07,
      "loss": 0.2179,
      "step": 35610
    },
    {
      "epoch": 0.9815917107583775,
      "grad_norm": 0.008654587902128696,
      "learning_rate": 3.687169312169313e-07,
      "loss": 0.0929,
      "step": 35620
    },
    {
      "epoch": 0.9818672839506173,
      "grad_norm": 0.030286863446235657,
      "learning_rate": 3.632054673721341e-07,
      "loss": 0.0989,
      "step": 35630
    },
    {
      "epoch": 0.9821428571428571,
      "grad_norm": 0.011268469505012035,
      "learning_rate": 3.576940035273369e-07,
      "loss": 0.0005,
      "step": 35640
    },
    {
      "epoch": 0.982418430335097,
      "grad_norm": 0.012365891598165035,
      "learning_rate": 3.521825396825397e-07,
      "loss": 0.0952,
      "step": 35650
    },
    {
      "epoch": 0.9826940035273368,
      "grad_norm": 0.14504730701446533,
      "learning_rate": 3.466710758377426e-07,
      "loss": 0.0016,
      "step": 35660
    },
    {
      "epoch": 0.9829695767195767,
      "grad_norm": 0.02159809321165085,
      "learning_rate": 3.4115961199294537e-07,
      "loss": 0.0009,
      "step": 35670
    },
    {
      "epoch": 0.9832451499118166,
      "grad_norm": 0.2933671772480011,
      "learning_rate": 3.356481481481482e-07,
      "loss": 0.0049,
      "step": 35680
    },
    {
      "epoch": 0.9835207231040565,
      "grad_norm": 0.684564471244812,
      "learning_rate": 3.30136684303351e-07,
      "loss": 0.1191,
      "step": 35690
    },
    {
      "epoch": 0.9837962962962963,
      "grad_norm": 0.014196948148310184,
      "learning_rate": 3.2462522045855387e-07,
      "loss": 0.1234,
      "step": 35700
    },
    {
      "epoch": 0.9840718694885362,
      "grad_norm": 0.015657830983400345,
      "learning_rate": 3.1911375661375666e-07,
      "loss": 0.0904,
      "step": 35710
    },
    {
      "epoch": 0.984347442680776,
      "grad_norm": 0.01681598089635372,
      "learning_rate": 3.136022927689595e-07,
      "loss": 0.1128,
      "step": 35720
    },
    {
      "epoch": 0.9846230158730159,
      "grad_norm": 0.0083263348788023,
      "learning_rate": 3.0809082892416227e-07,
      "loss": 0.1737,
      "step": 35730
    },
    {
      "epoch": 0.9848985890652557,
      "grad_norm": 0.011990067549049854,
      "learning_rate": 3.025793650793651e-07,
      "loss": 0.1848,
      "step": 35740
    },
    {
      "epoch": 0.9851741622574955,
      "grad_norm": 0.014210243709385395,
      "learning_rate": 2.9706790123456794e-07,
      "loss": 0.0943,
      "step": 35750
    },
    {
      "epoch": 0.9854497354497355,
      "grad_norm": 0.016246825456619263,
      "learning_rate": 2.915564373897707e-07,
      "loss": 0.1792,
      "step": 35760
    },
    {
      "epoch": 0.9857253086419753,
      "grad_norm": 0.014381094835698605,
      "learning_rate": 2.8604497354497356e-07,
      "loss": 0.1355,
      "step": 35770
    },
    {
      "epoch": 0.9860008818342152,
      "grad_norm": 0.007617264986038208,
      "learning_rate": 2.805335097001764e-07,
      "loss": 0.0006,
      "step": 35780
    },
    {
      "epoch": 0.986276455026455,
      "grad_norm": 0.0071404846385121346,
      "learning_rate": 2.7502204585537923e-07,
      "loss": 0.1068,
      "step": 35790
    },
    {
      "epoch": 0.9865520282186949,
      "grad_norm": 0.01202841755002737,
      "learning_rate": 2.69510582010582e-07,
      "loss": 0.062,
      "step": 35800
    },
    {
      "epoch": 0.9868276014109347,
      "grad_norm": 0.009798232465982437,
      "learning_rate": 2.6399911816578485e-07,
      "loss": 0.1409,
      "step": 35810
    },
    {
      "epoch": 0.9871031746031746,
      "grad_norm": 0.014495129697024822,
      "learning_rate": 2.584876543209877e-07,
      "loss": 0.1414,
      "step": 35820
    },
    {
      "epoch": 0.9873787477954145,
      "grad_norm": 0.012901180423796177,
      "learning_rate": 2.529761904761905e-07,
      "loss": 0.1144,
      "step": 35830
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 0.013757467269897461,
      "learning_rate": 2.474647266313933e-07,
      "loss": 0.0874,
      "step": 35840
    },
    {
      "epoch": 0.9879298941798942,
      "grad_norm": 0.024588927626609802,
      "learning_rate": 2.4195326278659613e-07,
      "loss": 0.1998,
      "step": 35850
    },
    {
      "epoch": 0.988205467372134,
      "grad_norm": 0.014430960640311241,
      "learning_rate": 2.3644179894179897e-07,
      "loss": 0.0005,
      "step": 35860
    },
    {
      "epoch": 0.9884810405643739,
      "grad_norm": 0.10390427708625793,
      "learning_rate": 2.3093033509700178e-07,
      "loss": 0.1582,
      "step": 35870
    },
    {
      "epoch": 0.9887566137566137,
      "grad_norm": 0.005611378699541092,
      "learning_rate": 2.254188712522046e-07,
      "loss": 0.1482,
      "step": 35880
    },
    {
      "epoch": 0.9890321869488536,
      "grad_norm": 0.010715901851654053,
      "learning_rate": 2.1990740740740742e-07,
      "loss": 0.1412,
      "step": 35890
    },
    {
      "epoch": 0.9893077601410935,
      "grad_norm": 0.05276906490325928,
      "learning_rate": 2.1439594356261026e-07,
      "loss": 0.0029,
      "step": 35900
    },
    {
      "epoch": 0.9895833333333334,
      "grad_norm": 0.19476519525051117,
      "learning_rate": 2.0888447971781307e-07,
      "loss": 0.0012,
      "step": 35910
    },
    {
      "epoch": 0.9898589065255732,
      "grad_norm": 0.04966018721461296,
      "learning_rate": 2.033730158730159e-07,
      "loss": 0.1523,
      "step": 35920
    },
    {
      "epoch": 0.9901344797178131,
      "grad_norm": 0.008615707978606224,
      "learning_rate": 1.978615520282187e-07,
      "loss": 0.1167,
      "step": 35930
    },
    {
      "epoch": 0.9904100529100529,
      "grad_norm": 125.0434799194336,
      "learning_rate": 1.9235008818342154e-07,
      "loss": 0.0908,
      "step": 35940
    },
    {
      "epoch": 0.9906856261022927,
      "grad_norm": 0.009761953726410866,
      "learning_rate": 1.8683862433862435e-07,
      "loss": 0.0112,
      "step": 35950
    },
    {
      "epoch": 0.9909611992945326,
      "grad_norm": 0.06513381004333496,
      "learning_rate": 1.813271604938272e-07,
      "loss": 0.0938,
      "step": 35960
    },
    {
      "epoch": 0.9912367724867724,
      "grad_norm": 0.011639795266091824,
      "learning_rate": 1.7581569664903e-07,
      "loss": 0.0007,
      "step": 35970
    },
    {
      "epoch": 0.9915123456790124,
      "grad_norm": 0.0074988482519984245,
      "learning_rate": 1.7030423280423283e-07,
      "loss": 0.205,
      "step": 35980
    },
    {
      "epoch": 0.9917879188712522,
      "grad_norm": 0.0074807326309382915,
      "learning_rate": 1.6479276895943564e-07,
      "loss": 0.1684,
      "step": 35990
    },
    {
      "epoch": 0.9920634920634921,
      "grad_norm": 0.00748440483585,
      "learning_rate": 1.5928130511463847e-07,
      "loss": 0.0187,
      "step": 36000
    },
    {
      "epoch": 0.9923390652557319,
      "grad_norm": 0.1526433527469635,
      "learning_rate": 1.5376984126984128e-07,
      "loss": 0.0903,
      "step": 36010
    },
    {
      "epoch": 0.9926146384479718,
      "grad_norm": 156.2809600830078,
      "learning_rate": 1.482583774250441e-07,
      "loss": 0.0207,
      "step": 36020
    },
    {
      "epoch": 0.9928902116402116,
      "grad_norm": 9.320517539978027,
      "learning_rate": 1.4274691358024693e-07,
      "loss": 0.1382,
      "step": 36030
    },
    {
      "epoch": 0.9931657848324515,
      "grad_norm": 0.03313383832573891,
      "learning_rate": 1.3723544973544974e-07,
      "loss": 0.0036,
      "step": 36040
    },
    {
      "epoch": 0.9934413580246914,
      "grad_norm": 0.010374422185122967,
      "learning_rate": 1.3172398589065257e-07,
      "loss": 0.0832,
      "step": 36050
    },
    {
      "epoch": 0.9937169312169312,
      "grad_norm": 0.006772358436137438,
      "learning_rate": 1.2621252204585538e-07,
      "loss": 0.2664,
      "step": 36060
    },
    {
      "epoch": 0.9939925044091711,
      "grad_norm": 0.023652080446481705,
      "learning_rate": 1.2070105820105821e-07,
      "loss": 0.2142,
      "step": 36070
    },
    {
      "epoch": 0.9942680776014109,
      "grad_norm": 0.05616245046257973,
      "learning_rate": 1.1518959435626102e-07,
      "loss": 0.0097,
      "step": 36080
    },
    {
      "epoch": 0.9945436507936508,
      "grad_norm": 0.09141131490468979,
      "learning_rate": 1.0967813051146384e-07,
      "loss": 0.098,
      "step": 36090
    },
    {
      "epoch": 0.9948192239858906,
      "grad_norm": 0.00981234759092331,
      "learning_rate": 1.0416666666666667e-07,
      "loss": 0.0014,
      "step": 36100
    },
    {
      "epoch": 0.9950947971781305,
      "grad_norm": 0.011515467427670956,
      "learning_rate": 9.865520282186949e-08,
      "loss": 0.0538,
      "step": 36110
    },
    {
      "epoch": 0.9953703703703703,
      "grad_norm": 0.008401630446314812,
      "learning_rate": 9.314373897707231e-08,
      "loss": 0.0702,
      "step": 36120
    },
    {
      "epoch": 0.9956459435626103,
      "grad_norm": 0.006847885437309742,
      "learning_rate": 8.763227513227513e-08,
      "loss": 0.0623,
      "step": 36130
    },
    {
      "epoch": 0.9959215167548501,
      "grad_norm": 0.38682588934898376,
      "learning_rate": 8.212081128747795e-08,
      "loss": 0.0906,
      "step": 36140
    },
    {
      "epoch": 0.99619708994709,
      "grad_norm": 45.07587432861328,
      "learning_rate": 7.660934744268078e-08,
      "loss": 0.1307,
      "step": 36150
    },
    {
      "epoch": 0.9964726631393298,
      "grad_norm": 0.025824181735515594,
      "learning_rate": 7.10978835978836e-08,
      "loss": 0.0051,
      "step": 36160
    },
    {
      "epoch": 0.9967482363315696,
      "grad_norm": 3.261557102203369,
      "learning_rate": 6.558641975308642e-08,
      "loss": 0.0919,
      "step": 36170
    },
    {
      "epoch": 0.9970238095238095,
      "grad_norm": 0.01655603013932705,
      "learning_rate": 6.007495590828924e-08,
      "loss": 0.0686,
      "step": 36180
    },
    {
      "epoch": 0.9972993827160493,
      "grad_norm": 0.01374781783670187,
      "learning_rate": 5.456349206349206e-08,
      "loss": 0.0139,
      "step": 36190
    },
    {
      "epoch": 0.9975749559082893,
      "grad_norm": 118.84033966064453,
      "learning_rate": 4.9052028218694885e-08,
      "loss": 0.0375,
      "step": 36200
    },
    {
      "epoch": 0.9978505291005291,
      "grad_norm": 0.17861463129520416,
      "learning_rate": 4.3540564373897706e-08,
      "loss": 0.1264,
      "step": 36210
    },
    {
      "epoch": 0.998126102292769,
      "grad_norm": 25.271102905273438,
      "learning_rate": 3.802910052910053e-08,
      "loss": 0.1498,
      "step": 36220
    },
    {
      "epoch": 0.9984016754850088,
      "grad_norm": 0.022504648193717003,
      "learning_rate": 3.251763668430335e-08,
      "loss": 0.0348,
      "step": 36230
    },
    {
      "epoch": 0.9986772486772487,
      "grad_norm": 38.75698471069336,
      "learning_rate": 2.7006172839506175e-08,
      "loss": 0.1034,
      "step": 36240
    },
    {
      "epoch": 0.9989528218694885,
      "grad_norm": 0.0143652418628335,
      "learning_rate": 2.1494708994708997e-08,
      "loss": 0.014,
      "step": 36250
    },
    {
      "epoch": 0.9992283950617284,
      "grad_norm": 0.035142574459314346,
      "learning_rate": 1.5983245149911815e-08,
      "loss": 0.0023,
      "step": 36260
    },
    {
      "epoch": 0.9995039682539683,
      "grad_norm": 0.15456581115722656,
      "learning_rate": 1.0471781305114639e-08,
      "loss": 0.0008,
      "step": 36270
    },
    {
      "epoch": 0.9997795414462081,
      "grad_norm": 0.7782891392707825,
      "learning_rate": 4.960317460317461e-09,
      "loss": 0.1205,
      "step": 36280
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9861111111111112,
      "eval_f1": 0.9860588625802169,
      "eval_loss": 0.06732171028852463,
      "eval_precision": 0.989782318969347,
      "eval_recall": 0.982363315696649,
      "eval_runtime": 49.0204,
      "eval_samples_per_second": 1480.525,
      "eval_steps_per_second": 185.066,
      "step": 36288
    }
  ],
  "logging_steps": 10,
  "max_steps": 36288,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 92205596928000.0,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
